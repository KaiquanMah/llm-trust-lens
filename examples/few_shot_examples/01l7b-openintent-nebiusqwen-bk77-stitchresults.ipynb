{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Refactored code for\n* Setting up and running Ollama in Kaggle\n* Downloading THUIAR dataset\n* Zero-Shot Prompt\n* Use LLM to classify intent from an input 'question' dataset\n* To configure your file/folder paths, LLM, dataset, start_index and end_index for each run, please update the config.py file\n\nThis notebook will also be used as the base to test any fixes to the LLM intent classification pipeline.\n* 2025.05.26: Updated results output file from JSON to Pickle, to store list of dictionaries. 1 dictionary contains the results for each record. Lists of dictionaries can be downloaded from multiple notebooks, then concatenated for analysis\n* 2025.05.30: Update prompt and bulletpts_intent.\n  * Check if dataset contains 'oos' (out of scope) category\n  * If dataset has no 'oos' (out of scope) category, turn 1 category into 'oos'. Use updated categories in bulletpts_intent. Also update prompt instructions on when to classify an example as 'oos'\n  * **This force_oos fix is implemented in [notebook 01E](https://www.kaggle.com/code/kaiquanmah/01e-kaggle-ollama-llama3-2-w-force-oos?scriptVersionId=242648764)**\n* 2025.05.30: Add pydantic schema with enums\n  * From an analysis of errors, the model previously had a 45% average accuracy rate across categories. The model predicted a set of categories outside of what we gave it in 'bulletpts_intent'\n  * To fix this, we will try to implement a pydantic schema solution for the model to only predict categories from the allowed list of categories ('bulletpts_intent')\n* 2025.05.30: Set Ollama chat temperature to 0\n  * Previously, we used the default temperature of 0.8, which might have caused the model to predict categories we did not provide to it ([Reading](https://docs.spring.io/spring-ai/reference/api/chat/ollama-chat.html))\n  * **The pydantic schema and temperature fixes are implemented in [notebook 01F](https://www.kaggle.com/code/kaiquanmah/01f-kaggle-ollama-llama3-2-w-pydantic-schema)**\n* 2025.06.03:\n  1. **Remove 'oos' from `bulletpts_intent` input into prompt**, to be consistent with the team's approach when exploring embedding approaches to classify 'oos' examples. **Keep 'oos' in pydantic enums/Literal (for LLM to output 'oos' as an allowed class value)**\n  2. **Remove 0.99 when defining the prompt format - to avoid anchoring LLM on outputting confidence of 0.99**\n  3. **Added ability for user to define which classes are 'oos'**\n  * **These 3 fixes are in [notebook 01G](https://www.kaggle.com/code/kaiquanmah/01g-kaggle-ollama-llama3-2-oos-update)**\n* 2025.06.10:\n  * From an error analysis earlier, **models can get confused between similar intent classes**\n  * Therefore **we will analyse similar intent classes/labels -> get their indexes -> put them into 'oos' in [notebook 01H](https://www.kaggle.com/code/kaiquanmah/01h1-openintent-ollama-llama3-2-3b-banking77)**\n  * **Going from zero-shot prompt previously, to few-shot prompt (with 5 examples) from known intents**. These 5 examples were **non-oos, and misclassified previously**. This 'fix' is in **[notebook 01i](https://www.kaggle.com/code/kaiquanmah/01i1-openintent-ollama-llama3-2-3b-banking77)**\n* 2025.06.16:\n  * For known intents (ie not in the 'oos' class), give 5 examples each in the few-shot prompt **[notebook 01J](https://www.kaggle.com/code/kaiquanmah/01j1-openintent-ollama-llama3-2-3b-banking77)**\n* 2025.06.17:\n  * Now we explore how changing the number of known intent classes affects the recall of oos in **[notebook 01K](https://www.kaggle.com/code/kaiquanmah/01k1-openintent-ollama-llama3-2-3b-banking77)**\n  * For quick experimentation, we implement (1) fewshot prompt with 1 example per known intent class, (2) changing number of known intent classes in various notebook runs, (3) 100 oos sentences for the model to classify (taking from first class for banking77 and stackoverflow dataset, or the oos class for CLINC150 oos dataset)\n    * For (3) - Added 'first_class' variable for each dataset to Config\n    * For (3) - Created new fn to filter and keep 100 records from 'first/oos class' to input to the model to classify\n* 2025.07.07:\n  * Explore free, rate-limited API model (such as Gemini) in **[notebook 01L](https://www.kaggle.com/code/kaiquanmah/01l1-openintent-gemini-banking77-explore)**\n  * Added retry for when we exhaust API limits per minute\n  * Updated end_index tracking that works with Ollama and Gemini when generating JSON results file\n  * **Explore Qwen model from the Nebius platform**","metadata":{}},{"cell_type":"code","source":"# 1. create dirs if they do not exist\nimport os\nos.makedirs('/kaggle/working/src', exist_ok=True)\nos.makedirs('/kaggle/working/prediction', exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T08:14:59.726298Z","iopub.execute_input":"2025-07-11T08:14:59.726670Z","iopub.status.idle":"2025-07-11T08:14:59.737469Z","shell.execute_reply.started":"2025-07-11T08:14:59.726637Z","shell.execute_reply":"2025-07-11T08:14:59.736565Z"}},"outputs":[{"name":"stdout","text":"The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%%writefile /kaggle/working/src/setup_ollama.py\nimport os\nimport subprocess\nimport time\nfrom src.config import Config # absolute import\n\n# 1. Install Ollama (if not already installed)\ntry:\n    # Check if Ollama is already installed\n    subprocess.run([\"ollama\", \"--version\"], capture_output=True, check=True)\n    print(\"Ollama is already installed.\")\nexcept FileNotFoundError:\n    print(\"Installing Ollama...\")\n    subprocess.run(\"curl -fsSL https://ollama.com/install.sh  | sh\", shell=True, check=True)\n\n# 2. Start Ollama server in the background\nprint(\"Starting Ollama server...\")\nprocess = subprocess.Popen(\"ollama serve\", shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n\n# Wait for the server to initialize\ntime.sleep(5)\n\n\n# 3. Pull the model\nmodel_name = Config.model_name\nprint(f\"Pulling {model_name} model...\")\nsubprocess.run([\"ollama\", \"pull\", model_name], check=True)\n\n# 4. Install Python client\nsubprocess.run([\"pip\", \"install\", \"ollama\"], check=True)\n\nprint(\"Ollama setup complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T08:14:59.739310Z","iopub.execute_input":"2025-07-11T08:14:59.739570Z","iopub.status.idle":"2025-07-11T08:14:59.745894Z","shell.execute_reply.started":"2025-07-11T08:14:59.739548Z","shell.execute_reply":"2025-07-11T08:14:59.744931Z"}},"outputs":[{"name":"stdout","text":"Writing /kaggle/working/src/setup_ollama.py\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"%%writefile requirements.txt\npandas\npydantic\ntyping\nhuggingface-hub\n# google-genai # only used for gemini model\nopenai # used for openrouter's gemini model\ntenacity # for gemini model retries\n# numpy\n# enum","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T08:14:59.746816Z","iopub.execute_input":"2025-07-11T08:14:59.747053Z","iopub.status.idle":"2025-07-11T08:14:59.752683Z","shell.execute_reply.started":"2025-07-11T08:14:59.747034Z","shell.execute_reply":"2025-07-11T08:14:59.751891Z"}},"outputs":[{"name":"stdout","text":"Writing requirements.txt\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"%%writefile /kaggle/working/src/__init__.py\n# folder for config","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T08:14:59.753882Z","iopub.execute_input":"2025-07-11T08:14:59.754219Z","iopub.status.idle":"2025-07-11T08:14:59.759263Z","shell.execute_reply.started":"2025-07-11T08:14:59.754190Z","shell.execute_reply":"2025-07-11T08:14:59.758558Z"}},"outputs":[{"name":"stdout","text":"Writing /kaggle/working/src/__init__.py\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"%%writefile /kaggle/working/src/config.py\nclass Config:\n    #######################################################\n    # working directory for files\n    #######################################################\n    target_dir = '/kaggle/working/data' # data directory to clone into\n    cloned_data_dir = target_dir + '/data'\n    prediction_dir = target_dir + '/prediction'\n    #######################################################\n    # dataset and model\n    #######################################################\n    dataset_name = 'banking' # UPDATE options: 'banking', 'stackoverflow', 'oos'\n    idx2label_target_dir = '/kaggle/working/idx2label'\n    idx2label_filename_hf = 'banking77_idx2label.csv' # UPDATE options: banking77_idx2label.csv, stackoverflow_idx2label.csv, clinc150_oos_idx2label.csv\n    fewshot_examples_dir = '/kaggle/working/fewshot'\n    fewshot_subdir = '/fewshot-5examples-per-nonoos/'\n    fewshot_examples_filename = 'banking_25perc_oos.txt' # UPDATE options: banking_25perc_oos.txt, stackoverflow_25perc_oos.txt, oos_25perc_oos.txt\n    list_oos_idx = [2, 10, 14, 15, 16, 17, 19, 25, 31, 32, 33, 34, 36, 52, 53, 54, 57, 73, 76] # UPDATE gathered from within the team - for reproducible, comparable results with other open intent classification approaches\n    model_name = 'Qwen3-30B-A3B' # 'gemma-2-9b-it-fast'\n    start_index=0 # eg: 0, 10001, 11851\n    end_index=None # eg: 10, 10000, 11850 or None (use end_index=None to process the full dataset)\n    log_every_n_examples=10 # 2\n    force_oos = True  # NEW: Add flag to force dataset to contain 'oos' class for the last class value (sorted alphabetically), if 'oos' class does not exist in the original dataset\n    #######################################################\n    # evaluate threshold when 'oos' recall drops\n    #######################################################\n    filter_oos_qns_only = False # True (when you are testing 'oos' recall threshold), False\n    n_oos_qns = 100\n    first_class_banking = 'activate_my_card' # following idx2label\n    first_class_stackoverflow = 'wordpress' # following idx2label\n    first_class_oos = 'oos'\n    #######################################################","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T08:14:59.909784Z","iopub.execute_input":"2025-07-11T08:14:59.910078Z","iopub.status.idle":"2025-07-11T08:14:59.917213Z","shell.execute_reply.started":"2025-07-11T08:14:59.910056Z","shell.execute_reply":"2025-07-11T08:14:59.916239Z"}},"outputs":[{"name":"stdout","text":"Writing /kaggle/working/src/config.py\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"%%writefile download_dataset.py\nfrom src.config import Config\nimport os\nimport subprocess\ntarget_dir = Config.target_dir # data directory to clone into\ncloned_data_dir = Config.cloned_data_dir\n\n# Create target directory if it doesn't exist\nos.makedirs(target_dir, exist_ok=True)\n\n# do not clone dataset repo if cloned data folder exists\nif os.path.exists(cloned_data_dir):\n    print(\"Dataset has already been downloaded. If this is incorrect, please delete the Adaptive-Decision-Boundary 'data' folder.\")\nelse:\n    # Clone the repository\n    subprocess.run([\"git\",\n                    \"clone\",\n                    \"https://github.com/thuiar/Adaptive-Decision-Boundary.git\",\n                    target_dir\n                   ])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T08:14:59.918948Z","iopub.execute_input":"2025-07-11T08:14:59.919252Z","iopub.status.idle":"2025-07-11T08:14:59.925259Z","shell.execute_reply.started":"2025-07-11T08:14:59.919229Z","shell.execute_reply":"2025-07-11T08:14:59.924117Z"}},"outputs":[{"name":"stdout","text":"Writing download_dataset.py\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"%%writefile predict_class.py\nfrom src.config import Config\nimport pandas as pd\nimport os\n# import ollama\nimport json\nimport pickle\nimport time\nfrom pydantic import BaseModel\nfrom typing import Literal\n# from enum import Enum\nfrom huggingface_hub import snapshot_download\n    \n###################\n# Gemini API\n###################\n# from google import genai\n# from google.genai.types import ThinkingConfig\n# from google.api_core import retry\nfrom openai import OpenAI\nfrom tenacity import retry, stop_after_attempt, wait_fixed\nfrom kaggle_secrets import UserSecretsClient\n\n\n###################\n\n\n# Config.target_dir\n# Config.cloned_data_dir'\n# Config.dataset_name\n# Config.model_name\n# Config.start_index\n# Config.end_index\n# Config.log_every_n_examples\n\n\n#######################\n# load data\n#######################\ndef load_data(data_dir):\n    \"\"\"Loads train, dev, and test datasets from a specified directory.\"\"\"\n\n    main_df = pd.DataFrame()\n    for split in ['train', 'dev', 'test']:\n        file_path = os.path.join(data_dir, f'{split}.tsv')\n        if os.path.exists(file_path):\n          try:\n            df = pd.read_csv(file_path, sep='\\t')\n            df['dataset'] = os.path.basename(data_dir)\n            df['split'] = split\n            main_df = pd.concat([main_df, df], ignore_index=True)\n          except pd.errors.ParserError as e:\n            print(f\"Error parsing {file_path}: {e}\")\n            # Handle the error appropriately, e.g., skip the file, log the error, etc.\n        else:\n            print(f\"Warning: {split}.tsv not found in {data_dir}\")\n    return main_df\n\n\ndef filter100examples_oos(dataset_name, df):\n    # dont input 'only oos qns to model'\n    if Config.filter_oos_qns_only == False:\n        filtered_df = df\n    # vs\n    # input 'only oos qns to model'\n    else:\n        if dataset_name == 'banking':\n            first_class = Config.first_class_banking\n        elif dataset_name == 'stackoverflow':\n            first_class = Config.first_class_stackoverflow\n        else:\n            first_class = Config.first_class_oos\n    \n        filtered_df = df.copy()\n        filtered_df = filtered_df.loc[filtered_df[\"label\"] == first_class]\n        filtered_df = filtered_df.sample(n=Config.n_oos_qns, random_state=38)\n    return filtered_df\n\n\ndf = pd.DataFrame()\n\ndata_dir = os.path.join(Config.cloned_data_dir, Config.dataset_name)\nif os.path.exists(data_dir):\n  df = load_data(data_dir)\n  print(f\"Loaded dataset into dataframe: {Config.dataset_name}\")\n  print(f\"Dimensions: {df.shape}\")\n  print(f\"Col names: {df.columns}\")\nelse:\n  print(f\"Warning: Directory {data_dir} not found.\")\n#######################\n\n\n\n#######################\n# unique intents\n#######################\nsorted_intent = list(sorted(df.label.unique()))\nprint(\"=\"*80)\nprint(f\"Original dataset intents: {sorted_intent}\")\nprint(f\"Number of original intents: {len(sorted_intent)}\\n\")\n\n\n# 2025.06.03\n# New OOS approach - get 25/50/75% of class indexes for each dataset within the team (for reproducibility and comparable results)\n# Change their class labels to 'oos'\nsnapshot_download(repo_id=\"KaiquanMah/open-intent-query-classification\", repo_type=\"space\", allow_patterns=\"*_idx2label.csv\", local_dir=Config.idx2label_target_dir)\nidx2label_filepath = Config.idx2label_target_dir + '/dataset_idx2label/' + Config.idx2label_filename_hf\nidx2label = pd.read_csv(idx2label_filepath)\nidx2label_oos = idx2label[idx2label.index.isin(Config.list_oos_idx)]\nidx2label_oos.reset_index(drop=True, inplace=True)\n\n# 2025.06.17 keep track of non-oos labels, to use in IntentSchema\nnonoos_labels = idx2label[~idx2label.label.isin(Config.list_oos_idx)]['label'].values\nprint(\"=\"*80)\nprint(\"Original intents to convert to OOS class\")\nprint(idx2label_oos)\nprint(f\"Percentage of original intents to convert to OOS class: {len(idx2label_oos)/len(idx2label)}\\n\")\n\noos_labels = idx2label_oos['label'].values\nlist_sorted_intent_aft_conversion = ['oos' if intent.lower() in oos_labels else intent for intent in sorted_intent]\nlist_sorted_intent_aft_conversion_deduped = sorted(set(list_sorted_intent_aft_conversion))\nprint(\"=\"*80)\nprint(\"Unique intents after converting some to OOS class\")\nprint(list_sorted_intent_aft_conversion_deduped)\nprint(f\"Number of unique intents after converting some to OOS class: {len(list_sorted_intent_aft_conversion_deduped)}\\n\")\n\n\n\n# unique intents - from set to bullet points (to use in prompts)\n# bulletpts_intent = \"\\n\".join(f\"- {category}\" for category in set_intent)\n# 2025.06.03: do not show 'oos' in the prompt (to avoid leakage of 'oos' class)\nbulletpts_intent = \"\\n\".join(f\"- {category}\" for category in list_sorted_intent_aft_conversion_deduped if category and (category!='oos'))\n\n# 2025.06.04: fix adjustment if 'oos' is already in the original dataset\nint_oos_in_orig_dataset = int('oos' in idx2label.label.values)\nadjust_if_oos_not_in_orig_dataset = [0 if int_oos_in_orig_dataset == 1 else 1][0]\n\nprint(\"=\"*80)\nprint(\"sanity check\")\nprint(f\"Number of original intents: {len(sorted_intent)}\")\nprint(f\"Number of original intents + 1 OOS class (if doesnt exist in original dataset): {len(sorted_intent) + adjust_if_oos_not_in_orig_dataset}\")\nprint(f\"Number of original intents to convert to OOS class: {len(idx2label_oos)}\")\nprint(f\"Percentage of original intents to convert to OOS class: {len(idx2label_oos)/len(idx2label)}\")\nprint(f\"Number of unique intents after converting some to OOS class: {len(list_sorted_intent_aft_conversion_deduped)}\")\nprint(f\"Number of original intents + 1 OOS class (if doesnt exist in original dataset) - converted classes: {len(sorted_intent) + adjust_if_oos_not_in_orig_dataset - len(idx2label_oos)}\")\nprint(f\"Numbers match: {(len(sorted_intent) + adjust_if_oos_not_in_orig_dataset - len(idx2label_oos)) == len(list_sorted_intent_aft_conversion_deduped)}\")\nprint(\"Prepared unique intents\")\n#######################\n\n\n\n\n#######################\n# Enforce schema on the model (e.g. allowed list of predicted categories)\n#######################\n\nclass IntentSchema(BaseModel):\n    # dynamically unpack list of categories for different dataset(s)\n    category: Literal[*list_sorted_intent_aft_conversion_deduped]\n    confidence: float\n    \n#######################\n\n\n\n\n#######################\n# filter after preparing intents\n#######################\ndf = filter100examples_oos(Config.dataset_name, df)\nprint(\"Filtered dataset\")\nprint(f\"Dimensions: {df.shape}\")\nprint(f\"Col names: {df.columns}\")\n#######################\n\n\n\n#######################\n# Prompt\n#######################\n# prompt 2 with less information/compute, improve efficiency\n# 2025.06.10 prompt 3 with 5 few shot examples only - notebook O1H1, O1i1\n# 2025.06.16 prompt 4 with 5 examples per each known intent (ie non-oos intent) - notebook 01J1\nsnapshot_download(repo_id=\"KaiquanMah/open-intent-query-classification\", repo_type=\"space\", allow_patterns=\"*.txt\", local_dir=Config.fewshot_examples_dir)\nwith open(Config.fewshot_examples_dir + Config.fewshot_subdir + Config.fewshot_examples_filename, 'r') as file:\n    fewshot_examples = file.read()\n\ndef get_prompt(dataset_name, split, question, categories, fewshot_examples):\n    \n    prompt = f'''\nYou are an expert in understanding and identifying what users are asking you.\n\nYour task is to analyze an input query from a user and assign the most appropriate category from the following list:\n{categories}\n\nOnly classify as \"oos\" (out of scope category) if none of the other categories apply.\n\nBelow are several examples to guide your classification:\n\n---\n{fewshot_examples}\n---\n\n===============================\n\nNew Question: {question}\n\n===============================\n\nProvide your final classification in **valid JSON format** with the following structure:\n{{\n  \"category\": \"your_chosen_category_name\",\n  \"confidence\": confidence_level_rounded_to_the_nearest_2_decimal_places\n}}\n\n\nEnsure the JSON has:\n- Opening and closing curly braces\n- Double quotes around keys and string values\n- Confidence as a number (not a string), with maximum 2 decimal places\n\nDo not include any explanations or extra text.\n            '''\n    return prompt\n\n\n\n#######################\n\n\n#######################\n# Model on 1 Dataset\n#######################\n# Save a list of dictionaries \n# containing a dictionary for each record's\n# - predicted category\n# - confidence level and\n# - original dataframe values\n\n\n# gemini\nuser_secrets = UserSecretsClient()\nNEBIUS_API_KEY = user_secrets.get_secret(\"NEBIUS_API_KEY\")\nclient = OpenAI(base_url=\"https://api.studio.nebius.com/v1/\",\n                api_key = NEBIUS_API_KEY)\n\n@retry(stop=stop_after_attempt(3), wait=wait_fixed(30))\ndef api_llm(client, prompt):\n    try:\n        print(\"CHECKPOINT_3A\")\n        # gemini_config = {\"temperature\": 0,\n        #                  \"response_mime_type\": \"application/json\",\n        #                  \"response_schema\": IntentSchema.model_json_schema(),\n        #                  \"seed\": 38,\n        #                  # # added for \"gemini-2.5-flash-lite-preview-06-17\" model\n        #                  # \"thinking_config\": ThinkingConfig(thinking_budget=-1, \n        #                  #                    include_thoughts=True)\n        #                 }\n        response = client.beta.chat.completions.parse(model = 'Qwen/'+Config.model_name,\n                                                      messages = [{\"role\": \"user\",\n                                                                  \"content\": prompt}],\n                                                      response_format = IntentSchema,\n                                                      seed = 38,\n                                                      temperature = 0\n                                                      )\n        # print(response)\n        # msg = response.parsed\n        response = response.choices[0].message.content\n        print(\"CHECKPOINT_3B\")\n        return response\n    except:\n        print(f\"CHECKPOINT_4A: Exception Type: {type(e).__name__}\")\n        print(f\"CHECKPOINT_4A: Exception Message: {str(e)}\")\n        \n        # Gemini-specific errors\n        if hasattr(e, 'code'):\n            print(f\"CHECKPOINT_4A: Status Code: {e.code}\")\n        if hasattr(e, 'details'):\n            print(f\"CHECKPOINT_4A: Details: {e.details}\")\n        \n        # raise the exception again so retry can work\n        raise\n\n    \n\ndef predict_intent(model_name, df, categories, start_index=0, end_index=None, log_every_n_examples=100):\n    start_time = time.time()\n    results = []  # Store processed results\n    \n    # Slice DataFrame based on start/end indices\n    if end_index is None:\n        subset_df = df.iloc[start_index:]\n    else:\n        subset_df = df.iloc[start_index:end_index+1]\n    \n    total_rows = len(subset_df)\n    subset_row_count = 0\n\n    \n\n    \n    \n    for row in subset_df.itertuples():\n        subset_row_count+=1\n        prompt = get_prompt(row.dataset, row.split, row.text, categories, fewshot_examples)\n        if subset_row_count == 1:\n            print(\"Example of how prompt looks, for the 1st example in this subset of data\")\n            print(prompt)\n\n            print(\"Example of how IntentSchema looks\")\n            print(IntentSchema.model_json_schema())\n        \n        \n        try:\n            print(\"CHECKPOINT_1A\")\n            \n            # response = ollama.chat(model=model_name, \n            #                        messages=[\n            #                                     {'role': 'user', 'content': prompt}\n            #                                 ],\n            #                        format = IntentSchema.model_json_schema(),\n            #                        options = {'temperature': 0},  # Set temperature to 0 for a more deterministic output\n            #                       )\n            # msg = response['message']['content']\n            # parsed = json.loads(msg)\n            \n            response = api_llm(client, prompt)\n            print(\"CHECKPOINT_1B\")\n            parsed = json.loads(response.text)\n            # parsed = response.parsed\n            print(\"CHECKPOINT_1C\")\n                        \n            # Safely extract keys with defaults - resolve parsing error\n            # maybe LLM did not output a particular key-value pair\n            category = parsed.get('category', 'error')\n            confidence = parsed.get('confidence', 0.0)\n            parsed = {'category': category, 'confidence': confidence}\n        except (json.JSONDecodeError, KeyError, Exception) as e:\n            print(f\"CHECKPOINT_2A: Exception Type: {type(e).__name__}\")\n            print(f\"CHECKPOINT_2A: Exception Message: {str(e)}\")\n            \n            # Gemini-specific errors\n            if hasattr(e, 'code'):\n                print(f\"CHECKPOINT_2A: Status Code: {e.code}\")\n            if hasattr(e, 'details'):\n                print(f\"CHECKPOINT_2A: Details: {e.details}\")\n                \n            parsed = {'category': 'error', 'confidence': 0.0}\n        \n        # Combine original row data with predictions\n        results.append({\n            \"Index\": row.Index,\n            \"text\": row.text,\n            \"label\": row.label,\n            \"dataset\": row.dataset,\n            \"split\": row.split,\n            \"predicted\": parsed['category'],\n            \"confidence\": parsed['confidence']\n        })\n\n        \n        # Log progress\n        if subset_row_count % log_every_n_examples == 0:\n            elapsed_time = time.time() - start_time\n            \n            avg_time_per_row = elapsed_time / subset_row_count\n            remaining_rows = total_rows - subset_row_count\n            eta = avg_time_per_row * remaining_rows\n            \n            print(f\"Processed original df idx {row.Index} (subset row {subset_row_count}) | \"\n                  f\"Elapsed: {elapsed_time:.2f}s | ETA: {eta:.2f}s\")\n    \n    return results  # Return list of dictionaries\n    \n\nprint(f\"Starting intent classification using {Config.model_name}\")\nsubset_results = predict_intent(Config.model_name, \n                                df, \n                                bulletpts_intent, \n                                start_index = Config.start_index, \n                                end_index = Config.end_index,\n                                log_every_n_examples = Config.log_every_n_examples)\n\n\n\n# # previously for Ollama\n# # update end_index for filename (if None is used for the end of the df)\n# # Get the last index of the DataFrame\n# last_index = df.index[-1] \n# # Use last index if Config.end_index is None\n# end_index = Config.end_index if Config.end_index is not None else last_index\n# 2025.07.07\n# now for Ollama AND Gemini\n# Gemini - needs to track 'end_index' for API JSON exports (when daily limits are exhausted)\n# Ollama - reuse this code\nend_index = max(r['Index'] for r in subset_results)\n\n\n\n# 2025.05.23 changed from JSON to PKL\n# because we are saving list of dictionaries\n# Save to PKL\n# 2025.06.04 explore changing back to JSON\n# with open(f'results_{Config.model_name}_{Config.dataset_name}_{Config.start_index}_{end_index}.pkl', 'wb') as f:\n#     pickle.dump(subset_results, f)\nwith open(f'results_{Config.model_name}_{Config.dataset_name}_{Config.start_index}_{end_index}.json', 'w') as f:\n    json.dump(subset_results, f, indent=2)\n\nprint(\"Completed intent classification\")\n\n\n#######################\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T08:14:59.926413Z","iopub.execute_input":"2025-07-11T08:14:59.926734Z","iopub.status.idle":"2025-07-11T08:14:59.939552Z","shell.execute_reply.started":"2025-07-11T08:14:59.926705Z","shell.execute_reply":"2025-07-11T08:14:59.938690Z"}},"outputs":[{"name":"stdout","text":"Writing predict_class.py\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"%%writefile /kaggle/working/main.py\nimport subprocess\nimport sys\nfrom src.config import Config\n\n\n# 1. Install libraries from requirements.txt\nprint(\"Installing dependencies...\")\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", \"/kaggle/working/requirements.txt\"], check=True)\n\n\n# # 2. Run setup_ollama.py\n# if 'gemini' not in Config.model_name:\n#     print(\"Starting Ollama setup...\")\n#     # subprocess.run([\"python3\", \"/kaggle/working/src/setup_ollama.py\"], check=True)\n#     print(\"Starting Ollama setup...\")\n#     subprocess.run(\n#         [\"python3\", \"-m\", \"src.setup_ollama\"],  # Run as a module\n#         cwd=\"/kaggle/working\",  # Set working directory to parent of 'src'\n#         check=True\n#     )\n    \n\n# 3. Run download_dataset.py\nprint(\"Downloading dataset...\")\nsubprocess.run([\"python3\", \"/kaggle/working/download_dataset.py\"], check=True)\n\n# 4. Run predict_class.py\nprint(\"Running prediction script...\")\nsubprocess.run([\"python3\", \"/kaggle/working/predict_class.py\"], check=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T08:14:59.940418Z","iopub.execute_input":"2025-07-11T08:14:59.940725Z","iopub.status.idle":"2025-07-11T08:14:59.947016Z","shell.execute_reply.started":"2025-07-11T08:14:59.940697Z","shell.execute_reply":"2025-07-11T08:14:59.946258Z"}},"outputs":[{"name":"stdout","text":"Writing /kaggle/working/main.py\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# Model on subset of examples","metadata":{}},{"cell_type":"code","source":"!python3 /kaggle/working/main.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T06:42:43.036942Z","iopub.execute_input":"2025-07-07T06:42:43.037254Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Sanity check folders","metadata":{}},{"cell_type":"code","source":"!cd /kaggle/working/ && ls -la","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!cd /kaggle/working/src && ls -la","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!cd /kaggle/working/data/data && ls -la","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# idx2label_oos examples","metadata":{}},{"cell_type":"code","source":"pip install huggingface-hub","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import snapshot_download\nsnapshot_download(repo_id=\"KaiquanMah/open-intent-query-classification\", repo_type=\"space\", allow_patterns=\"*_idx2label.csv\", local_dir='/kaggle/working/idx2label')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nidx2label = pd.read_csv('/kaggle/working/idx2label/dataset_idx2label/banking77_idx2label.csv')\nidx2label","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"idx2label_oos = idx2label[idx2label.index.isin([31,32,33,36])]\nidx2label_oos","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(idx2label_oos)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"idx2label_oos.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# percentage of OOS classes over ALL classes in the dataset\nlen(idx2label_oos)/len(idx2label)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Stitch Nebius Qwen API Batch Results for Banking77","metadata":{}},{"cell_type":"code","source":"import subprocess\nimport sys\nfrom src.config import Config\n\n\n# 1. Install libraries from requirements.txt\nprint(\"Installing dependencies...\")\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", \"/kaggle/working/requirements.txt\"], check=True)\n\n\n# # 2. Run setup_ollama.py\n# if 'gemini' not in Config.model_name:\n#     print(\"Starting Ollama setup...\")\n#     # subprocess.run([\"python3\", \"/kaggle/working/src/setup_ollama.py\"], check=True)\n#     print(\"Starting Ollama setup...\")\n#     subprocess.run(\n#         [\"python3\", \"-m\", \"src.setup_ollama\"],  # Run as a module\n#         cwd=\"/kaggle/working\",  # Set working directory to parent of 'src'\n#         check=True\n#     )\n    \n\n# 3. Run download_dataset.py\nprint(\"Downloading dataset...\")\nsubprocess.run([\"python3\", \"/kaggle/working/download_dataset.py\"], check=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T08:15:10.486336Z","iopub.execute_input":"2025-07-11T08:15:10.486619Z","iopub.status.idle":"2025-07-11T08:15:20.810662Z","shell.execute_reply.started":"2025-07-11T08:15:10.486597Z","shell.execute_reply":"2025-07-11T08:15:20.809856Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Installing dependencies...\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 1)) (2.2.3)\nRequirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 2)) (2.11.4)\nCollecting typing (from -r /kaggle/working/requirements.txt (line 3))\n  Downloading typing-3.7.4.3.tar.gz (78 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.6/78.6 kB 2.3 MB/s eta 0:00:00\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'done'\nRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 4)) (0.31.1)\nRequirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 6)) (1.70.0)\nRequirement already satisfied: tenacity in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 7)) (9.1.2)\nRequirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas->-r /kaggle/working/requirements.txt (line 1)) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->-r /kaggle/working/requirements.txt (line 1)) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->-r /kaggle/working/requirements.txt (line 1)) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->-r /kaggle/working/requirements.txt (line 1)) (2025.2)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->-r /kaggle/working/requirements.txt (line 2)) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->-r /kaggle/working/requirements.txt (line 2)) (2.33.2)\nRequirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->-r /kaggle/working/requirements.txt (line 2)) (4.13.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->-r /kaggle/working/requirements.txt (line 2)) (0.4.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->-r /kaggle/working/requirements.txt (line 4)) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->-r /kaggle/working/requirements.txt (line 4)) (2025.3.2)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->-r /kaggle/working/requirements.txt (line 4)) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->-r /kaggle/working/requirements.txt (line 4)) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->-r /kaggle/working/requirements.txt (line 4)) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->-r /kaggle/working/requirements.txt (line 4)) (4.67.1)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->-r /kaggle/working/requirements.txt (line 4)) (1.1.0)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai->-r /kaggle/working/requirements.txt (line 6)) (4.9.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai->-r /kaggle/working/requirements.txt (line 6)) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai->-r /kaggle/working/requirements.txt (line 6)) (0.28.1)\nRequirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai->-r /kaggle/working/requirements.txt (line 6)) (0.9.0)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai->-r /kaggle/working/requirements.txt (line 6)) (1.3.1)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai->-r /kaggle/working/requirements.txt (line 6)) (3.10)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai->-r /kaggle/working/requirements.txt (line 6)) (2025.4.26)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai->-r /kaggle/working/requirements.txt (line 6)) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r /kaggle/working/requirements.txt (line 6)) (0.14.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas->-r /kaggle/working/requirements.txt (line 1)) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas->-r /kaggle/working/requirements.txt (line 1)) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas->-r /kaggle/working/requirements.txt (line 1)) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas->-r /kaggle/working/requirements.txt (line 1)) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas->-r /kaggle/working/requirements.txt (line 1)) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas->-r /kaggle/working/requirements.txt (line 1)) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->-r /kaggle/working/requirements.txt (line 1)) (1.17.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->-r /kaggle/working/requirements.txt (line 4)) (3.4.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->-r /kaggle/working/requirements.txt (line 4)) (2.4.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas->-r /kaggle/working/requirements.txt (line 1)) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas->-r /kaggle/working/requirements.txt (line 1)) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.23.2->pandas->-r /kaggle/working/requirements.txt (line 1)) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.23.2->pandas->-r /kaggle/working/requirements.txt (line 1)) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.23.2->pandas->-r /kaggle/working/requirements.txt (line 1)) (2024.2.0)\nBuilding wheels for collected packages: typing\n  Building wheel for typing (setup.py): started\n  Building wheel for typing (setup.py): finished with status 'done'\n  Created wheel for typing: filename=typing-3.7.4.3-py3-none-any.whl size=26304 sha256=e3c9fe744cf1f9f4c5936906a87c1163ce71f23d5f8aea14e73b578a61de6cc2\n  Stored in directory: /root/.cache/pip/wheels/9d/67/2f/53e3ef32ec48d11d7d60245255e2d71e908201d20c880c08ee\nSuccessfully built typing\nInstalling collected packages: typing\nSuccessfully installed typing-3.7.4.3\nDownloading dataset...\n","output_type":"stream"},{"name":"stderr","text":"Cloning into '/kaggle/working/data'...\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"CompletedProcess(args=['python3', '/kaggle/working/download_dataset.py'], returncode=0)"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"from src.config import Config\nimport pandas as pd\nimport os\n# import ollama\nimport json\nimport pickle\nimport time\nfrom pydantic import BaseModel\nfrom typing import Literal\n# from enum import Enum\nfrom huggingface_hub import snapshot_download\n    \n###################\n# Gemini API\n###################\n# from google import genai\n# from google.genai.types import ThinkingConfig\n# from google.api_core import retry\nfrom openai import OpenAI\nfrom tenacity import retry, stop_after_attempt, wait_fixed\nfrom kaggle_secrets import UserSecretsClient\n\n\n###################\n\n\n# Config.target_dir\n# Config.cloned_data_dir'\n# Config.dataset_name\n# Config.model_name\n# Config.start_index\n# Config.end_index\n# Config.log_every_n_examples\n\n\n#######################\n# load data\n#######################\ndef load_data(data_dir):\n    \"\"\"Loads train, dev, and test datasets from a specified directory.\"\"\"\n\n    main_df = pd.DataFrame()\n    for split in ['train', 'dev', 'test']:\n        file_path = os.path.join(data_dir, f'{split}.tsv')\n        if os.path.exists(file_path):\n          try:\n            df = pd.read_csv(file_path, sep='\\t')\n            df['dataset'] = os.path.basename(data_dir)\n            df['split'] = split\n            main_df = pd.concat([main_df, df], ignore_index=True)\n          except pd.errors.ParserError as e:\n            print(f\"Error parsing {file_path}: {e}\")\n            # Handle the error appropriately, e.g., skip the file, log the error, etc.\n        else:\n            print(f\"Warning: {split}.tsv not found in {data_dir}\")\n    return main_df\n\n\ndef filter100examples_oos(dataset_name, df):\n    # dont input 'only oos qns to model'\n    if Config.filter_oos_qns_only == False:\n        filtered_df = df\n    # vs\n    # input 'only oos qns to model'\n    else:\n        if dataset_name == 'banking':\n            first_class = Config.first_class_banking\n        elif dataset_name == 'stackoverflow':\n            first_class = Config.first_class_stackoverflow\n        else:\n            first_class = Config.first_class_oos\n    \n        filtered_df = df.copy()\n        filtered_df = filtered_df.loc[filtered_df[\"label\"] == first_class]\n        filtered_df = filtered_df.sample(n=Config.n_oos_qns, random_state=38)\n    return filtered_df\n\n\ndf = pd.DataFrame()\n\ndata_dir = os.path.join(Config.cloned_data_dir, Config.dataset_name)\nif os.path.exists(data_dir):\n  df = load_data(data_dir)\n  print(f\"Loaded dataset into dataframe: {Config.dataset_name}\")\n  print(f\"Dimensions: {df.shape}\")\n  print(f\"Col names: {df.columns}\")\nelse:\n  print(f\"Warning: Directory {data_dir} not found.\")\n#######################\n\n\n\n#######################\n# unique intents\n#######################\nsorted_intent = list(sorted(df.label.unique()))\nprint(\"=\"*80)\nprint(f\"Original dataset intents: {sorted_intent}\")\nprint(f\"Number of original intents: {len(sorted_intent)}\\n\")\n\n\n# 2025.06.03\n# New OOS approach - get 25/50/75% of class indexes for each dataset within the team (for reproducibility and comparable results)\n# Change their class labels to 'oos'\nsnapshot_download(repo_id=\"KaiquanMah/open-intent-query-classification\", repo_type=\"space\", allow_patterns=\"*_idx2label.csv\", local_dir=Config.idx2label_target_dir)\nidx2label_filepath = Config.idx2label_target_dir + '/dataset_idx2label/' + Config.idx2label_filename_hf\nidx2label = pd.read_csv(idx2label_filepath)\nidx2label_oos = idx2label[idx2label.index.isin(Config.list_oos_idx)]\nidx2label_oos.reset_index(drop=True, inplace=True)\n\n# 2025.06.17 keep track of non-oos labels, to use in IntentSchema\nnonoos_labels = idx2label[~idx2label.label.isin(Config.list_oos_idx)]['label'].values\nprint(\"=\"*80)\nprint(\"Original intents to convert to OOS class\")\nprint(idx2label_oos)\nprint(f\"Percentage of original intents to convert to OOS class: {len(idx2label_oos)/len(idx2label)}\\n\")\n\noos_labels = idx2label_oos['label'].values\nlist_sorted_intent_aft_conversion = ['oos' if intent.lower() in oos_labels else intent for intent in sorted_intent]\nlist_sorted_intent_aft_conversion_deduped = sorted(set(list_sorted_intent_aft_conversion))\nprint(\"=\"*80)\nprint(\"Unique intents after converting some to OOS class\")\nprint(list_sorted_intent_aft_conversion_deduped)\nprint(f\"Number of unique intents after converting some to OOS class: {len(list_sorted_intent_aft_conversion_deduped)}\\n\")\n\n\n\n# unique intents - from set to bullet points (to use in prompts)\n# bulletpts_intent = \"\\n\".join(f\"- {category}\" for category in set_intent)\n# 2025.06.03: do not show 'oos' in the prompt (to avoid leakage of 'oos' class)\nbulletpts_intent = \"\\n\".join(f\"- {category}\" for category in list_sorted_intent_aft_conversion_deduped if category and (category!='oos'))\n\n# 2025.06.04: fix adjustment if 'oos' is already in the original dataset\nint_oos_in_orig_dataset = int('oos' in idx2label.label.values)\nadjust_if_oos_not_in_orig_dataset = [0 if int_oos_in_orig_dataset == 1 else 1][0]\n\nprint(\"=\"*80)\nprint(\"sanity check\")\nprint(f\"Number of original intents: {len(sorted_intent)}\")\nprint(f\"Number of original intents + 1 OOS class (if doesnt exist in original dataset): {len(sorted_intent) + adjust_if_oos_not_in_orig_dataset}\")\nprint(f\"Number of original intents to convert to OOS class: {len(idx2label_oos)}\")\nprint(f\"Percentage of original intents to convert to OOS class: {len(idx2label_oos)/len(idx2label)}\")\nprint(f\"Number of unique intents after converting some to OOS class: {len(list_sorted_intent_aft_conversion_deduped)}\")\nprint(f\"Number of original intents + 1 OOS class (if doesnt exist in original dataset) - converted classes: {len(sorted_intent) + adjust_if_oos_not_in_orig_dataset - len(idx2label_oos)}\")\nprint(f\"Numbers match: {(len(sorted_intent) + adjust_if_oos_not_in_orig_dataset - len(idx2label_oos)) == len(list_sorted_intent_aft_conversion_deduped)}\")\nprint(\"Prepared unique intents\")\n#######################\n\n\n\n\n#######################\n# Enforce schema on the model (e.g. allowed list of predicted categories)\n#######################\n\nclass IntentSchema(BaseModel):\n    # dynamically unpack list of categories for different dataset(s)\n    category: Literal[*list_sorted_intent_aft_conversion_deduped]\n    confidence: float\n    \n#######################\n\n\n\n\n#######################\n# filter after preparing intents\n#######################\ndf = filter100examples_oos(Config.dataset_name, df)\nprint(\"Filtered dataset\")\nprint(f\"Dimensions: {df.shape}\")\nprint(f\"Col names: {df.columns}\")\n#######################\n\n\n\n#######################\n# Prompt\n#######################\n# prompt 2 with less information/compute, improve efficiency\n# 2025.06.10 prompt 3 with 5 few shot examples only - notebook O1H1, O1i1\n# 2025.06.16 prompt 4 with 5 examples per each known intent (ie non-oos intent) - notebook 01J1\nsnapshot_download(repo_id=\"KaiquanMah/open-intent-query-classification\", repo_type=\"space\", allow_patterns=\"*.txt\", local_dir=Config.fewshot_examples_dir)\nwith open(Config.fewshot_examples_dir + Config.fewshot_subdir + Config.fewshot_examples_filename, 'r') as file:\n    fewshot_examples = file.read()\n\ndef get_prompt(dataset_name, split, question, categories, fewshot_examples):\n    \n    prompt = f'''\nYou are an expert in understanding and identifying what users are asking you.\n\nYour task is to analyze an input query from a user and assign the most appropriate category from the following list:\n{categories}\n\nOnly classify as \"oos\" (out of scope category) if none of the other categories apply.\n\nBelow are several examples to guide your classification:\n\n---\n{fewshot_examples}\n---\n\n===============================\n\nNew Question: {question}\n\n===============================\n\nProvide your final classification in **valid JSON format** with the following structure:\n{{\n  \"category\": \"your_chosen_category_name\",\n  \"confidence\": confidence_level_rounded_to_the_nearest_2_decimal_places\n}}\n\n\nEnsure the JSON has:\n- Opening and closing curly braces\n- Double quotes around keys and string values\n- Confidence as a number (not a string), with maximum 2 decimal places\n\nDo not include any explanations or extra text.\n            '''\n    return prompt\n\n\n\n#######################\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T08:15:25.727903Z","iopub.execute_input":"2025-07-11T08:15:25.728229Z","iopub.status.idle":"2025-07-11T08:15:31.555894Z","shell.execute_reply.started":"2025-07-11T08:15:25.728205Z","shell.execute_reply":"2025-07-11T08:15:31.554823Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Loaded dataset into dataframe: banking\nDimensions: (13083, 4)\nCol names: Index(['text', 'label', 'dataset', 'split'], dtype='object')\n================================================================================\nOriginal dataset intents: ['Refund_not_showing_up', 'activate_my_card', 'age_limit', 'apple_pay_or_google_pay', 'atm_support', 'automatic_top_up', 'balance_not_updated_after_bank_transfer', 'balance_not_updated_after_cheque_or_cash_deposit', 'beneficiary_not_allowed', 'cancel_transfer', 'card_about_to_expire', 'card_acceptance', 'card_arrival', 'card_delivery_estimate', 'card_linking', 'card_not_working', 'card_payment_fee_charged', 'card_payment_not_recognised', 'card_payment_wrong_exchange_rate', 'card_swallowed', 'cash_withdrawal_charge', 'cash_withdrawal_not_recognised', 'change_pin', 'compromised_card', 'contactless_not_working', 'country_support', 'declined_card_payment', 'declined_cash_withdrawal', 'declined_transfer', 'direct_debit_payment_not_recognised', 'disposable_card_limits', 'edit_personal_details', 'exchange_charge', 'exchange_rate', 'exchange_via_app', 'extra_charge_on_statement', 'failed_transfer', 'fiat_currency_support', 'get_disposable_virtual_card', 'get_physical_card', 'getting_spare_card', 'getting_virtual_card', 'lost_or_stolen_card', 'lost_or_stolen_phone', 'order_physical_card', 'passcode_forgotten', 'pending_card_payment', 'pending_cash_withdrawal', 'pending_top_up', 'pending_transfer', 'pin_blocked', 'receiving_money', 'request_refund', 'reverted_card_payment?', 'supported_cards_and_currencies', 'terminate_account', 'top_up_by_bank_transfer_charge', 'top_up_by_card_charge', 'top_up_by_cash_or_cheque', 'top_up_failed', 'top_up_limits', 'top_up_reverted', 'topping_up_by_card', 'transaction_charged_twice', 'transfer_fee_charged', 'transfer_into_account', 'transfer_not_received_by_recipient', 'transfer_timing', 'unable_to_verify_identity', 'verify_my_identity', 'verify_source_of_funds', 'verify_top_up', 'virtual_card_not_working', 'visa_or_mastercard', 'why_verify_identity', 'wrong_amount_of_cash_received', 'wrong_exchange_rate_for_cash_withdrawal']\nNumber of original intents: 77\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efc821d8f7994d13b435d770655d46d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"stackoverflow_idx2label.csv:   0%|          | 0.00/224 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e206f73add7347d2a0121572bf352e00"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"clinc150_oos_idx2label.csv: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98d30aa48eb34b3284a2641799efa9a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"banking77_idx2label.csv: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53cd0146f7e34f76acdcffda8d009de8"}},"metadata":{}},{"name":"stdout","text":"================================================================================\nOriginal intents to convert to OOS class\n    index                                    label\n0       2                  apple_pay_or_google_pay\n1      10                          card_acceptance\n2      14                         card_not_working\n3      15                 card_payment_fee_charged\n4      16              card_payment_not_recognised\n5      17         card_payment_wrong_exchange_rate\n6      19                   cash_withdrawal_charge\n7      25                    declined_card_payment\n8      31                          exchange_charge\n9      32                            exchange_rate\n10     33                         exchange_via_app\n11     34                extra_charge_on_statement\n12     36                    fiat_currency_support\n13     52                           request_refund\n14     53                   reverted_card_payment?\n15     54           supported_cards_and_currencies\n16     57                    top_up_by_card_charge\n17     73                       visa_or_mastercard\n18     76  wrong_exchange_rate_for_cash_withdrawal\nPercentage of original intents to convert to OOS class: 0.24675324675324675\n\n================================================================================\nUnique intents after converting some to OOS class\n['Refund_not_showing_up', 'activate_my_card', 'age_limit', 'atm_support', 'automatic_top_up', 'balance_not_updated_after_bank_transfer', 'balance_not_updated_after_cheque_or_cash_deposit', 'beneficiary_not_allowed', 'cancel_transfer', 'card_about_to_expire', 'card_arrival', 'card_delivery_estimate', 'card_linking', 'card_swallowed', 'cash_withdrawal_not_recognised', 'change_pin', 'compromised_card', 'contactless_not_working', 'country_support', 'declined_cash_withdrawal', 'declined_transfer', 'direct_debit_payment_not_recognised', 'disposable_card_limits', 'edit_personal_details', 'failed_transfer', 'get_disposable_virtual_card', 'get_physical_card', 'getting_spare_card', 'getting_virtual_card', 'lost_or_stolen_card', 'lost_or_stolen_phone', 'oos', 'order_physical_card', 'passcode_forgotten', 'pending_card_payment', 'pending_cash_withdrawal', 'pending_top_up', 'pending_transfer', 'pin_blocked', 'receiving_money', 'terminate_account', 'top_up_by_bank_transfer_charge', 'top_up_by_cash_or_cheque', 'top_up_failed', 'top_up_limits', 'top_up_reverted', 'topping_up_by_card', 'transaction_charged_twice', 'transfer_fee_charged', 'transfer_into_account', 'transfer_not_received_by_recipient', 'transfer_timing', 'unable_to_verify_identity', 'verify_my_identity', 'verify_source_of_funds', 'verify_top_up', 'virtual_card_not_working', 'why_verify_identity', 'wrong_amount_of_cash_received']\nNumber of unique intents after converting some to OOS class: 59\n\n================================================================================\nsanity check\nNumber of original intents: 77\nNumber of original intents + 1 OOS class (if doesnt exist in original dataset): 78\nNumber of original intents to convert to OOS class: 19\nPercentage of original intents to convert to OOS class: 0.24675324675324675\nNumber of unique intents after converting some to OOS class: 59\nNumber of original intents + 1 OOS class (if doesnt exist in original dataset) - converted classes: 59\nNumbers match: True\nPrepared unique intents\nFiltered dataset\nDimensions: (13083, 4)\nCol names: Index(['text', 'label', 'dataset', 'split'], dtype='object')\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 62 files:   0%|          | 0/62 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dce025c626f045fa9f64deb27b0e4c26"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"banking_only15notoos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5afa447435b048f3b994670dc3c4b536"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"banking_only35notoos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64846964bebb4e73831299b9dd353a2a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"banking_only10notoos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"437015a8a7414cf6a5c2577d21dab7b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"banking_only3notoos.txt:   0%|          | 0.00/472 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"694b4e86b93645ffbd60edfa1090788e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"banking_only1notoos.txt:   0%|          | 0.00/177 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c1814f59d014f7498c2057810c204f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"banking_only20notoos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13a48c1544954319aa6c92b6bce939cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"banking_only5notoos.txt:   0%|          | 0.00/835 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"389d014e06424e25a23cf74e3f0764b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"banking_only2notoos.txt:   0%|          | 0.00/419 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a04403ac9e240d2ae3c61e1472a0cc0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"banking_only30notoos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0abb48121a646e9a65068fd218f08c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"banking_only4notoos.txt:   0%|          | 0.00/651 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2bfda8b8ee1423dbac5852f20a85e90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"banking_only50notoos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54e3d001a84a48668261bc41f6ebd3d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"banking_only60notoos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"624f19da0c914b108e7175da1c340241"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"banking_only40notoos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de3f652e0bc740c1b951b4f89b9359d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"banking_only70notoos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fea1e248a21479bb52fb9317074368b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"oos_12notoos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee1288a88eac403c891551b053efc7d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"oos_11notoos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c616571c6b447f1884b0a1a69e154fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"oos_100notoos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b874ac0b7fd449deb3666d2cfd3ea283"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"oos_120notoos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab74c188a8a44954a29b1df9a7ddbf7c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"oos_10notoos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc9194fd2b634b5bb81f475c1bdbbfa2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"oos_13notoos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"120ec32a439948859f2b348b1cdfb810"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"oos_140notoos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef6d3df8969840a1833269f2fed7bf15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"oos_14notoos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a6cb1f946874f4c8d04c4a02889c308"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"oos_1notoos.txt:   0%|          | 0.00/121 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e6c0c499ca74e4a979360fec76850b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"oos_20notoos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b5fa5b6c34d428184ef5745a49bc6a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"oos_15notoos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2afb9dfc3144b77ad3b3326ea2e7af9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"oos_2notoos.txt:   0%|          | 0.00/259 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"854fb5b6b8484f82bc9ec40c7ffc5d89"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"oos_40notoos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ed05afc6e90446099f0738986aac458"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"oos_30notoos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a229a2b52d2f4d48ba1f954cf37e727c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"oos_3notoos.txt:   0%|          | 0.00/387 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a8a339820714787a2502b60f71a5a1f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"oos_4notoos.txt:   0%|          | 0.00/537 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e2cc2a845ca459ba61184db4175f0fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"oos_5notoos.txt:   0%|          | 0.00/731 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c33060f339f4bcaba5db5229c426ee6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"stackoverflow_only10notoos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7342e5f639a54ea387306f52b185b88d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"oos_75notoos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"adaba880df8b4651bb15e6ef73fd25c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"oos_50notoos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e3f9909b37e456fa0d122121f661273"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"stackoverflow_only16notoos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"157d8886fe964476bc51b144bc2c4e56"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"stackoverflow_only14notoos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e92838e0519496b93a03c2f0db7f646"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"stackoverflow_only12notoos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"871f4a56797643d7ac4e1d28fe7406e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"stackoverflow_only18notoos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"062c2e707ef44507b5f2e5bab9ac7ec4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"stackoverflow_only1notoos.txt:   0%|          | 0.00/132 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c921143e86744d49e66e1560f84e4c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"stackoverflow_only2notoos.txt:   0%|          | 0.00/288 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50dbb18c5b7c474e869c7b58bc43dab7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"stackoverflow_only3notoos.txt:   0%|          | 0.00/420 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6180d1590a284cf990843181fa36b5a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"stackoverflow_only6notoos.txt:   0%|          | 0.00/884 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70bee842f79547c099ba1789bb0f6950"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"stackoverflow_only4notoos.txt:   0%|          | 0.00/591 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f89264ba9d7468196e4438a3de62c0b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"stackoverflow_only5notoos.txt:   0%|          | 0.00/715 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f658685a2dca40a5b5a28f77c03ba014"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"stackoverflow_only8notoos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca2b97a8fdfe41f9a3fc123cf4c32ca5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"oos_25perc_oos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b00ee50463cb4d8ca95b0063964c5ae1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"banking_25perc_oos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe90a65dc31a4c5392087d29fb9b52de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"logs_cpu_2025.05.22%20round1.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"935056d2d8ce41338d8fdee5409bde88"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"stackoverflow_25perc_oos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33a16e386ecc4c3099b7609ab3716577"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"stackoverflow_only2notoos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9222e05d36954cc0865360dfa891f4df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)u_2025.05.22%20round3%20parallelise1.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43d472ec80194b7298511a762e601aab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"logs_cpu_2025.05.22%20round2.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9da100f2fef34acb96f4c67f9ee56667"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"logs_gpu_2025.05.22.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a99dd01a105d44158b1c413eb54535f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)5.05.22%20round2%20w%20start-end-idx.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5a5eede89a440a9b4b0296b47913920"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)u_2025.05.22%20round4%20parallelise2.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ac61df6342643468341c7f5f2cf33f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)ion_report_llama3.2_3b_stackoverflow.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcf22badf8b64c4cad6cb7cae70638c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)lassification_report_llama3.2_3b_oos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b88d6b4cd0c4e848079f24e51905787"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)ification_report_llama3.2_3b_banking.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b3a93739b5e4a4da0d107125cde91ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)lama3.2_3b_stackoverflow_only2notoos.txt:   0%|          | 0.00/380 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed6fad913cbd447e992575689a1ebd27"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)tion_report_llama3.2_3b_banking_full.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7638f1c4f374d9b853cce67b9764112"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)fication_report_llama3.2_3b_oos_full.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71fe639aebcc472a806bfccf43b45201"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)eport_llama3.2_3b_stackoverflow_full.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf112da4f9074aad99af04defa41a343"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"len(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T08:15:54.133922Z","iopub.execute_input":"2025-07-11T08:15:54.134275Z","iopub.status.idle":"2025-07-11T08:15:54.140921Z","shell.execute_reply.started":"2025-07-11T08:15:54.134243Z","shell.execute_reply":"2025-07-11T08:15:54.139609Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"13083"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T08:15:56.147984Z","iopub.execute_input":"2025-07-11T08:15:56.148898Z","iopub.status.idle":"2025-07-11T08:15:56.172335Z","shell.execute_reply.started":"2025-07-11T08:15:56.148863Z","shell.execute_reply":"2025-07-11T08:15:56.171480Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"                                                    text  \\\n0         Could you help my figure out the exchange fee?   \n1      I made a cash deposit to my account but i don'...   \n2      Hello - I'm on the app and trying to purchase ...   \n3             Why is it saying I have a pending payment?   \n4      Is there an extra charge to exchange different...   \n...                                                  ...   \n13078  I cannot locate the verification code for my t...   \n13079      If I make a top-up are there charges applied?   \n13080                           What is the minimum age?   \n13081  Is there a reason why my transaction is taking...   \n13082                im not sure what this charge is for   \n\n                                                  label  dataset  split  \n0                                       exchange_charge  banking  train  \n1      balance_not_updated_after_cheque_or_cash_deposit  banking  train  \n2                               beneficiary_not_allowed  banking  train  \n3                                  pending_card_payment  banking  train  \n4                                       exchange_charge  banking  train  \n...                                                 ...      ...    ...  \n13078                                     verify_top_up  banking   test  \n13079                    top_up_by_bank_transfer_charge  banking   test  \n13080                                         age_limit  banking   test  \n13081                transfer_not_received_by_recipient  banking   test  \n13082                          card_payment_fee_charged  banking   test  \n\n[13083 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n      <th>dataset</th>\n      <th>split</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Could you help my figure out the exchange fee?</td>\n      <td>exchange_charge</td>\n      <td>banking</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I made a cash deposit to my account but i don'...</td>\n      <td>balance_not_updated_after_cheque_or_cash_deposit</td>\n      <td>banking</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Hello - I'm on the app and trying to purchase ...</td>\n      <td>beneficiary_not_allowed</td>\n      <td>banking</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Why is it saying I have a pending payment?</td>\n      <td>pending_card_payment</td>\n      <td>banking</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Is there an extra charge to exchange different...</td>\n      <td>exchange_charge</td>\n      <td>banking</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>13078</th>\n      <td>I cannot locate the verification code for my t...</td>\n      <td>verify_top_up</td>\n      <td>banking</td>\n      <td>test</td>\n    </tr>\n    <tr>\n      <th>13079</th>\n      <td>If I make a top-up are there charges applied?</td>\n      <td>top_up_by_bank_transfer_charge</td>\n      <td>banking</td>\n      <td>test</td>\n    </tr>\n    <tr>\n      <th>13080</th>\n      <td>What is the minimum age?</td>\n      <td>age_limit</td>\n      <td>banking</td>\n      <td>test</td>\n    </tr>\n    <tr>\n      <th>13081</th>\n      <td>Is there a reason why my transaction is taking...</td>\n      <td>transfer_not_received_by_recipient</td>\n      <td>banking</td>\n      <td>test</td>\n    </tr>\n    <tr>\n      <th>13082</th>\n      <td>im not sure what this charge is for</td>\n      <td>card_payment_fee_charged</td>\n      <td>banking</td>\n      <td>test</td>\n    </tr>\n  </tbody>\n</table>\n<p>13083 rows × 4 columns</p>\n</div>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"######################\n# preprocess batch outputs from Nebius\n# then export to intermediate.json\n######################\nimport json\n\ndef jsonl_to_json_extract_content(jsonl_file_path, json_file_path):\n    \"\"\"\n    Reads a JSONL file, extracts LLM message content, and saves to a JSON file.\n\n    Args:\n        jsonl_file_path (str): The path to the input JSONL file.\n        json_file_path (str): The path to the output JSON file.\n    \"\"\"\n    extracted_data = []\n    with open(jsonl_file_path, 'r') as infile:\n        for line in infile:\n            record_dict = {}\n\n            data = json.loads(line)\n            # Extract content from the nested structure\n            record_dict[\"Index\"] = data[\"custom_id\"]\n\n            content = json.loads(data[\"response\"][\"choices\"][0][\"message\"][\"content\"])\n            record_dict[\"predicted\"] = content[\"category\"]\n            record_dict[\"confidence\"] = content[\"confidence\"]\n            # Remove backslashes and newline characters\n            # content = content.replace('\\\\', '').replace('\\n', '').strip()\n            extracted_data.append(record_dict)\n\n    with open(json_file_path, 'w') as outfile:\n        json.dump(extracted_data, outfile, indent=2)\n\n# Example usage\njsonl_file_path = '/kaggle/input/01l7-openintent-nebiusqwen-banking77-batchfull/batch_outputs_banking_0_None.jsonl'\njson_file_path = 'batch_outputs_banking77_intermediate.json'\njsonl_to_json_extract_content(jsonl_file_path, json_file_path)\n\nprint(f\"Extracted content saved to {json_file_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T08:20:12.981291Z","iopub.execute_input":"2025-07-11T08:20:12.981623Z","iopub.status.idle":"2025-07-11T08:20:13.265381Z","shell.execute_reply.started":"2025-07-11T08:20:12.981599Z","shell.execute_reply":"2025-07-11T08:20:13.264419Z"}},"outputs":[{"name":"stdout","text":"Extracted content saved to batch_outputs_banking77_intermediate.json\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"######################\n# import intermediate.json\n# then stitch data with original df \n# to trace and retrieve original text qn and label/class\n######################\nimport pandas as pd\nimport json\n\n# Load JSON data\nwith open('/kaggle/working/batch_outputs_banking77_intermediate.json', 'r') as f:\n    json_data = json.load(f)  # List of dictionaries\n\n# Create lookup dictionary: {Index: {predicted, confidence}}\njson_lookup = {item[\"Index\"]: item for item in json_data}  # Keys are strings\n\n# Initialize results list\nresults = []\n\n# Iterate through DataFrame rows\nfor row in df.itertuples():\n    # Convert DataFrame Index to string for consistent lookup\n    idx = row.Index\n    \n    # Retrieve prediction if exists, else None\n    pred_entry = json_lookup.get(str(idx))\n    \n    # Build result dictionary\n    results.append({\n        \"Index\": row.Index,\n        \"text\": row.text,\n        \"label\": row.label,\n        \"dataset\": row.dataset,\n        \"split\": row.split,\n        \"predicted\": pred_entry['predicted'],\n        \"confidence\": pred_entry['confidence']\n    })\n\n# Now `results` contains your combined data\nsubset_results = results\n\n#################################################\n# export to JSON in the format we expect\n#################################################\nmodel_name = Config.model_name\ndf = df\ncategories = bulletpts_intent\nstart_index = Config.start_index\nend_index = Config.end_index\nlog_every_n_examples = Config.log_every_n_examples\nend_index = max(r['Index'] for r in subset_results)\n\nwith open(f'results_{Config.model_name}_{Config.dataset_name}_{Config.start_index}_{end_index}.json', 'w') as f:\n    json.dump(subset_results, f, indent=2)\n\nprint(\"Completed intent classification\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T08:28:02.186493Z","iopub.execute_input":"2025-07-11T08:28:02.186827Z","iopub.status.idle":"2025-07-11T08:28:02.365975Z","shell.execute_reply.started":"2025-07-11T08:28:02.186805Z","shell.execute_reply":"2025-07-11T08:28:02.365094Z"}},"outputs":[{"name":"stdout","text":"Completed intent classification\n","output_type":"stream"}],"execution_count":21}]}
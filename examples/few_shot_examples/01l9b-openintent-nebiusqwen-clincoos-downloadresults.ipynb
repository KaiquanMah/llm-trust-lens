{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Refactored code for\n* Setting up and running Ollama in Kaggle\n* Downloading THUIAR dataset\n* Zero-Shot Prompt\n* Use LLM to classify intent from an input 'question' dataset\n* To configure your file/folder paths, LLM, dataset, start_index and end_index for each run, please update the config.py file\n\nThis notebook will also be used as the base to test any fixes to the LLM intent classification pipeline.\n* 2025.05.26: Updated results output file from JSON to Pickle, to store list of dictionaries. 1 dictionary contains the results for each record. Lists of dictionaries can be downloaded from multiple notebooks, then concatenated for analysis\n* 2025.05.30: Update prompt and bulletpts_intent.\n  * Check if dataset contains 'oos' (out of scope) category\n  * If dataset has no 'oos' (out of scope) category, turn 1 category into 'oos'. Use updated categories in bulletpts_intent. Also update prompt instructions on when to classify an example as 'oos'\n  * **This force_oos fix is implemented in [notebook 01E](https://www.kaggle.com/code/kaiquanmah/01e-kaggle-ollama-llama3-2-w-force-oos?scriptVersionId=242648764)**\n* 2025.05.30: Add pydantic schema with enums\n  * From an analysis of errors, the model previously had a 45% average accuracy rate across categories. The model predicted a set of categories outside of what we gave it in 'bulletpts_intent'\n  * To fix this, we will try to implement a pydantic schema solution for the model to only predict categories from the allowed list of categories ('bulletpts_intent')\n* 2025.05.30: Set Ollama chat temperature to 0\n  * Previously, we used the default temperature of 0.8, which might have caused the model to predict categories we did not provide to it ([Reading](https://docs.spring.io/spring-ai/reference/api/chat/ollama-chat.html))\n  * **The pydantic schema and temperature fixes are implemented in [notebook 01F](https://www.kaggle.com/code/kaiquanmah/01f-kaggle-ollama-llama3-2-w-pydantic-schema)**\n* 2025.06.03:\n  1. **Remove 'oos' from `bulletpts_intent` input into prompt**, to be consistent with the team's approach when exploring embedding approaches to classify 'oos' examples. **Keep 'oos' in pydantic enums/Literal (for LLM to output 'oos' as an allowed class value)**\n  2. **Remove 0.99 when defining the prompt format - to avoid anchoring LLM on outputting confidence of 0.99**\n  3. **Added ability for user to define which classes are 'oos'**\n  * **These 3 fixes are in [notebook 01G](https://www.kaggle.com/code/kaiquanmah/01g-kaggle-ollama-llama3-2-oos-update)**\n* 2025.06.10:\n  * From an error analysis earlier, **models can get confused between similar intent classes**\n  * Therefore **we will analyse similar intent classes/labels -> get their indexes -> put them into 'oos' in [notebook 01H](https://www.kaggle.com/code/kaiquanmah/01h1-openintent-ollama-llama3-2-3b-banking77)**\n  * **Going from zero-shot prompt previously, to few-shot prompt (with 5 examples) from known intents**. These 5 examples were **non-oos, and misclassified previously**. This 'fix' is in **[notebook 01i](https://www.kaggle.com/code/kaiquanmah/01i1-openintent-ollama-llama3-2-3b-banking77)**\n* 2025.06.16:\n  * For known intents (ie not in the 'oos' class), give 5 examples each in the few-shot prompt **[notebook 01J](https://www.kaggle.com/code/kaiquanmah/01j1-openintent-ollama-llama3-2-3b-banking77)**\n* 2025.06.17:\n  * Now we explore how changing the number of known intent classes affects the recall of oos in **[notebook 01K](https://www.kaggle.com/code/kaiquanmah/01k1-openintent-ollama-llama3-2-3b-banking77)**\n  * For quick experimentation, we implement (1) fewshot prompt with 1 example per known intent class, (2) changing number of known intent classes in various notebook runs, (3) 100 oos sentences for the model to classify (taking from first class for banking77 and stackoverflow dataset, or the oos class for CLINC150 oos dataset)\n    * For (3) - Added 'first_class' variable for each dataset to Config\n    * For (3) - Created new fn to filter and keep 100 records from 'first/oos class' to input to the model to classify\n* 2025.07.07:\n  * Explore free, rate-limited API model (such as Gemini) in **[notebook 01L](https://www.kaggle.com/code/kaiquanmah/01l1-openintent-gemini-banking77-explore)**\n  * Added retry for when we exhaust API limits per minute\n  * Updated end_index tracking that works with Ollama and Gemini when generating JSON results file\n  * **Explore Qwen model from the Nebius platform**","metadata":{}},{"cell_type":"code","source":"# 1. create dirs if they do not exist\nimport os\nos.makedirs('/kaggle/working/src', exist_ok=True)\nos.makedirs('/kaggle/working/prediction', exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T06:42:08.482117Z","iopub.execute_input":"2025-07-11T06:42:08.482548Z","iopub.status.idle":"2025-07-11T06:42:08.492230Z","shell.execute_reply.started":"2025-07-11T06:42:08.482521Z","shell.execute_reply":"2025-07-11T06:42:08.491353Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"%%writefile /kaggle/working/src/setup_ollama.py\nimport os\nimport subprocess\nimport time\nfrom src.config import Config # absolute import\n\n# 1. Install Ollama (if not already installed)\ntry:\n    # Check if Ollama is already installed\n    subprocess.run([\"ollama\", \"--version\"], capture_output=True, check=True)\n    print(\"Ollama is already installed.\")\nexcept FileNotFoundError:\n    print(\"Installing Ollama...\")\n    subprocess.run(\"curl -fsSL https://ollama.com/install.sh  | sh\", shell=True, check=True)\n\n# 2. Start Ollama server in the background\nprint(\"Starting Ollama server...\")\nprocess = subprocess.Popen(\"ollama serve\", shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n\n# Wait for the server to initialize\ntime.sleep(5)\n\n\n# 3. Pull the model\nmodel_name = Config.model_name\nprint(f\"Pulling {model_name} model...\")\nsubprocess.run([\"ollama\", \"pull\", model_name], check=True)\n\n# 4. Install Python client\nsubprocess.run([\"pip\", \"install\", \"ollama\"], check=True)\n\nprint(\"Ollama setup complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T06:42:08.493785Z","iopub.execute_input":"2025-07-11T06:42:08.494083Z","iopub.status.idle":"2025-07-11T06:42:08.514358Z","shell.execute_reply.started":"2025-07-11T06:42:08.494060Z","shell.execute_reply":"2025-07-11T06:42:08.513446Z"}},"outputs":[{"name":"stdout","text":"Writing /kaggle/working/src/setup_ollama.py\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"%%writefile requirements.txt\npandas\npydantic\ntyping\nhuggingface-hub\n# google-genai # only used for gemini model\nopenai # used for openrouter's gemini model\ntenacity # for gemini model retries\n# numpy\n# enum","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T06:42:08.515074Z","iopub.execute_input":"2025-07-11T06:42:08.515423Z","iopub.status.idle":"2025-07-11T06:42:08.533696Z","shell.execute_reply.started":"2025-07-11T06:42:08.515392Z","shell.execute_reply":"2025-07-11T06:42:08.532696Z"}},"outputs":[{"name":"stdout","text":"Writing requirements.txt\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"%%writefile /kaggle/working/src/__init__.py\n# folder for config","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T06:42:08.535650Z","iopub.execute_input":"2025-07-11T06:42:08.535889Z","iopub.status.idle":"2025-07-11T06:42:08.557893Z","shell.execute_reply.started":"2025-07-11T06:42:08.535871Z","shell.execute_reply":"2025-07-11T06:42:08.557132Z"}},"outputs":[{"name":"stdout","text":"Writing /kaggle/working/src/__init__.py\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"%%writefile /kaggle/working/src/config.py\nclass Config:\n    #######################################################\n    # working directory for files\n    #######################################################\n    target_dir = '/kaggle/working/data' # data directory to clone into\n    cloned_data_dir = target_dir + '/data'\n    prediction_dir = target_dir + '/prediction'\n    #######################################################\n    # dataset and model\n    #######################################################\n    dataset_name = 'oos' # UPDATE options: 'banking', 'stackoverflow', 'oos'\n    idx2label_target_dir = '/kaggle/working/idx2label'\n    idx2label_filename_hf = 'clinc150_oos_idx2label.csv' # UPDATE options: banking77_idx2label.csv, stackoverflow_idx2label.csv, clinc150_oos_idx2label.csv\n    fewshot_examples_dir = '/kaggle/working/fewshot'\n    fewshot_subdir = '/fewshot-5examples-per-nonoos/'\n    fewshot_examples_filename = 'oos_25perc_oos.txt' # UPDATE options: banking_25perc_oos.txt, stackoverflow_25perc_oos.txt, oos_25perc_oos.txt\n    list_oos_idx = [2, 7, 12, 20, 27, 28, 35, 38, 41, 49, 51, 56, 58, 59, 61, 64, 66, 68, 69, 70, 71, 74, 79, 87, 89, 90, 91, 93, 100, 103, 108, 109, 111, 133, 137, 143] # UPDATE gathered from within the team - for reproducible, comparable results with other open intent classification approaches\n    model_name = 'Qwen3-30B-A3B' # 'gemma-2-9b-it-fast'\n    start_index=0 # eg: 0, 10001, 11851\n    end_index=None # eg: 10, 10000, 11850 or None (use end_index=None to process the full dataset)\n    log_every_n_examples=10 # 2\n    force_oos = True  # NEW: Add flag to force dataset to contain 'oos' class for the last class value (sorted alphabetically), if 'oos' class does not exist in the original dataset\n    #######################################################\n    # evaluate threshold when 'oos' recall drops\n    #######################################################\n    filter_oos_qns_only = False # True (when you are testing 'oos' recall threshold), False\n    n_oos_qns = 100\n    first_class_banking = 'activate_my_card' # following idx2label\n    first_class_stackoverflow = 'wordpress' # following idx2label\n    first_class_oos = 'oos'\n    #######################################################","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T06:42:08.558790Z","iopub.execute_input":"2025-07-11T06:42:08.559033Z","iopub.status.idle":"2025-07-11T06:42:08.578979Z","shell.execute_reply.started":"2025-07-11T06:42:08.559014Z","shell.execute_reply":"2025-07-11T06:42:08.578035Z"}},"outputs":[{"name":"stdout","text":"Writing /kaggle/working/src/config.py\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"%%writefile download_dataset.py\nfrom src.config import Config\nimport os\nimport subprocess\ntarget_dir = Config.target_dir # data directory to clone into\ncloned_data_dir = Config.cloned_data_dir\n\n# Create target directory if it doesn't exist\nos.makedirs(target_dir, exist_ok=True)\n\n# do not clone dataset repo if cloned data folder exists\nif os.path.exists(cloned_data_dir):\n    print(\"Dataset has already been downloaded. If this is incorrect, please delete the Adaptive-Decision-Boundary 'data' folder.\")\nelse:\n    # Clone the repository\n    subprocess.run([\"git\",\n                    \"clone\",\n                    \"https://github.com/thuiar/Adaptive-Decision-Boundary.git\",\n                    target_dir\n                   ])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T06:42:08.579940Z","iopub.execute_input":"2025-07-11T06:42:08.580240Z","iopub.status.idle":"2025-07-11T06:42:08.605086Z","shell.execute_reply.started":"2025-07-11T06:42:08.580221Z","shell.execute_reply":"2025-07-11T06:42:08.603902Z"}},"outputs":[{"name":"stdout","text":"Writing download_dataset.py\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"%%writefile predict_class.py\nfrom src.config import Config\nimport pandas as pd\nimport os\n# import ollama\nimport json\nimport pickle\nimport time\nfrom pydantic import BaseModel\nfrom typing import Literal\n# from enum import Enum\nfrom huggingface_hub import snapshot_download\n    \n###################\n# Gemini API\n###################\n# from google import genai\n# from google.genai.types import ThinkingConfig\n# from google.api_core import retry\nfrom openai import OpenAI\nfrom tenacity import retry, stop_after_attempt, wait_fixed\nfrom kaggle_secrets import UserSecretsClient\n\n\n###################\n\n\n# Config.target_dir\n# Config.cloned_data_dir'\n# Config.dataset_name\n# Config.model_name\n# Config.start_index\n# Config.end_index\n# Config.log_every_n_examples\n\n\n#######################\n# load data\n#######################\ndef load_data(data_dir):\n    \"\"\"Loads train, dev, and test datasets from a specified directory.\"\"\"\n\n    main_df = pd.DataFrame()\n    for split in ['train', 'dev', 'test']:\n        file_path = os.path.join(data_dir, f'{split}.tsv')\n        if os.path.exists(file_path):\n          try:\n            df = pd.read_csv(file_path, sep='\\t')\n            df['dataset'] = os.path.basename(data_dir)\n            df['split'] = split\n            main_df = pd.concat([main_df, df], ignore_index=True)\n          except pd.errors.ParserError as e:\n            print(f\"Error parsing {file_path}: {e}\")\n            # Handle the error appropriately, e.g., skip the file, log the error, etc.\n        else:\n            print(f\"Warning: {split}.tsv not found in {data_dir}\")\n    return main_df\n\n\ndef filter100examples_oos(dataset_name, df):\n    # dont input 'only oos qns to model'\n    if Config.filter_oos_qns_only == False:\n        filtered_df = df\n    # vs\n    # input 'only oos qns to model'\n    else:\n        if dataset_name == 'banking':\n            first_class = Config.first_class_banking\n        elif dataset_name == 'stackoverflow':\n            first_class = Config.first_class_stackoverflow\n        else:\n            first_class = Config.first_class_oos\n    \n        filtered_df = df.copy()\n        filtered_df = filtered_df.loc[filtered_df[\"label\"] == first_class]\n        filtered_df = filtered_df.sample(n=Config.n_oos_qns, random_state=38)\n    return filtered_df\n\n\ndf = pd.DataFrame()\n\ndata_dir = os.path.join(Config.cloned_data_dir, Config.dataset_name)\nif os.path.exists(data_dir):\n  df = load_data(data_dir)\n  print(f\"Loaded dataset into dataframe: {Config.dataset_name}\")\n  print(f\"Dimensions: {df.shape}\")\n  print(f\"Col names: {df.columns}\")\nelse:\n  print(f\"Warning: Directory {data_dir} not found.\")\n#######################\n\n\n\n#######################\n# unique intents\n#######################\nsorted_intent = list(sorted(df.label.unique()))\nprint(\"=\"*80)\nprint(f\"Original dataset intents: {sorted_intent}\")\nprint(f\"Number of original intents: {len(sorted_intent)}\\n\")\n\n\n# 2025.06.03\n# New OOS approach - get 25/50/75% of class indexes for each dataset within the team (for reproducibility and comparable results)\n# Change their class labels to 'oos'\nsnapshot_download(repo_id=\"KaiquanMah/open-intent-query-classification\", repo_type=\"space\", allow_patterns=\"*_idx2label.csv\", local_dir=Config.idx2label_target_dir)\nidx2label_filepath = Config.idx2label_target_dir + '/dataset_idx2label/' + Config.idx2label_filename_hf\nidx2label = pd.read_csv(idx2label_filepath)\nidx2label_oos = idx2label[idx2label.index.isin(Config.list_oos_idx)]\nidx2label_oos.reset_index(drop=True, inplace=True)\n\n# 2025.06.17 keep track of non-oos labels, to use in IntentSchema\nnonoos_labels = idx2label[~idx2label.label.isin(Config.list_oos_idx)]['label'].values\nprint(\"=\"*80)\nprint(\"Original intents to convert to OOS class\")\nprint(idx2label_oos)\nprint(f\"Percentage of original intents to convert to OOS class: {len(idx2label_oos)/len(idx2label)}\\n\")\n\noos_labels = idx2label_oos['label'].values\nlist_sorted_intent_aft_conversion = ['oos' if intent.lower() in oos_labels else intent for intent in sorted_intent]\nlist_sorted_intent_aft_conversion_deduped = sorted(set(list_sorted_intent_aft_conversion))\nprint(\"=\"*80)\nprint(\"Unique intents after converting some to OOS class\")\nprint(list_sorted_intent_aft_conversion_deduped)\nprint(f\"Number of unique intents after converting some to OOS class: {len(list_sorted_intent_aft_conversion_deduped)}\\n\")\n\n\n\n# unique intents - from set to bullet points (to use in prompts)\n# bulletpts_intent = \"\\n\".join(f\"- {category}\" for category in set_intent)\n# 2025.06.03: do not show 'oos' in the prompt (to avoid leakage of 'oos' class)\nbulletpts_intent = \"\\n\".join(f\"- {category}\" for category in list_sorted_intent_aft_conversion_deduped if category and (category!='oos'))\n\n# 2025.06.04: fix adjustment if 'oos' is already in the original dataset\nint_oos_in_orig_dataset = int('oos' in idx2label.label.values)\nadjust_if_oos_not_in_orig_dataset = [0 if int_oos_in_orig_dataset == 1 else 1][0]\n\nprint(\"=\"*80)\nprint(\"sanity check\")\nprint(f\"Number of original intents: {len(sorted_intent)}\")\nprint(f\"Number of original intents + 1 OOS class (if doesnt exist in original dataset): {len(sorted_intent) + adjust_if_oos_not_in_orig_dataset}\")\nprint(f\"Number of original intents to convert to OOS class: {len(idx2label_oos)}\")\nprint(f\"Percentage of original intents to convert to OOS class: {len(idx2label_oos)/len(idx2label)}\")\nprint(f\"Number of unique intents after converting some to OOS class: {len(list_sorted_intent_aft_conversion_deduped)}\")\nprint(f\"Number of original intents + 1 OOS class (if doesnt exist in original dataset) - converted classes: {len(sorted_intent) + adjust_if_oos_not_in_orig_dataset - len(idx2label_oos)}\")\nprint(f\"Numbers match: {(len(sorted_intent) + adjust_if_oos_not_in_orig_dataset - len(idx2label_oos)) == len(list_sorted_intent_aft_conversion_deduped)}\")\nprint(\"Prepared unique intents\")\n#######################\n\n\n\n\n#######################\n# Enforce schema on the model (e.g. allowed list of predicted categories)\n#######################\n\nclass IntentSchema(BaseModel):\n    # dynamically unpack list of categories for different dataset(s)\n    category: Literal[*list_sorted_intent_aft_conversion_deduped]\n    confidence: float\n    \n#######################\n\n\n\n\n#######################\n# filter after preparing intents\n#######################\ndf = filter100examples_oos(Config.dataset_name, df)\nprint(\"Filtered dataset\")\nprint(f\"Dimensions: {df.shape}\")\nprint(f\"Col names: {df.columns}\")\n#######################\n\n\n\n#######################\n# Prompt\n#######################\n# prompt 2 with less information/compute, improve efficiency\n# 2025.06.10 prompt 3 with 5 few shot examples only - notebook O1H1, O1i1\n# 2025.06.16 prompt 4 with 5 examples per each known intent (ie non-oos intent) - notebook 01J1\nsnapshot_download(repo_id=\"KaiquanMah/open-intent-query-classification\", repo_type=\"space\", allow_patterns=\"*.txt\", local_dir=Config.fewshot_examples_dir)\nwith open(Config.fewshot_examples_dir + Config.fewshot_subdir + Config.fewshot_examples_filename, 'r') as file:\n    fewshot_examples = file.read()\n\ndef get_prompt(dataset_name, split, question, categories, fewshot_examples):\n    \n    prompt = f'''\nYou are an expert in understanding and identifying what users are asking you.\n\nYour task is to analyze an input query from a user and assign the most appropriate category from the following list:\n{categories}\n\nOnly classify as \"oos\" (out of scope category) if none of the other categories apply.\n\nBelow are several examples to guide your classification:\n\n---\n{fewshot_examples}\n---\n\n===============================\n\nNew Question: {question}\n\n===============================\n\nProvide your final classification in **valid JSON format** with the following structure:\n{{\n  \"category\": \"your_chosen_category_name\",\n  \"confidence\": confidence_level_rounded_to_the_nearest_2_decimal_places\n}}\n\n\nEnsure the JSON has:\n- Opening and closing curly braces\n- Double quotes around keys and string values\n- Confidence as a number (not a string), with maximum 2 decimal places\n\nDo not include any explanations or extra text.\n            '''\n    return prompt\n\n\n\n#######################\n\n\n#######################\n# Model on 1 Dataset\n#######################\n# Save a list of dictionaries \n# containing a dictionary for each record's\n# - predicted category\n# - confidence level and\n# - original dataframe values\n\n\n# gemini\nuser_secrets = UserSecretsClient()\nNEBIUS_API_KEY = user_secrets.get_secret(\"NEBIUS_API_KEY\")\nclient = OpenAI(base_url=\"https://api.studio.nebius.com/v1/\",\n                api_key = NEBIUS_API_KEY)\n\n@retry(stop=stop_after_attempt(3), wait=wait_fixed(30))\ndef api_llm(client, prompt):\n    try:\n        print(\"CHECKPOINT_3A\")\n        # gemini_config = {\"temperature\": 0,\n        #                  \"response_mime_type\": \"application/json\",\n        #                  \"response_schema\": IntentSchema.model_json_schema(),\n        #                  \"seed\": 38,\n        #                  # # added for \"gemini-2.5-flash-lite-preview-06-17\" model\n        #                  # \"thinking_config\": ThinkingConfig(thinking_budget=-1, \n        #                  #                    include_thoughts=True)\n        #                 }\n        response = client.beta.chat.completions.parse(model = 'Qwen/'+Config.model_name,\n                                                      messages = [{\"role\": \"user\",\n                                                                  \"content\": prompt}],\n                                                      response_format = IntentSchema,\n                                                      seed = 38,\n                                                      temperature = 0\n                                                      )\n        # print(response)\n        # msg = response.parsed\n        response = response.choices[0].message.content\n        print(\"CHECKPOINT_3B\")\n        return response\n    except:\n        print(f\"CHECKPOINT_4A: Exception Type: {type(e).__name__}\")\n        print(f\"CHECKPOINT_4A: Exception Message: {str(e)}\")\n        \n        # Gemini-specific errors\n        if hasattr(e, 'code'):\n            print(f\"CHECKPOINT_4A: Status Code: {e.code}\")\n        if hasattr(e, 'details'):\n            print(f\"CHECKPOINT_4A: Details: {e.details}\")\n        \n        # raise the exception again so retry can work\n        raise\n\n    \n\ndef predict_intent(model_name, df, categories, start_index=0, end_index=None, log_every_n_examples=100):\n    start_time = time.time()\n    results = []  # Store processed results\n    \n    # Slice DataFrame based on start/end indices\n    if end_index is None:\n        subset_df = df.iloc[start_index:]\n    else:\n        subset_df = df.iloc[start_index:end_index+1]\n    \n    total_rows = len(subset_df)\n    subset_row_count = 0\n\n    \n\n    \n    \n    for row in subset_df.itertuples():\n        subset_row_count+=1\n        prompt = get_prompt(row.dataset, row.split, row.text, categories, fewshot_examples)\n        if subset_row_count == 1:\n            print(\"Example of how prompt looks, for the 1st example in this subset of data\")\n            print(prompt)\n\n            print(\"Example of how IntentSchema looks\")\n            print(IntentSchema.model_json_schema())\n        \n        \n        try:\n            print(\"CHECKPOINT_1A\")\n            \n            # response = ollama.chat(model=model_name, \n            #                        messages=[\n            #                                     {'role': 'user', 'content': prompt}\n            #                                 ],\n            #                        format = IntentSchema.model_json_schema(),\n            #                        options = {'temperature': 0},  # Set temperature to 0 for a more deterministic output\n            #                       )\n            # msg = response['message']['content']\n            # parsed = json.loads(msg)\n            \n            response = api_llm(client, prompt)\n            print(\"CHECKPOINT_1B\")\n            parsed = json.loads(response.text)\n            # parsed = response.parsed\n            print(\"CHECKPOINT_1C\")\n                        \n            # Safely extract keys with defaults - resolve parsing error\n            # maybe LLM did not output a particular key-value pair\n            category = parsed.get('category', 'error')\n            confidence = parsed.get('confidence', 0.0)\n            parsed = {'category': category, 'confidence': confidence}\n        except (json.JSONDecodeError, KeyError, Exception) as e:\n            print(f\"CHECKPOINT_2A: Exception Type: {type(e).__name__}\")\n            print(f\"CHECKPOINT_2A: Exception Message: {str(e)}\")\n            \n            # Gemini-specific errors\n            if hasattr(e, 'code'):\n                print(f\"CHECKPOINT_2A: Status Code: {e.code}\")\n            if hasattr(e, 'details'):\n                print(f\"CHECKPOINT_2A: Details: {e.details}\")\n                \n            parsed = {'category': 'error', 'confidence': 0.0}\n        \n        # Combine original row data with predictions\n        results.append({\n            \"Index\": row.Index,\n            \"text\": row.text,\n            \"label\": row.label,\n            \"dataset\": row.dataset,\n            \"split\": row.split,\n            \"predicted\": parsed['category'],\n            \"confidence\": parsed['confidence']\n        })\n\n        \n        # Log progress\n        if subset_row_count % log_every_n_examples == 0:\n            elapsed_time = time.time() - start_time\n            \n            avg_time_per_row = elapsed_time / subset_row_count\n            remaining_rows = total_rows - subset_row_count\n            eta = avg_time_per_row * remaining_rows\n            \n            print(f\"Processed original df idx {row.Index} (subset row {subset_row_count}) | \"\n                  f\"Elapsed: {elapsed_time:.2f}s | ETA: {eta:.2f}s\")\n    \n    return results  # Return list of dictionaries\n    \n\nprint(f\"Starting intent classification using {Config.model_name}\")\nsubset_results = predict_intent(Config.model_name, \n                                df, \n                                bulletpts_intent, \n                                start_index = Config.start_index, \n                                end_index = Config.end_index,\n                                log_every_n_examples = Config.log_every_n_examples)\n\n\n\n# # previously for Ollama\n# # update end_index for filename (if None is used for the end of the df)\n# # Get the last index of the DataFrame\n# last_index = df.index[-1] \n# # Use last index if Config.end_index is None\n# end_index = Config.end_index if Config.end_index is not None else last_index\n# 2025.07.07\n# now for Ollama AND Gemini\n# Gemini - needs to track 'end_index' for API JSON exports (when daily limits are exhausted)\n# Ollama - reuse this code\nend_index = max(r['Index'] for r in subset_results)\n\n\n\n# 2025.05.23 changed from JSON to PKL\n# because we are saving list of dictionaries\n# Save to PKL\n# 2025.06.04 explore changing back to JSON\n# with open(f'results_{Config.model_name}_{Config.dataset_name}_{Config.start_index}_{end_index}.pkl', 'wb') as f:\n#     pickle.dump(subset_results, f)\nwith open(f'results_{Config.model_name}_{Config.dataset_name}_{Config.start_index}_{end_index}.json', 'w') as f:\n    json.dump(subset_results, f, indent=2)\n\nprint(\"Completed intent classification\")\n\n\n#######################\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T06:42:08.608119Z","iopub.execute_input":"2025-07-11T06:42:08.608427Z","iopub.status.idle":"2025-07-11T06:42:08.633523Z","shell.execute_reply.started":"2025-07-11T06:42:08.608403Z","shell.execute_reply":"2025-07-11T06:42:08.632538Z"}},"outputs":[{"name":"stdout","text":"Writing predict_class.py\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"%%writefile /kaggle/working/main.py\nimport subprocess\nimport sys\nfrom src.config import Config\n\n\n# 1. Install libraries from requirements.txt\nprint(\"Installing dependencies...\")\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", \"/kaggle/working/requirements.txt\"], check=True)\n\n\n# # 2. Run setup_ollama.py\n# if 'gemini' not in Config.model_name:\n#     print(\"Starting Ollama setup...\")\n#     # subprocess.run([\"python3\", \"/kaggle/working/src/setup_ollama.py\"], check=True)\n#     print(\"Starting Ollama setup...\")\n#     subprocess.run(\n#         [\"python3\", \"-m\", \"src.setup_ollama\"],  # Run as a module\n#         cwd=\"/kaggle/working\",  # Set working directory to parent of 'src'\n#         check=True\n#     )\n    \n\n# 3. Run download_dataset.py\nprint(\"Downloading dataset...\")\nsubprocess.run([\"python3\", \"/kaggle/working/download_dataset.py\"], check=True)\n\n# 4. Run predict_class.py\nprint(\"Running prediction script...\")\nsubprocess.run([\"python3\", \"/kaggle/working/predict_class.py\"], check=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T06:42:08.635117Z","iopub.execute_input":"2025-07-11T06:42:08.635442Z","iopub.status.idle":"2025-07-11T06:42:08.658422Z","shell.execute_reply.started":"2025-07-11T06:42:08.635408Z","shell.execute_reply":"2025-07-11T06:42:08.657241Z"}},"outputs":[{"name":"stdout","text":"Writing /kaggle/working/main.py\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# Model on subset of examples","metadata":{}},{"cell_type":"code","source":"!python3 /kaggle/working/main.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T06:42:43.036942Z","iopub.execute_input":"2025-07-07T06:42:43.037254Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Sanity check folders","metadata":{}},{"cell_type":"code","source":"!cd /kaggle/working/ && ls -la","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!cd /kaggle/working/src && ls -la","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!cd /kaggle/working/data/data && ls -la","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# idx2label_oos examples","metadata":{}},{"cell_type":"code","source":"pip install huggingface-hub","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import snapshot_download\nsnapshot_download(repo_id=\"KaiquanMah/open-intent-query-classification\", repo_type=\"space\", allow_patterns=\"*_idx2label.csv\", local_dir='/kaggle/working/idx2label')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nidx2label = pd.read_csv('/kaggle/working/idx2label/dataset_idx2label/banking77_idx2label.csv')\nidx2label","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"idx2label_oos = idx2label[idx2label.index.isin([31,32,33,36])]\nidx2label_oos","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(idx2label_oos)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"idx2label_oos.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# percentage of OOS classes over ALL classes in the dataset\nlen(idx2label_oos)/len(idx2label)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Test Batch","metadata":{}},{"cell_type":"code","source":"import subprocess\nimport sys\nfrom src.config import Config\n\n\n# 1. Install libraries from requirements.txt\nprint(\"Installing dependencies...\")\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", \"/kaggle/working/requirements.txt\"], check=True)\n\n\n# # 2. Run setup_ollama.py\n# if 'gemini' not in Config.model_name:\n#     print(\"Starting Ollama setup...\")\n#     # subprocess.run([\"python3\", \"/kaggle/working/src/setup_ollama.py\"], check=True)\n#     print(\"Starting Ollama setup...\")\n#     subprocess.run(\n#         [\"python3\", \"-m\", \"src.setup_ollama\"],  # Run as a module\n#         cwd=\"/kaggle/working\",  # Set working directory to parent of 'src'\n#         check=True\n#     )\n    \n\n# 3. Run download_dataset.py\nprint(\"Downloading dataset...\")\nsubprocess.run([\"python3\", \"/kaggle/working/download_dataset.py\"], check=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T06:42:15.179455Z","iopub.execute_input":"2025-07-11T06:42:15.179794Z","iopub.status.idle":"2025-07-11T06:42:24.220018Z","shell.execute_reply.started":"2025-07-11T06:42:15.179767Z","shell.execute_reply":"2025-07-11T06:42:24.218915Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Installing dependencies...\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 1)) (2.2.3)\nRequirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 2)) (2.11.4)\nCollecting typing (from -r /kaggle/working/requirements.txt (line 3))\n  Downloading typing-3.7.4.3.tar.gz (78 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.6/78.6 kB 2.6 MB/s eta 0:00:00\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'done'\nRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 4)) (0.31.1)\nRequirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 6)) (1.70.0)\nRequirement already satisfied: tenacity in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 7)) (9.1.2)\nRequirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas->-r /kaggle/working/requirements.txt (line 1)) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->-r /kaggle/working/requirements.txt (line 1)) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->-r /kaggle/working/requirements.txt (line 1)) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->-r /kaggle/working/requirements.txt (line 1)) (2025.2)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->-r /kaggle/working/requirements.txt (line 2)) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->-r /kaggle/working/requirements.txt (line 2)) (2.33.2)\nRequirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->-r /kaggle/working/requirements.txt (line 2)) (4.13.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->-r /kaggle/working/requirements.txt (line 2)) (0.4.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->-r /kaggle/working/requirements.txt (line 4)) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->-r /kaggle/working/requirements.txt (line 4)) (2025.3.2)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->-r /kaggle/working/requirements.txt (line 4)) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->-r /kaggle/working/requirements.txt (line 4)) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->-r /kaggle/working/requirements.txt (line 4)) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->-r /kaggle/working/requirements.txt (line 4)) (4.67.1)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->-r /kaggle/working/requirements.txt (line 4)) (1.1.0)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai->-r /kaggle/working/requirements.txt (line 6)) (4.9.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai->-r /kaggle/working/requirements.txt (line 6)) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai->-r /kaggle/working/requirements.txt (line 6)) (0.28.1)\nRequirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai->-r /kaggle/working/requirements.txt (line 6)) (0.9.0)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai->-r /kaggle/working/requirements.txt (line 6)) (1.3.1)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai->-r /kaggle/working/requirements.txt (line 6)) (3.10)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai->-r /kaggle/working/requirements.txt (line 6)) (2025.4.26)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai->-r /kaggle/working/requirements.txt (line 6)) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r /kaggle/working/requirements.txt (line 6)) (0.14.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas->-r /kaggle/working/requirements.txt (line 1)) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas->-r /kaggle/working/requirements.txt (line 1)) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas->-r /kaggle/working/requirements.txt (line 1)) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas->-r /kaggle/working/requirements.txt (line 1)) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas->-r /kaggle/working/requirements.txt (line 1)) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas->-r /kaggle/working/requirements.txt (line 1)) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->-r /kaggle/working/requirements.txt (line 1)) (1.17.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->-r /kaggle/working/requirements.txt (line 4)) (3.4.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->-r /kaggle/working/requirements.txt (line 4)) (2.4.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas->-r /kaggle/working/requirements.txt (line 1)) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas->-r /kaggle/working/requirements.txt (line 1)) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.23.2->pandas->-r /kaggle/working/requirements.txt (line 1)) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.23.2->pandas->-r /kaggle/working/requirements.txt (line 1)) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.23.2->pandas->-r /kaggle/working/requirements.txt (line 1)) (2024.2.0)\nBuilding wheels for collected packages: typing\n  Building wheel for typing (setup.py): started\n  Building wheel for typing (setup.py): finished with status 'done'\n  Created wheel for typing: filename=typing-3.7.4.3-py3-none-any.whl size=26304 sha256=9c87fd9bb34381819d3f39378b989086bcb005c00070a3f322c386aeae4b97c9\n  Stored in directory: /root/.cache/pip/wheels/9d/67/2f/53e3ef32ec48d11d7d60245255e2d71e908201d20c880c08ee\nSuccessfully built typing\nInstalling collected packages: typing\nSuccessfully installed typing-3.7.4.3\nDownloading dataset...\n","output_type":"stream"},{"name":"stderr","text":"Cloning into '/kaggle/working/data'...\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"CompletedProcess(args=['python3', '/kaggle/working/download_dataset.py'], returncode=0)"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"from src.config import Config\nimport pandas as pd\nimport os\n# import ollama\nimport json\nimport pickle\nimport time\nfrom pydantic import BaseModel\nfrom typing import Literal\n# from enum import Enum\nfrom huggingface_hub import snapshot_download\n    \n###################\n# Gemini API\n###################\n# from google import genai\n# from google.genai.types import ThinkingConfig\n# from google.api_core import retry\nfrom openai import OpenAI\nfrom tenacity import retry, stop_after_attempt, wait_fixed\nfrom kaggle_secrets import UserSecretsClient\n\n\n###################\n\n\n# Config.target_dir\n# Config.cloned_data_dir'\n# Config.dataset_name\n# Config.model_name\n# Config.start_index\n# Config.end_index\n# Config.log_every_n_examples\n\n\n#######################\n# load data\n#######################\ndef load_data(data_dir):\n    \"\"\"Loads train, dev, and test datasets from a specified directory.\"\"\"\n\n    main_df = pd.DataFrame()\n    for split in ['train', 'dev', 'test']:\n        file_path = os.path.join(data_dir, f'{split}.tsv')\n        if os.path.exists(file_path):\n          try:\n            df = pd.read_csv(file_path, sep='\\t')\n            df['dataset'] = os.path.basename(data_dir)\n            df['split'] = split\n            main_df = pd.concat([main_df, df], ignore_index=True)\n          except pd.errors.ParserError as e:\n            print(f\"Error parsing {file_path}: {e}\")\n            # Handle the error appropriately, e.g., skip the file, log the error, etc.\n        else:\n            print(f\"Warning: {split}.tsv not found in {data_dir}\")\n    return main_df\n\n\ndef filter100examples_oos(dataset_name, df):\n    # dont input 'only oos qns to model'\n    if Config.filter_oos_qns_only == False:\n        filtered_df = df\n    # vs\n    # input 'only oos qns to model'\n    else:\n        if dataset_name == 'banking':\n            first_class = Config.first_class_banking\n        elif dataset_name == 'stackoverflow':\n            first_class = Config.first_class_stackoverflow\n        else:\n            first_class = Config.first_class_oos\n    \n        filtered_df = df.copy()\n        filtered_df = filtered_df.loc[filtered_df[\"label\"] == first_class]\n        filtered_df = filtered_df.sample(n=Config.n_oos_qns, random_state=38)\n    return filtered_df\n\n\ndf = pd.DataFrame()\n\ndata_dir = os.path.join(Config.cloned_data_dir, Config.dataset_name)\nif os.path.exists(data_dir):\n  df = load_data(data_dir)\n  print(f\"Loaded dataset into dataframe: {Config.dataset_name}\")\n  print(f\"Dimensions: {df.shape}\")\n  print(f\"Col names: {df.columns}\")\nelse:\n  print(f\"Warning: Directory {data_dir} not found.\")\n#######################\n\n\n\n#######################\n# unique intents\n#######################\nsorted_intent = list(sorted(df.label.unique()))\nprint(\"=\"*80)\nprint(f\"Original dataset intents: {sorted_intent}\")\nprint(f\"Number of original intents: {len(sorted_intent)}\\n\")\n\n\n# 2025.06.03\n# New OOS approach - get 25/50/75% of class indexes for each dataset within the team (for reproducibility and comparable results)\n# Change their class labels to 'oos'\nsnapshot_download(repo_id=\"KaiquanMah/open-intent-query-classification\", repo_type=\"space\", allow_patterns=\"*_idx2label.csv\", local_dir=Config.idx2label_target_dir)\nidx2label_filepath = Config.idx2label_target_dir + '/dataset_idx2label/' + Config.idx2label_filename_hf\nidx2label = pd.read_csv(idx2label_filepath)\nidx2label_oos = idx2label[idx2label.index.isin(Config.list_oos_idx)]\nidx2label_oos.reset_index(drop=True, inplace=True)\n\n# 2025.06.17 keep track of non-oos labels, to use in IntentSchema\nnonoos_labels = idx2label[~idx2label.label.isin(Config.list_oos_idx)]['label'].values\nprint(\"=\"*80)\nprint(\"Original intents to convert to OOS class\")\nprint(idx2label_oos)\nprint(f\"Percentage of original intents to convert to OOS class: {len(idx2label_oos)/len(idx2label)}\\n\")\n\noos_labels = idx2label_oos['label'].values\nlist_sorted_intent_aft_conversion = ['oos' if intent.lower() in oos_labels else intent for intent in sorted_intent]\nlist_sorted_intent_aft_conversion_deduped = sorted(set(list_sorted_intent_aft_conversion))\nprint(\"=\"*80)\nprint(\"Unique intents after converting some to OOS class\")\nprint(list_sorted_intent_aft_conversion_deduped)\nprint(f\"Number of unique intents after converting some to OOS class: {len(list_sorted_intent_aft_conversion_deduped)}\\n\")\n\n\n\n# unique intents - from set to bullet points (to use in prompts)\n# bulletpts_intent = \"\\n\".join(f\"- {category}\" for category in set_intent)\n# 2025.06.03: do not show 'oos' in the prompt (to avoid leakage of 'oos' class)\nbulletpts_intent = \"\\n\".join(f\"- {category}\" for category in list_sorted_intent_aft_conversion_deduped if category and (category!='oos'))\n\n# 2025.06.04: fix adjustment if 'oos' is already in the original dataset\nint_oos_in_orig_dataset = int('oos' in idx2label.label.values)\nadjust_if_oos_not_in_orig_dataset = [0 if int_oos_in_orig_dataset == 1 else 1][0]\n\nprint(\"=\"*80)\nprint(\"sanity check\")\nprint(f\"Number of original intents: {len(sorted_intent)}\")\nprint(f\"Number of original intents + 1 OOS class (if doesnt exist in original dataset): {len(sorted_intent) + adjust_if_oos_not_in_orig_dataset}\")\nprint(f\"Number of original intents to convert to OOS class: {len(idx2label_oos)}\")\nprint(f\"Percentage of original intents to convert to OOS class: {len(idx2label_oos)/len(idx2label)}\")\nprint(f\"Number of unique intents after converting some to OOS class: {len(list_sorted_intent_aft_conversion_deduped)}\")\nprint(f\"Number of original intents + 1 OOS class (if doesnt exist in original dataset) - converted classes: {len(sorted_intent) + adjust_if_oos_not_in_orig_dataset - len(idx2label_oos)}\")\nprint(f\"Numbers match: {(len(sorted_intent) + adjust_if_oos_not_in_orig_dataset - len(idx2label_oos)) == len(list_sorted_intent_aft_conversion_deduped)}\")\nprint(\"Prepared unique intents\")\n#######################\n\n\n\n\n#######################\n# Enforce schema on the model (e.g. allowed list of predicted categories)\n#######################\n\nclass IntentSchema(BaseModel):\n    # dynamically unpack list of categories for different dataset(s)\n    category: Literal[*list_sorted_intent_aft_conversion_deduped]\n    confidence: float\n    \n#######################\n\n\n\n\n#######################\n# filter after preparing intents\n#######################\ndf = filter100examples_oos(Config.dataset_name, df)\nprint(\"Filtered dataset\")\nprint(f\"Dimensions: {df.shape}\")\nprint(f\"Col names: {df.columns}\")\n#######################\n\n\n\n#######################\n# Prompt\n#######################\n# prompt 2 with less information/compute, improve efficiency\n# 2025.06.10 prompt 3 with 5 few shot examples only - notebook O1H1, O1i1\n# 2025.06.16 prompt 4 with 5 examples per each known intent (ie non-oos intent) - notebook 01J1\nsnapshot_download(repo_id=\"KaiquanMah/open-intent-query-classification\", repo_type=\"space\", allow_patterns=\"*.txt\", local_dir=Config.fewshot_examples_dir)\nwith open(Config.fewshot_examples_dir + Config.fewshot_subdir + Config.fewshot_examples_filename, 'r') as file:\n    fewshot_examples = file.read()\n\ndef get_prompt(dataset_name, split, question, categories, fewshot_examples):\n    \n    prompt = f'''\nYou are an expert in understanding and identifying what users are asking you.\n\nYour task is to analyze an input query from a user and assign the most appropriate category from the following list:\n{categories}\n\nOnly classify as \"oos\" (out of scope category) if none of the other categories apply.\n\nBelow are several examples to guide your classification:\n\n---\n{fewshot_examples}\n---\n\n===============================\n\nNew Question: {question}\n\n===============================\n\nProvide your final classification in **valid JSON format** with the following structure:\n{{\n  \"category\": \"your_chosen_category_name\",\n  \"confidence\": confidence_level_rounded_to_the_nearest_2_decimal_places\n}}\n\n\nEnsure the JSON has:\n- Opening and closing curly braces\n- Double quotes around keys and string values\n- Confidence as a number (not a string), with maximum 2 decimal places\n\nDo not include any explanations or extra text.\n            '''\n    return prompt\n\n\n\n#######################\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T06:42:24.221685Z","iopub.execute_input":"2025-07-11T06:42:24.222769Z","iopub.status.idle":"2025-07-11T06:42:30.441327Z","shell.execute_reply.started":"2025-07-11T06:42:24.222738Z","shell.execute_reply":"2025-07-11T06:42:30.439696Z"}},"outputs":[{"name":"stdout","text":"Loaded dataset into dataframe: oos\nDimensions: (23700, 4)\nCol names: Index(['text', 'label', 'dataset', 'split'], dtype='object')\n================================================================================\nOriginal dataset intents: ['accept_reservations', 'account_blocked', 'alarm', 'application_status', 'apr', 'are_you_a_bot', 'balance', 'bill_balance', 'bill_due', 'book_flight', 'book_hotel', 'calculator', 'calendar', 'calendar_update', 'calories', 'cancel', 'cancel_reservation', 'car_rental', 'card_declined', 'carry_on', 'change_accent', 'change_ai_name', 'change_language', 'change_speed', 'change_user_name', 'change_volume', 'confirm_reservation', 'cook_time', 'credit_limit', 'credit_limit_change', 'credit_score', 'current_location', 'damaged_card', 'date', 'definition', 'direct_deposit', 'directions', 'distance', 'do_you_have_pets', 'exchange_rate', 'expiration_date', 'find_phone', 'flight_status', 'flip_coin', 'food_last', 'freeze_account', 'fun_fact', 'gas', 'gas_type', 'goodbye', 'greeting', 'how_busy', 'how_old_are_you', 'improve_credit_score', 'income', 'ingredient_substitution', 'ingredients_list', 'insurance', 'insurance_change', 'interest_rate', 'international_fees', 'international_visa', 'jump_start', 'last_maintenance', 'lost_luggage', 'make_call', 'maybe', 'meal_suggestion', 'meaning_of_life', 'measurement_conversion', 'meeting_schedule', 'min_payment', 'mpg', 'new_card', 'next_holiday', 'next_song', 'no', 'nutrition_info', 'oil_change_how', 'oil_change_when', 'oos', 'order', 'order_checks', 'order_status', 'pay_bill', 'payday', 'pin_change', 'play_music', 'plug_type', 'pto_balance', 'pto_request', 'pto_request_status', 'pto_used', 'recipe', 'redeem_rewards', 'reminder', 'reminder_update', 'repeat', 'replacement_card_duration', 'report_fraud', 'report_lost_card', 'reset_settings', 'restaurant_reservation', 'restaurant_reviews', 'restaurant_suggestion', 'rewards_balance', 'roll_dice', 'rollover_401k', 'routing', 'schedule_maintenance', 'schedule_meeting', 'share_location', 'shopping_list', 'shopping_list_update', 'smart_home', 'spelling', 'spending_history', 'sync_device', 'taxes', 'tell_joke', 'text', 'thank_you', 'time', 'timer', 'timezone', 'tire_change', 'tire_pressure', 'todo_list', 'todo_list_update', 'traffic', 'transactions', 'transfer', 'translate', 'travel_alert', 'travel_notification', 'travel_suggestion', 'uber', 'update_playlist', 'user_name', 'vaccines', 'w2', 'weather', 'what_are_your_hobbies', 'what_can_i_ask_you', 'what_is_your_name', 'what_song', 'where_are_you_from', 'whisper_mode', 'who_do_you_work_for', 'who_made_you', 'yes']\nNumber of original intents: 151\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b61b4119d334b63aa8f2fe50b7bf660"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"clinc150_oos_idx2label.csv: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89c2da0ab9c2453f8c590983cfb4db5e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"stackoverflow_idx2label.csv:   0%|          | 0.00/224 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c00a56caf9bd4100b38c8b7a68d3732f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"banking77_idx2label.csv: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83049ef6bcb14756a90d87808a6f6783"}},"metadata":{}},{"name":"stdout","text":"================================================================================\nOriginal intents to convert to OOS class\n    index                      label\n0       2            account_blocked\n1       7              interest_rate\n2      12           report_lost_card\n3      20             freeze_account\n4      27               bill_balance\n5      28       improve_credit_score\n6      35                    balance\n7      38              exchange_rate\n8      41               credit_limit\n9      49              card_declined\n10     51              change_accent\n11     56                   bill_due\n12     58             share_location\n13     59         international_visa\n14     61                  translate\n15     64           insurance_change\n16     66                   timezone\n17     68               transactions\n18     69               credit_score\n19     70               report_fraud\n20     71           spending_history\n21     74                  insurance\n22     79                     payday\n23     87               damaged_card\n24     89                 pin_change\n25     90  replacement_card_duration\n26     91                   new_card\n27     93                     income\n28    100              rollover_401k\n29    103         application_status\n30    108             direct_deposit\n31    109        credit_limit_change\n32    111                   pay_bill\n33    133                   transfer\n34    137         international_fees\n35    143                min_payment\nPercentage of original intents to convert to OOS class: 0.23841059602649006\n\n================================================================================\nUnique intents after converting some to OOS class\n['accept_reservations', 'alarm', 'apr', 'are_you_a_bot', 'book_flight', 'book_hotel', 'calculator', 'calendar', 'calendar_update', 'calories', 'cancel', 'cancel_reservation', 'car_rental', 'carry_on', 'change_ai_name', 'change_language', 'change_speed', 'change_user_name', 'change_volume', 'confirm_reservation', 'cook_time', 'current_location', 'date', 'definition', 'directions', 'distance', 'do_you_have_pets', 'expiration_date', 'find_phone', 'flight_status', 'flip_coin', 'food_last', 'fun_fact', 'gas', 'gas_type', 'goodbye', 'greeting', 'how_busy', 'how_old_are_you', 'ingredient_substitution', 'ingredients_list', 'jump_start', 'last_maintenance', 'lost_luggage', 'make_call', 'maybe', 'meal_suggestion', 'meaning_of_life', 'measurement_conversion', 'meeting_schedule', 'mpg', 'next_holiday', 'next_song', 'no', 'nutrition_info', 'oil_change_how', 'oil_change_when', 'oos', 'order', 'order_checks', 'order_status', 'play_music', 'plug_type', 'pto_balance', 'pto_request', 'pto_request_status', 'pto_used', 'recipe', 'redeem_rewards', 'reminder', 'reminder_update', 'repeat', 'reset_settings', 'restaurant_reservation', 'restaurant_reviews', 'restaurant_suggestion', 'rewards_balance', 'roll_dice', 'routing', 'schedule_maintenance', 'schedule_meeting', 'shopping_list', 'shopping_list_update', 'smart_home', 'spelling', 'sync_device', 'taxes', 'tell_joke', 'text', 'thank_you', 'time', 'timer', 'tire_change', 'tire_pressure', 'todo_list', 'todo_list_update', 'traffic', 'travel_alert', 'travel_notification', 'travel_suggestion', 'uber', 'update_playlist', 'user_name', 'vaccines', 'w2', 'weather', 'what_are_your_hobbies', 'what_can_i_ask_you', 'what_is_your_name', 'what_song', 'where_are_you_from', 'whisper_mode', 'who_do_you_work_for', 'who_made_you', 'yes']\nNumber of unique intents after converting some to OOS class: 115\n\n================================================================================\nsanity check\nNumber of original intents: 151\nNumber of original intents + 1 OOS class (if doesnt exist in original dataset): 151\nNumber of original intents to convert to OOS class: 36\nPercentage of original intents to convert to OOS class: 0.23841059602649006\nNumber of unique intents after converting some to OOS class: 115\nNumber of original intents + 1 OOS class (if doesnt exist in original dataset) - converted classes: 115\nNumbers match: True\nPrepared unique intents\nFiltered dataset\nDimensions: (23700, 4)\nCol names: Index(['text', 'label', 'dataset', 'split'], dtype='object')\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 62 files:   0%|          | 0/62 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f66c326747a473ca3e94ad1c8e791ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"banking_only10notoos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddc7b21b0b164558a0e6cb7b4f77d971"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"banking_only1notoos.txt:   0%|          | 0.00/177 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33c79c6f1a9f4f01b678991a0cb51dde"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"banking_only40notoos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77a1185ba2ef41df956d1aac31e77eaa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"banking_only60notoos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2dfd10b0ce12467b87fc9118c1f26265"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"banking_only50notoos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7dfb5f2b1f644373afe03271223e894d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"banking_only4notoos.txt:   0%|          | 0.00/651 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f957acefcfcc4e558ae8ecca4e242ca7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"banking_only5notoos.txt:   0%|          | 0.00/835 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a84af20c423c4ed787728d3542a23c9c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"oos_100notoos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edbbbe4ceb1441ac8f3cc1c4a1593b00"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"banking_only70notoos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f990b465130a42d48eb06c026c463073"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"oos_10notoos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6ead40ec5fb418695e11ac01d72969c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"oos_11notoos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bc58793935647caa83c10c13d7740e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"oos_12notoos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a881574a47f140eb83a7bd509f95eca6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"oos_14notoos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1978e284a829471b8975c8ed17effdce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"oos_13notoos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a66a77a2b52a49f9b1101be3128d505a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"oos_120notoos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b207dfc579fa42ce951f94da5979d18b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"oos_140notoos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8ee81f2c74f419e831cf57af3febaae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"oos_15notoos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce42ee807a5642b29c3294f92cba74fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"oos_1notoos.txt:   0%|          | 0.00/121 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2394b29735d440e9f7e3936b596ab42"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"oos_20notoos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc8be0bc9a6c4f42bdb509b96d6bbf93"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"oos_2notoos.txt:   0%|          | 0.00/259 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f819dce596184573a4fbfc4054d404b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"oos_3notoos.txt:   0%|          | 0.00/387 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfbd8f855e9342e380658030d6d5617e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"oos_4notoos.txt:   0%|          | 0.00/537 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"780335ac88434f58b43d035dbbe80982"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"oos_50notoos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a7330bfb5184ced80ce1363fbe2e585"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"oos_5notoos.txt:   0%|          | 0.00/731 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f55b5e8a0f0493e8ec185467d7634f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"oos_30notoos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c353ab38c9c4861938fed222e76d316"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"oos_75notoos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c77595dc4cb143999e6f9752ccc955d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"oos_40notoos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94f13d1d5a784858bef3e12838a81c75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"stackoverflow_only10notoos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de103a9c88ac4379864045c587f414ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"stackoverflow_only12notoos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91c04eb9f79d41429cce852cfa54fff4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"stackoverflow_only2notoos.txt:   0%|          | 0.00/288 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"945ec603f0b240ec95286e4a1a5a87e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"stackoverflow_only16notoos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21091bc5a5af40d38920ca5e50dd32c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"stackoverflow_only1notoos.txt:   0%|          | 0.00/132 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7b0641077264565afec6d4d28e0fef7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"stackoverflow_only18notoos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10ad0ae89c6445b09f774f8e3e656537"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"stackoverflow_only14notoos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21e381b41046462581cb320e3e66baf3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"stackoverflow_only3notoos.txt:   0%|          | 0.00/420 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20163e76887e4ba39525d32f63fc2ad7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"stackoverflow_only4notoos.txt:   0%|          | 0.00/591 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ccf4a3ea1fd4d9aa1b57e6a068452e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"banking_25perc_oos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"326ed84eb3574ac498c55f18acbf39af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"stackoverflow_only6notoos.txt:   0%|          | 0.00/884 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca72b38c3b0b42d4b70bd8b41ffe54e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"stackoverflow_only5notoos.txt:   0%|          | 0.00/715 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a264fa0c61542f9b4a97000e822c703"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"stackoverflow_only8notoos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f9c7357acad416e8dd6d4a61a533266"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"oos_25perc_oos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"124c15a95d22489488e5cdef17fb0793"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"stackoverflow_25perc_oos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6162df4870f54c599866d2bb4d1bdbdf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"logs_cpu_2025.05.22%20round1.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11e5382593be4be7aecf7b2474cf1bb1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"stackoverflow_only2notoos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad6aa0653249483a9124017bd37fb863"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"logs_cpu_2025.05.22%20round2.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a222686ec25436b9c04ab6334607da1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)u_2025.05.22%20round4%20parallelise2.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b2b3fe0516341138056e08089733aa3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)u_2025.05.22%20round3%20parallelise1.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df33054fe77e43b2a62ec4f83da0aa7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)lassification_report_llama3.2_3b_oos.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31ebed8129344ec098e25235d96c3617"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)5.05.22%20round2%20w%20start-end-idx.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc30da353db941d1a84335725c380aaa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)ification_report_llama3.2_3b_banking.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21c18058e7b44c489ed37baccc6f5c2d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)ion_report_llama3.2_3b_stackoverflow.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ebd2e7953ba4ea3b6a129408bbc7234"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"logs_gpu_2025.05.22.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d2c26a912454324a6dd7a2cd1e49533"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)fication_report_llama3.2_3b_oos_full.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a93b6b9aa4ab43f1a91de72c449cbb18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)tion_report_llama3.2_3b_banking_full.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7539355a772404a97b7ef90d8d2fcd9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)lama3.2_3b_stackoverflow_only2notoos.txt:   0%|          | 0.00/380 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94a224d915e441ceacb3572c08ce8cc6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)eport_llama3.2_3b_stackoverflow_full.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f058991fc724f61abddfac4bde84573"}},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"## [New Task] Create Batch JSONL file of requests to Nebius' Qwen model\n* https://docs.nebius.com/studio/inference/batch","metadata":{}},{"cell_type":"code","source":"# gemini\nuser_secrets = UserSecretsClient()\nNEBIUS_API_KEY = user_secrets.get_secret(\"NEBIUS_API_KEY\")\nNEBIUS2 = user_secrets.get_secret(\"NEBIUS2\")\nclient = OpenAI(base_url=\"https://api.studio.nebius.com/v1/\",\n                api_key = NEBIUS2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T06:43:14.322654Z","iopub.execute_input":"2025-07-11T06:43:14.322955Z","iopub.status.idle":"2025-07-11T06:43:15.021155Z","shell.execute_reply.started":"2025-07-11T06:43:14.322927Z","shell.execute_reply":"2025-07-11T06:43:15.020349Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"model_name = Config.model_name\ndf = df\ncategories = bulletpts_intent\nstart_index = Config.start_index\nend_index = Config.end_index\nlog_every_n_examples = Config.log_every_n_examples\n\n\n\nstart_time = time.time()\nresults = []  # Store processed results\n\n# Slice DataFrame based on start/end indices\nif end_index is None:\n    subset_df = df.iloc[start_index:]\nelse:\n    subset_df = df.iloc[start_index:end_index+1]\n\ntotal_rows = len(subset_df)\nsubset_row_count = 0\n\n\n\n\n\nfor row in subset_df.itertuples():\n    subset_row_count+=1\n    prompt = get_prompt(row.dataset, row.split, row.text, categories, fewshot_examples)\n    if subset_row_count == 1:\n        print(\"Example of how prompt looks, for the 1st example in this subset of data\")\n        # print(prompt)\n        break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T05:18:33.049118Z","iopub.execute_input":"2025-07-08T05:18:33.049396Z","iopub.status.idle":"2025-07-08T05:18:33.057139Z","shell.execute_reply.started":"2025-07-08T05:18:33.049376Z","shell.execute_reply":"2025-07-08T05:18:33.056334Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_batch_file(df, categories):\n    requests = []\n    for row in df.itertuples():\n        # # round 1 - test 10 records\n        # if row.Index < 10:\n        # round 2 - FULL RUN\n        prompt = get_prompt(row.dataset, row.split, row.text, categories, fewshot_examples)\n        requests.append({\n            \"custom_id\": row.Index,\n            \"method\": \"POST\",\n            \"url\": \"/v1/chat/completions\",\n            \"body\": {\n                \"model\": f\"Qwen/{Config.model_name}\",\n                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n                \"seed\": 38,\n                \"temperature\": 0,\n                \"extra_body\": {\"guided_json\": IntentSchema.model_json_schema()}\n            }\n        })\n    \n    # Save to JSONL file\n    batch_file_path = f\"/kaggle/working/batch_prompts_{Config.dataset_name}_{Config.start_index}_{Config.end_index}.jsonl\"\n    with open(batch_file_path, 'w') as f:\n        for req in requests:\n            f.write(json.dumps(req) + '\\n')\n    \n    return batch_file_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T05:18:39.381134Z","iopub.execute_input":"2025-07-08T05:18:39.381521Z","iopub.status.idle":"2025-07-08T05:18:39.390282Z","shell.execute_reply.started":"2025-07-08T05:18:39.381491Z","shell.execute_reply":"2025-07-08T05:18:39.389383Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"start_index","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T05:18:44.136421Z","iopub.execute_input":"2025-07-08T05:18:44.136734Z","iopub.status.idle":"2025-07-08T05:18:44.142199Z","shell.execute_reply.started":"2025-07-08T05:18:44.13671Z","shell.execute_reply":"2025-07-08T05:18:44.141356Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(subset_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T05:18:45.217756Z","iopub.execute_input":"2025-07-08T05:18:45.218091Z","iopub.status.idle":"2025-07-08T05:18:45.223742Z","shell.execute_reply.started":"2025-07-08T05:18:45.218069Z","shell.execute_reply":"2025-07-08T05:18:45.222999Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T05:18:46.182514Z","iopub.execute_input":"2025-07-08T05:18:46.182813Z","iopub.status.idle":"2025-07-08T05:18:46.188365Z","shell.execute_reply.started":"2025-07-08T05:18:46.18279Z","shell.execute_reply":"2025-07-08T05:18:46.187576Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_file_path = create_batch_file(df, categories)\nbatch_file_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T05:18:48.640511Z","iopub.execute_input":"2025-07-08T05:18:48.641168Z","iopub.status.idle":"2025-07-08T05:19:13.241778Z","shell.execute_reply.started":"2025-07-08T05:18:48.641135Z","shell.execute_reply":"2025-07-08T05:19:13.240759Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nlist_prompts = []\n\nwith open(f'/kaggle/working/batch_prompts_{Config.dataset_name}_{Config.start_index}_{Config.end_index}.jsonl', 'r') as f:\n    list_prompts = [json.loads(line) for line in f if line.strip()]\n\nlist_prompts[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T05:19:16.160851Z","iopub.execute_input":"2025-07-08T05:19:16.161168Z","iopub.status.idle":"2025-07-08T05:19:30.237888Z","shell.execute_reply.started":"2025-07-08T05:19:16.161145Z","shell.execute_reply":"2025-07-08T05:19:30.237024Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(list_prompts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T05:19:30.239377Z","iopub.execute_input":"2025-07-08T05:19:30.239601Z","iopub.status.idle":"2025-07-08T05:19:30.244854Z","shell.execute_reply.started":"2025-07-08T05:19:30.239581Z","shell.execute_reply":"2025-07-08T05:19:30.244247Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# sanity check min, max custom_id / row.Index for batch requests\nlist_custom_id = []\nfor req in list_prompts:\n    # print(req['custom_id'])  # Example: print custom_id of each request\n    list_custom_id.append(req['custom_id'])\nprint(f\"custom_id. min: {min(list_custom_id)}, max: {max(list_custom_id)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T05:19:30.245792Z","iopub.execute_input":"2025-07-08T05:19:30.246167Z","iopub.status.idle":"2025-07-08T05:19:30.277268Z","shell.execute_reply.started":"2025-07-08T05:19:30.246146Z","shell.execute_reply":"2025-07-08T05:19:30.276455Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Batch API call","metadata":{}},{"cell_type":"code","source":"# upload JSONL file of API requests\nbatch_requests = client.files.create(\n    file=open(batch_file_path, \"rb\"),\n    purpose=\"batch\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T05:19:53.809522Z","iopub.execute_input":"2025-07-08T05:19:53.810302Z","iopub.status.idle":"2025-07-08T05:22:19.615963Z","shell.execute_reply.started":"2025-07-08T05:19:53.810273Z","shell.execute_reply":"2025-07-08T05:22:19.615052Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_requests","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T05:22:19.617426Z","iopub.execute_input":"2025-07-08T05:22:19.61786Z","iopub.status.idle":"2025-07-08T05:22:19.623917Z","shell.execute_reply.started":"2025-07-08T05:22:19.617827Z","shell.execute_reply":"2025-07-08T05:22:19.623053Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_requests.__dict__","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T05:22:19.624787Z","iopub.execute_input":"2025-07-08T05:22:19.625068Z","iopub.status.idle":"2025-07-08T05:22:19.644431Z","shell.execute_reply.started":"2025-07-08T05:22:19.625037Z","shell.execute_reply":"2025-07-08T05:22:19.643476Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Config.dataset_name","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T05:22:19.645877Z","iopub.execute_input":"2025-07-08T05:22:19.646444Z","iopub.status.idle":"2025-07-08T05:22:19.665238Z","shell.execute_reply.started":"2025-07-08T05:22:19.646421Z","shell.execute_reply":"2025-07-08T05:22:19.664463Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# submit batch requests to Nebius Qwen model\nclient.batches.create(\n    input_file_id=batch_requests.id,\n    endpoint=\"/v1/chat/completions\",\n    completion_window=\"24h\",\n    metadata={\n        \"description\": f\"{Config.dataset_name} - Qwen - BatchFull\"\n    }\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T05:24:48.211883Z","iopub.execute_input":"2025-07-08T05:24:48.212537Z","iopub.status.idle":"2025-07-08T05:24:48.629744Z","shell.execute_reply.started":"2025-07-08T05:24:48.21251Z","shell.execute_reply":"2025-07-08T05:24:48.628947Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# follow up on batch request status\nbatch_id = 'batch_40bd2f2e-d962-4c57-b9de-8eed6b7281fa'\ncompleted_batch = client.batches.retrieve(batch_id)\ncompleted_batch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T05:46:34.345655Z","iopub.execute_input":"2025-07-08T05:46:34.345952Z","iopub.status.idle":"2025-07-08T05:46:34.897891Z","shell.execute_reply.started":"2025-07-08T05:46:34.345932Z","shell.execute_reply":"2025-07-08T05:46:34.897063Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"completed_batch.__dict__","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T05:46:39.036084Z","iopub.execute_input":"2025-07-08T05:46:39.036695Z","iopub.status.idle":"2025-07-08T05:46:39.041833Z","shell.execute_reply.started":"2025-07-08T05:46:39.036669Z","shell.execute_reply":"2025-07-08T05:46:39.041034Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# BEFORE TOPUP CREDITS\n\nclient = OpenAI(base_url=\"https://api.studio.nebius.com/v1/\",\n                api_key = NEBIUS_API_KEY)\n\n# retrieve results using uploaded JSONL 'file id'\n\n# DO NOT RETRIEVE INPUT FILE from 'batch_requests.id'!\n# batch_result = client.files.content(batch_requests.id)\n\n# INSTEAD, PLEASE RETRIEVE file using 'output_file_id'!!!\nbatch_result = client.files.content('50f912d0-e7a7-4a14-ba00-432e0768df28')\n# print 1st 1k characters\nprint(batch_result.text[:1000])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T06:44:04.320218Z","iopub.execute_input":"2025-07-11T06:44:04.320566Z","iopub.status.idle":"2025-07-11T06:44:05.014348Z","shell.execute_reply.started":"2025-07-11T06:44:04.320543Z","shell.execute_reply":"2025-07-11T06:44:05.013146Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAPIStatusError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/1149725454.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# INSTEAD, PLEASE RETRIEVE file using 'output_file_id'!!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mbatch_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'50f912d0-e7a7-4a14-ba00-432e0768df28'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m# print 1st 1k characters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/files.py\u001b[0m in \u001b[0;36mcontent\u001b[0;34m(self, file_id, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Expected a non-empty value for `file_id` but received {file_id!r}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0mextra_headers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"Accept\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"application/binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextra_headers\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         return self._get(\n\u001b[0m\u001b[1;32m    287\u001b[0m             \u001b[0;34mf\"/files/{file_id}/content\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             options=make_request_options(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, path, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1186\u001b[0m         \u001b[0;31m# cast is required because mypy complains about returning Any even though\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0;31m# it understands the type variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1188\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 919\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    920\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1023\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m         return self._process_response(\n","\u001b[0;31mAPIStatusError\u001b[0m: Error code: 402 - {'detail': 'Payment Required: You have exhausted your budget. Please add funds to continue using the API.'}"],"ename":"APIStatusError","evalue":"Error code: 402 - {'detail': 'Payment Required: You have exhausted your budget. Please add funds to continue using the API.'}","output_type":"error"}],"execution_count":14},{"cell_type":"code","source":"!curl --location 'https://api.studio.nebius.com/v1/files/file-50f912d0-e7a7-4a14-ba00-432e0768df28/content' \\\n--header 'accept: application/json' \\\n--header 'Authorization: Bearer {NEBIUS_API_KEY}'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T16:07:55.454144Z","iopub.execute_input":"2025-07-09T16:07:55.454487Z","iopub.status.idle":"2025-07-09T16:07:56.323318Z","shell.execute_reply.started":"2025-07-09T16:07:55.454452Z","shell.execute_reply":"2025-07-09T16:07:56.321848Z"}},"outputs":[{"name":"stdout","text":"{\"detail\":\"Payment Required: You have exhausted your budget. Please add funds to continue using the API.\"}","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# BORROWING API_KEY #2 (a different user with credits)\n# retrieve results using uploaded JSONL 'file id'\n\n# DO NOT RETRIEVE INPUT FILE from 'batch_requests.id'!\n# batch_result = client.files.content(batch_requests.id)\n\n# INSTEAD, PLEASE RETRIEVE file using 'output_file_id'!!!\nbatch_result = client.files.content('50f912d0-e7a7-4a14-ba00-432e0768df28')\n# print 1st 1k characters\nprint(batch_result.text[:1000])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T06:43:26.321507Z","iopub.execute_input":"2025-07-11T06:43:26.321832Z","iopub.status.idle":"2025-07-11T06:43:27.172795Z","shell.execute_reply.started":"2025-07-11T06:43:26.321810Z","shell.execute_reply":"2025-07-11T06:43:27.171616Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPermissionDeniedError\u001b[0m                     Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/3817295601.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# INSTEAD, PLEASE RETRIEVE file using 'output_file_id'!!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mbatch_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'50f912d0-e7a7-4a14-ba00-432e0768df28'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# print 1st 1k characters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/files.py\u001b[0m in \u001b[0;36mcontent\u001b[0;34m(self, file_id, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Expected a non-empty value for `file_id` but received {file_id!r}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0mextra_headers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"Accept\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"application/binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextra_headers\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         return self._get(\n\u001b[0m\u001b[1;32m    287\u001b[0m             \u001b[0;34mf\"/files/{file_id}/content\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             options=make_request_options(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, path, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1186\u001b[0m         \u001b[0;31m# cast is required because mypy complains about returning Any even though\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0;31m# it understands the type variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1188\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 919\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    920\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1023\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m         return self._process_response(\n","\u001b[0;31mPermissionDeniedError\u001b[0m: Error code: 403 - {'detail': 'Forbidden'}"],"ename":"PermissionDeniedError","evalue":"Error code: 403 - {'detail': 'Forbidden'}","output_type":"error"}],"execution_count":12},{"cell_type":"code","source":"!curl --location 'https://api.studio.nebius.com/v1/files/file-50f912d0-e7a7-4a14-ba00-432e0768df28/content' \\\n--header 'accept: application/json' \\\n--header 'Authorization: Bearer {NEBIUS2}'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T16:07:51.741886Z","iopub.execute_input":"2025-07-09T16:07:51.742233Z","iopub.status.idle":"2025-07-09T16:07:52.493300Z","shell.execute_reply.started":"2025-07-09T16:07:51.742211Z","shell.execute_reply":"2025-07-09T16:07:52.492128Z"}},"outputs":[{"name":"stdout","text":"{\"detail\":\"File not found\"}","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# AFTER TOPUP CREDITS\nclient = OpenAI(base_url=\"https://api.studio.nebius.com/v1/\",\n                api_key = NEBIUS_API_KEY)\n\n# retrieve results using uploaded JSONL 'file id'\n\n# DO NOT RETRIEVE INPUT FILE from 'batch_requests.id'!\n# batch_result = client.files.content(batch_requests.id)\n\n# INSTEAD, PLEASE RETRIEVE file using 'output_file_id'!!!\nbatch_result = client.files.content('50f912d0-e7a7-4a14-ba00-432e0768df28')\n# print 1st 1k characters\nprint(batch_result.text[:1000])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T07:17:37.604938Z","iopub.execute_input":"2025-07-11T07:17:37.605283Z","iopub.status.idle":"2025-07-11T07:17:41.559703Z","shell.execute_reply.started":"2025-07-11T07:17:37.605243Z","shell.execute_reply":"2025-07-11T07:17:41.558601Z"}},"outputs":[{"name":"stdout","text":"{\"id\": \"batch_req_9dd0554a-011e-4bd3-8b9b-e86d95d43c20\", \"custom_id\": \"1186\", \"response\": {\"id\": \"chatcmpl-3bec33b04d1c437e9e80b0ae483bed39\", \"choices\": [{\"finish_reason\": \"stop\", \"index\": 0, \"logprobs\": null, \"message\": {\"content\": \"{\\\"category\\\":\\\"oos\\\",\\\"confidence\\\":99.99}\", \"refusal\": null, \"role\": \"assistant\", \"audio\": null, \"function_call\": null, \"tool_calls\": [], \"reasoning_content\": null}, \"stop_reason\": null}], \"created\": 1751952453, \"model\": \"Qwen/Qwen3-30B-A3B\", \"object\": \"chat.completion\", \"service_tier\": null, \"system_fingerprint\": null, \"usage\": {\"completion_tokens\": 15, \"prompt_tokens\": 25324, \"total_tokens\": 25339, \"completion_tokens_details\": null, \"prompt_tokens_details\": null}, \"prompt_logprobs\": null}, \"error\": null}\n{\"id\": \"batch_req_f2e7aeea-cbac-4172-a5b4-2d823b3d08da\", \"custom_id\": \"1866\", \"response\": {\"id\": \"chatcmpl-a9c840c2d9e046f491001522c8051c8d\", \"choices\": [{\"finish_reason\": \"stop\", \"index\": 0, \"logprobs\": null, \"message\": {\"content\": \"{\\\"category\\\":\\\"ch\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"type(batch_result.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T07:17:41.561472Z","iopub.execute_input":"2025-07-11T07:17:41.561740Z","iopub.status.idle":"2025-07-11T07:17:41.568205Z","shell.execute_reply.started":"2025-07-11T07:17:41.561718Z","shell.execute_reply":"2025-07-11T07:17:41.567223Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"str"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"import json\n\n# from 1 string of ALL dictionary lines -> to list of 1 string per dictionary line\nlines = [line.strip() for line in batch_result.text.splitlines() if line.strip()]\n# from list of string 'dictionary lines' -> to list of dictionary objects\njson_objects = [json.loads(line) for line in lines]\n# sort LLM 'response' dictionaries based on row.Index (ie custom_id)\nsorted_objects = sorted(json_objects, \n                        key = lambda x: int(x[\"custom_id\"]))\n\n# 1st 5 dictionaries\nsorted_objects[:5]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T07:17:47.660479Z","iopub.execute_input":"2025-07-11T07:17:47.660761Z","iopub.status.idle":"2025-07-11T07:17:48.442340Z","shell.execute_reply.started":"2025-07-11T07:17:47.660742Z","shell.execute_reply":"2025-07-11T07:17:48.440961Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"[{'id': 'batch_req_a383be60-2942-4f39-8d65-857797f68717',\n  'custom_id': '0',\n  'response': {'id': 'chatcmpl-13e8bd394861441982f27c5eff7f9298',\n   'choices': [{'finish_reason': 'stop',\n     'index': 0,\n     'logprobs': None,\n     'message': {'content': '{\"category\":\"oos\",\"confidence\":97.68}',\n      'refusal': None,\n      'role': 'assistant',\n      'audio': None,\n      'function_call': None,\n      'tool_calls': [],\n      'reasoning_content': None},\n     'stop_reason': None}],\n   'created': 1751952408,\n   'model': 'Qwen/Qwen3-30B-A3B',\n   'object': 'chat.completion',\n   'service_tier': None,\n   'system_fingerprint': None,\n   'usage': {'completion_tokens': 15,\n    'prompt_tokens': 25334,\n    'total_tokens': 25349,\n    'completion_tokens_details': None,\n    'prompt_tokens_details': None},\n   'prompt_logprobs': None},\n  'error': None},\n {'id': 'batch_req_c5d6363d-cd60-4c29-9b5b-3ab59993834a',\n  'custom_id': '1',\n  'response': {'id': 'chatcmpl-383cb94a6fd24c39b5237b6f1d398611',\n   'choices': [{'finish_reason': 'stop',\n     'index': 0,\n     'logprobs': None,\n     'message': {'content': '{\"category\":\"oos\",\"confidence\":42.52}',\n      'refusal': None,\n      'role': 'assistant',\n      'audio': None,\n      'function_call': None,\n      'tool_calls': [],\n      'reasoning_content': None},\n     'stop_reason': None}],\n   'created': 1751952406,\n   'model': 'Qwen/Qwen3-30B-A3B',\n   'object': 'chat.completion',\n   'service_tier': None,\n   'system_fingerprint': None,\n   'usage': {'completion_tokens': 15,\n    'prompt_tokens': 25336,\n    'total_tokens': 25351,\n    'completion_tokens_details': None,\n    'prompt_tokens_details': None},\n   'prompt_logprobs': None},\n  'error': None},\n {'id': 'batch_req_b0a7345b-4608-495e-a9ce-513e95fbbe4d',\n  'custom_id': '2',\n  'response': {'id': 'chatcmpl-1f94f3c17d5d4bb687b30eb8d7636645',\n   'choices': [{'finish_reason': 'stop',\n     'index': 0,\n     'logprobs': None,\n     'message': {'content': '{\"category\":\"oos\",\"confidence\":42.52}',\n      'refusal': None,\n      'role': 'assistant',\n      'audio': None,\n      'function_call': None,\n      'tool_calls': [],\n      'reasoning_content': None},\n     'stop_reason': None}],\n   'created': 1751952406,\n   'model': 'Qwen/Qwen3-30B-A3B',\n   'object': 'chat.completion',\n   'service_tier': None,\n   'system_fingerprint': None,\n   'usage': {'completion_tokens': 15,\n    'prompt_tokens': 25332,\n    'total_tokens': 25347,\n    'completion_tokens_details': None,\n    'prompt_tokens_details': None},\n   'prompt_logprobs': None},\n  'error': None},\n {'id': 'batch_req_746bd2f3-30c1-4558-8c8c-e3667e961589',\n  'custom_id': '3',\n  'response': {'id': 'chatcmpl-56c5e2adf38d42cd992f6baeb622d3b9',\n   'choices': [{'finish_reason': 'stop',\n     'index': 0,\n     'logprobs': None,\n     'message': {'content': '{\"category\":\"oos\",\"confidence\":42.52}',\n      'refusal': None,\n      'role': 'assistant',\n      'audio': None,\n      'function_call': None,\n      'tool_calls': [],\n      'reasoning_content': None},\n     'stop_reason': None}],\n   'created': 1751952406,\n   'model': 'Qwen/Qwen3-30B-A3B',\n   'object': 'chat.completion',\n   'service_tier': None,\n   'system_fingerprint': None,\n   'usage': {'completion_tokens': 15,\n    'prompt_tokens': 25334,\n    'total_tokens': 25349,\n    'completion_tokens_details': None,\n    'prompt_tokens_details': None},\n   'prompt_logprobs': None},\n  'error': None},\n {'id': 'batch_req_217e9ee5-b9f8-4e55-8e6d-2db063b3164e',\n  'custom_id': '4',\n  'response': {'id': 'chatcmpl-aad5171b2e334b33aa2af7f6ca9ef271',\n   'choices': [{'finish_reason': 'stop',\n     'index': 0,\n     'logprobs': None,\n     'message': {'content': '{\"category\":\"oos\",\"confidence\":99.99}',\n      'refusal': None,\n      'role': 'assistant',\n      'audio': None,\n      'function_call': None,\n      'tool_calls': [],\n      'reasoning_content': None},\n     'stop_reason': None}],\n   'created': 1751952406,\n   'model': 'Qwen/Qwen3-30B-A3B',\n   'object': 'chat.completion',\n   'service_tier': None,\n   'system_fingerprint': None,\n   'usage': {'completion_tokens': 15,\n    'prompt_tokens': 25334,\n    'total_tokens': 25349,\n    'completion_tokens_details': None,\n    'prompt_tokens_details': None},\n   'prompt_logprobs': None},\n  'error': None}]"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"# Save to JSONL file\nwith open(f\"batch_outputs_{Config.dataset_name}_{Config.start_index}_{Config.end_index}.jsonl\", \"w\") as f:\n    for obj in sorted_objects:\n        f.write(json.dumps(obj) + \"\\n\")  # Write each JSON object as a single line","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T07:18:02.054221Z","iopub.execute_input":"2025-07-11T07:18:02.054597Z","iopub.status.idle":"2025-07-11T07:18:02.437045Z","shell.execute_reply.started":"2025-07-11T07:18:02.054573Z","shell.execute_reply":"2025-07-11T07:18:02.435429Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"import json\nbatchfull = []\n\n# Load as a list of dictionaries\nwith open(f\"batch_outputs_{Config.dataset_name}_{Config.start_index}_{Config.end_index}.jsonl\", \"r\") as f:\n    # batchof10 = [json.loads(line) for line in f if line.strip()]\n    batchfull = [json.loads(line) for line in f]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T07:18:03.428719Z","iopub.execute_input":"2025-07-11T07:18:03.429080Z","iopub.status.idle":"2025-07-11T07:18:04.083921Z","shell.execute_reply.started":"2025-07-11T07:18:03.429057Z","shell.execute_reply":"2025-07-11T07:18:04.083194Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"batchfull[:5]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T07:18:11.589402Z","iopub.execute_input":"2025-07-11T07:18:11.589718Z","iopub.status.idle":"2025-07-11T07:18:11.598832Z","shell.execute_reply.started":"2025-07-11T07:18:11.589691Z","shell.execute_reply":"2025-07-11T07:18:11.598056Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"[{'id': 'batch_req_a383be60-2942-4f39-8d65-857797f68717',\n  'custom_id': '0',\n  'response': {'id': 'chatcmpl-13e8bd394861441982f27c5eff7f9298',\n   'choices': [{'finish_reason': 'stop',\n     'index': 0,\n     'logprobs': None,\n     'message': {'content': '{\"category\":\"oos\",\"confidence\":97.68}',\n      'refusal': None,\n      'role': 'assistant',\n      'audio': None,\n      'function_call': None,\n      'tool_calls': [],\n      'reasoning_content': None},\n     'stop_reason': None}],\n   'created': 1751952408,\n   'model': 'Qwen/Qwen3-30B-A3B',\n   'object': 'chat.completion',\n   'service_tier': None,\n   'system_fingerprint': None,\n   'usage': {'completion_tokens': 15,\n    'prompt_tokens': 25334,\n    'total_tokens': 25349,\n    'completion_tokens_details': None,\n    'prompt_tokens_details': None},\n   'prompt_logprobs': None},\n  'error': None},\n {'id': 'batch_req_c5d6363d-cd60-4c29-9b5b-3ab59993834a',\n  'custom_id': '1',\n  'response': {'id': 'chatcmpl-383cb94a6fd24c39b5237b6f1d398611',\n   'choices': [{'finish_reason': 'stop',\n     'index': 0,\n     'logprobs': None,\n     'message': {'content': '{\"category\":\"oos\",\"confidence\":42.52}',\n      'refusal': None,\n      'role': 'assistant',\n      'audio': None,\n      'function_call': None,\n      'tool_calls': [],\n      'reasoning_content': None},\n     'stop_reason': None}],\n   'created': 1751952406,\n   'model': 'Qwen/Qwen3-30B-A3B',\n   'object': 'chat.completion',\n   'service_tier': None,\n   'system_fingerprint': None,\n   'usage': {'completion_tokens': 15,\n    'prompt_tokens': 25336,\n    'total_tokens': 25351,\n    'completion_tokens_details': None,\n    'prompt_tokens_details': None},\n   'prompt_logprobs': None},\n  'error': None},\n {'id': 'batch_req_b0a7345b-4608-495e-a9ce-513e95fbbe4d',\n  'custom_id': '2',\n  'response': {'id': 'chatcmpl-1f94f3c17d5d4bb687b30eb8d7636645',\n   'choices': [{'finish_reason': 'stop',\n     'index': 0,\n     'logprobs': None,\n     'message': {'content': '{\"category\":\"oos\",\"confidence\":42.52}',\n      'refusal': None,\n      'role': 'assistant',\n      'audio': None,\n      'function_call': None,\n      'tool_calls': [],\n      'reasoning_content': None},\n     'stop_reason': None}],\n   'created': 1751952406,\n   'model': 'Qwen/Qwen3-30B-A3B',\n   'object': 'chat.completion',\n   'service_tier': None,\n   'system_fingerprint': None,\n   'usage': {'completion_tokens': 15,\n    'prompt_tokens': 25332,\n    'total_tokens': 25347,\n    'completion_tokens_details': None,\n    'prompt_tokens_details': None},\n   'prompt_logprobs': None},\n  'error': None},\n {'id': 'batch_req_746bd2f3-30c1-4558-8c8c-e3667e961589',\n  'custom_id': '3',\n  'response': {'id': 'chatcmpl-56c5e2adf38d42cd992f6baeb622d3b9',\n   'choices': [{'finish_reason': 'stop',\n     'index': 0,\n     'logprobs': None,\n     'message': {'content': '{\"category\":\"oos\",\"confidence\":42.52}',\n      'refusal': None,\n      'role': 'assistant',\n      'audio': None,\n      'function_call': None,\n      'tool_calls': [],\n      'reasoning_content': None},\n     'stop_reason': None}],\n   'created': 1751952406,\n   'model': 'Qwen/Qwen3-30B-A3B',\n   'object': 'chat.completion',\n   'service_tier': None,\n   'system_fingerprint': None,\n   'usage': {'completion_tokens': 15,\n    'prompt_tokens': 25334,\n    'total_tokens': 25349,\n    'completion_tokens_details': None,\n    'prompt_tokens_details': None},\n   'prompt_logprobs': None},\n  'error': None},\n {'id': 'batch_req_217e9ee5-b9f8-4e55-8e6d-2db063b3164e',\n  'custom_id': '4',\n  'response': {'id': 'chatcmpl-aad5171b2e334b33aa2af7f6ca9ef271',\n   'choices': [{'finish_reason': 'stop',\n     'index': 0,\n     'logprobs': None,\n     'message': {'content': '{\"category\":\"oos\",\"confidence\":99.99}',\n      'refusal': None,\n      'role': 'assistant',\n      'audio': None,\n      'function_call': None,\n      'tool_calls': [],\n      'reasoning_content': None},\n     'stop_reason': None}],\n   'created': 1751952406,\n   'model': 'Qwen/Qwen3-30B-A3B',\n   'object': 'chat.completion',\n   'service_tier': None,\n   'system_fingerprint': None,\n   'usage': {'completion_tokens': 15,\n    'prompt_tokens': 25334,\n    'total_tokens': 25349,\n    'completion_tokens_details': None,\n    'prompt_tokens_details': None},\n   'prompt_logprobs': None},\n  'error': None}]"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"type(batchfull[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T07:18:15.486180Z","iopub.execute_input":"2025-07-11T07:18:15.486575Z","iopub.status.idle":"2025-07-11T07:18:15.492571Z","shell.execute_reply.started":"2025-07-11T07:18:15.486551Z","shell.execute_reply":"2025-07-11T07:18:15.491588Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"dict"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"dict0 = batchfull[0]\ndict0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T07:18:17.079968Z","iopub.execute_input":"2025-07-11T07:18:17.080368Z","iopub.status.idle":"2025-07-11T07:18:17.087153Z","shell.execute_reply.started":"2025-07-11T07:18:17.080343Z","shell.execute_reply":"2025-07-11T07:18:17.086277Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"{'id': 'batch_req_a383be60-2942-4f39-8d65-857797f68717',\n 'custom_id': '0',\n 'response': {'id': 'chatcmpl-13e8bd394861441982f27c5eff7f9298',\n  'choices': [{'finish_reason': 'stop',\n    'index': 0,\n    'logprobs': None,\n    'message': {'content': '{\"category\":\"oos\",\"confidence\":97.68}',\n     'refusal': None,\n     'role': 'assistant',\n     'audio': None,\n     'function_call': None,\n     'tool_calls': [],\n     'reasoning_content': None},\n    'stop_reason': None}],\n  'created': 1751952408,\n  'model': 'Qwen/Qwen3-30B-A3B',\n  'object': 'chat.completion',\n  'service_tier': None,\n  'system_fingerprint': None,\n  'usage': {'completion_tokens': 15,\n   'prompt_tokens': 25334,\n   'total_tokens': 25349,\n   'completion_tokens_details': None,\n   'prompt_tokens_details': None},\n  'prompt_logprobs': None},\n 'error': None}"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"dict0response = json.loads(dict0[\"response\"][\"choices\"][0][\"message\"][\"content\"])\ndict0response","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T07:18:23.015949Z","iopub.execute_input":"2025-07-11T07:18:23.016310Z","iopub.status.idle":"2025-07-11T07:18:23.023248Z","shell.execute_reply.started":"2025-07-11T07:18:23.016242Z","shell.execute_reply":"2025-07-11T07:18:23.021837Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"{'category': 'oos', 'confidence': 97.68}"},"metadata":{}}],"execution_count":24},{"cell_type":"markdown","source":"## Individual API Call Workings","metadata":{}},{"cell_type":"code","source":"\n#######################\n# Model on 1 Dataset\n#######################\n# Save a list of dictionaries \n# containing a dictionary for each record's\n# - predicted category\n# - confidence level and\n# - original dataframe values\n\n\n# gemini\nuser_secrets = UserSecretsClient()\nNEBIUS_API_KEY = user_secrets.get_secret(\"NEBIUS_API_KEY\")\nclient = OpenAI(base_url=\"https://api.studio.nebius.com/v1/\",\n                api_key = NEBIUS_API_KEY)\n\n@retry(stop=stop_after_attempt(3), wait=wait_fixed(30))\ndef api_llm(client, prompt):\n    try:\n        print(\"CHECKPOINT_3A\")\n        # gemini_config = {\"temperature\": 0,\n        #                  \"response_mime_type\": \"application/json\",\n        #                  \"response_schema\": IntentSchema.model_json_schema(),\n        #                  \"seed\": 38,\n        #                  # # added for \"gemini-2.5-flash-lite-preview-06-17\" model\n        #                  # \"thinking_config\": ThinkingConfig(thinking_budget=-1, \n        #                  #                    include_thoughts=True)\n        #                 }\n        response = client.beta.chat.completions.parse(model = 'Qwen/'+Config.model_name,\n                                                      messages = [{\"role\": \"user\",\n                                                                  \"content\": prompt}],\n                                                      response_format = IntentSchema,\n                                                      seed = 38,\n                                                      temperature = 0\n                                                      )\n        # print(response)\n        # msg = response.parsed\n        response = response.choices[0].message.content\n        print(\"CHECKPOINT_3B\")\n        return response\n    except:\n        print(f\"CHECKPOINT_4A: Exception Type: {type(e).__name__}\")\n        print(f\"CHECKPOINT_4A: Exception Message: {str(e)}\")\n        \n        # Gemini-specific errors\n        if hasattr(e, 'code'):\n            print(f\"CHECKPOINT_4A: Status Code: {e.code}\")\n        if hasattr(e, 'details'):\n            print(f\"CHECKPOINT_4A: Details: {e.details}\")\n        \n        # raise the exception again so retry can work\n        raise\n\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T02:28:29.290722Z","iopub.execute_input":"2025-07-08T02:28:29.291061Z","iopub.status.idle":"2025-07-08T02:28:29.790215Z","shell.execute_reply.started":"2025-07-08T02:28:29.291036Z","shell.execute_reply":"2025-07-08T02:28:29.789327Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from src.config import Config\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T13:05:41.973871Z","iopub.execute_input":"2025-07-07T13:05:41.974414Z","iopub.status.idle":"2025-07-07T13:05:41.979297Z","shell.execute_reply.started":"2025-07-07T13:05:41.974379Z","shell.execute_reply":"2025-07-07T13:05:41.978387Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Config.end_index","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T14:16:52.889806Z","iopub.execute_input":"2025-07-07T14:16:52.890098Z","iopub.status.idle":"2025-07-07T14:16:52.895479Z","shell.execute_reply.started":"2025-07-07T14:16:52.890077Z","shell.execute_reply":"2025-07-07T14:16:52.894686Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_name = Config.model_name\ndf = df\ncategories = bulletpts_intent\nstart_index = Config.start_index\nend_index = Config.end_index\nlog_every_n_examples = Config.log_every_n_examples\n\n\n\nstart_time = time.time()\nresults = []  # Store processed results\n\n# Slice DataFrame based on start/end indices\nif end_index is None:\n    subset_df = df.iloc[start_index:]\nelse:\n    subset_df = df.iloc[start_index:end_index+1]\n\ntotal_rows = len(subset_df)\nsubset_row_count = 0\n\n\n\n\n\nfor row in subset_df.itertuples():\n    subset_row_count+=1\n    prompt = get_prompt(row.dataset, row.split, row.text, categories, fewshot_examples)\n    if subset_row_count == 1:\n        print(\"Example of how prompt looks, for the 1st example in this subset of data\")\n        # print(prompt)\n        break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T02:28:37.79143Z","iopub.execute_input":"2025-07-08T02:28:37.791717Z","iopub.status.idle":"2025-07-08T02:28:37.800178Z","shell.execute_reply.started":"2025-07-08T02:28:37.791699Z","shell.execute_reply":"2025-07-08T02:28:37.799149Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"response = api_llm(client, prompt)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T08:25:07.625516Z","iopub.execute_input":"2025-07-07T08:25:07.626404Z","iopub.status.idle":"2025-07-07T08:25:47.225781Z","shell.execute_reply.started":"2025-07-07T08:25:07.626373Z","shell.execute_reply":"2025-07-07T08:25:47.224544Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"response = client.beta.chat.completions.parse(model = 'Qwen/'+Config.model_name,\n                                              messages = [{\"role\": \"user\",\n                                                          \"content\": prompt}],\n                                              response_format = IntentSchema,\n                                              seed = 38,\n                                              temperature = 0\n                                              )\nresponse","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T14:08:54.89788Z","iopub.execute_input":"2025-07-07T14:08:54.898163Z","iopub.status.idle":"2025-07-07T14:08:57.314335Z","shell.execute_reply.started":"2025-07-07T14:08:54.898142Z","shell.execute_reply":"2025-07-07T14:08:57.313648Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"response2 = client.chat.completions.create(model = 'Qwen/'+Config.model_name,\n                                              messages = [{\"role\": \"user\",\n                                                          \"content\": prompt}],\n                                              seed = 38,\n                                              temperature = 0,\n                                              extra_body = {\"guided_json\": IntentSchema.model_json_schema()}\n                                              )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T14:53:02.503392Z","iopub.execute_input":"2025-07-07T14:53:02.504454Z","iopub.status.idle":"2025-07-07T14:53:04.505864Z","shell.execute_reply.started":"2025-07-07T14:53:02.504416Z","shell.execute_reply":"2025-07-07T14:53:04.504866Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# response2\n# ChatCompletion(id='chatcmpl-f6f492a52fc447d198ee34318e5c1801', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='{\"category\":\"oos\",\"confidence\":0.00}', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[], reasoning_content=None), stop_reason=None)], created=1751899983, model='Qwen/Qwen3-30B-A3B', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=14, prompt_tokens=14012, total_tokens=14026, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None)\n\n# response2.choices[0].message.content\n# '{\"category\":\"oos\",\"confidence\":0.00}'\n\n# json.loads(response2.choices[0].message.content)\n# {'category': 'oos', 'confidence': 0.0}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T14:54:04.527542Z","iopub.execute_input":"2025-07-07T14:54:04.527857Z","iopub.status.idle":"2025-07-07T14:54:04.533676Z","shell.execute_reply.started":"2025-07-07T14:54:04.527797Z","shell.execute_reply":"2025-07-07T14:54:04.532858Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"IntentSchema","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T14:48:56.886024Z","iopub.execute_input":"2025-07-07T14:48:56.88638Z","iopub.status.idle":"2025-07-07T14:48:56.892198Z","shell.execute_reply.started":"2025-07-07T14:48:56.886357Z","shell.execute_reply":"2025-07-07T14:48:56.891313Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"IntentSchema.model_json_schema()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T14:49:20.016628Z","iopub.execute_input":"2025-07-07T14:49:20.017021Z","iopub.status.idle":"2025-07-07T14:49:20.030741Z","shell.execute_reply.started":"2025-07-07T14:49:20.016992Z","shell.execute_reply":"2025-07-07T14:49:20.029796Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# response.__dict__\n\"\"\"\n{'id': 'chatcmpl-20151be35483463a9136d12957c6e0ec',\n 'choices': [ParsedChoice[IntentSchema](finish_reason='stop', index=0, logprobs=None, message=ParsedChatCompletionMessage[IntentSchema](content='{\"category\":\"oos\",\"confidence\":0.00}', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, parsed=IntentSchema(category='oos', confidence=0.0), reasoning_content=None), stop_reason=None)],\n 'created': 1751897335,\n 'model': 'Qwen/Qwen3-30B-A3B',\n 'object': 'chat.completion',\n 'service_tier': None,\n 'system_fingerprint': None,\n 'usage': CompletionUsage(completion_tokens=14, prompt_tokens=14012, total_tokens=14026, completion_tokens_details=None, prompt_tokens_details=None),\n '_request_id': None}\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T14:09:15.334007Z","iopub.execute_input":"2025-07-07T14:09:15.334843Z","iopub.status.idle":"2025-07-07T14:09:15.339838Z","shell.execute_reply.started":"2025-07-07T14:09:15.334817Z","shell.execute_reply":"2025-07-07T14:09:15.339067Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# response.choices\n# [ParsedChoice[IntentSchema](finish_reason='stop', index=0, logprobs=None, message=ParsedChatCompletionMessage[IntentSchema](content='{\"category\":\"oos\",\"confidence\":0.00}', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, parsed=IntentSchema(category='oos', confidence=0.0), reasoning_content=None), stop_reason=None)]\n\n# response.choices[0]\n# ParsedChoice[IntentSchema](finish_reason='stop', index=0, logprobs=None, message=ParsedChatCompletionMessage[IntentSchema](content='{\"category\":\"oos\",\"confidence\":0.00}', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, parsed=IntentSchema(category='oos', confidence=0.0), reasoning_content=None), stop_reason=None)\n\n# response.choices[0].__dict__\n# \"\"\"\n# {'finish_reason': 'stop',\n#  'index': 0,\n#  'logprobs': None,\n#  'message': ParsedChatCompletionMessage[IntentSchema](content='{\"category\":\"oos\",\"confidence\":0.00}', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, parsed=IntentSchema(category='oos', confidence=0.0), reasoning_content=None)}\n# \"\"\"\n\n# response.choices[0].message.content\n# '{\"category\":\"oos\",\"confidence\":0.00}'\n\n# type(response.choices[0].message.content)\n# str\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T14:10:02.430919Z","iopub.execute_input":"2025-07-07T14:10:02.431216Z","iopub.status.idle":"2025-07-07T14:10:02.436803Z","shell.execute_reply.started":"2025-07-07T14:10:02.431188Z","shell.execute_reply":"2025-07-07T14:10:02.435646Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef predict_intent(model_name, df, categories, start_index=0, end_index=None, log_every_n_examples=100):\n    start_time = time.time()\n    results = []  # Store processed results\n    \n    # Slice DataFrame based on start/end indices\n    if end_index is None:\n        subset_df = df.iloc[start_index:]\n    else:\n        subset_df = df.iloc[start_index:end_index+1]\n    \n    total_rows = len(subset_df)\n    subset_row_count = 0\n\n    \n\n    \n    \n    for row in subset_df.itertuples():\n        subset_row_count+=1\n        prompt = get_prompt(row.dataset, row.split, row.text, categories, fewshot_examples)\n        if subset_row_count == 1:\n            print(\"Example of how prompt looks, for the 1st example in this subset of data\")\n            print(prompt)\n\n            print(\"Example of how IntentSchema looks\")\n            print(IntentSchema.model_json_schema())\n        \n        \n        try:\n            print(\"CHECKPOINT_1A\")\n            \n            # response = ollama.chat(model=model_name, \n            #                        messages=[\n            #                                     {'role': 'user', 'content': prompt}\n            #                                 ],\n            #                        format = IntentSchema.model_json_schema(),\n            #                        options = {'temperature': 0},  # Set temperature to 0 for a more deterministic output\n            #                       )\n            # msg = response['message']['content']\n            # parsed = json.loads(msg)\n            \n            response = api_llm(_client, prompt)\n            print(\"CHECKPOINT_1B\")\n            parsed = json.loads(response.text)\n            # parsed = response.parsed\n            print(\"CHECKPOINT_1C\")\n                        \n            # Safely extract keys with defaults - resolve parsing error\n            # maybe LLM did not output a particular key-value pair\n            category = parsed.get('category', 'error')\n            confidence = parsed.get('confidence', 0.0)\n            parsed = {'category': category, 'confidence': confidence}\n        except (json.JSONDecodeError, KeyError, Exception) as e:\n            print(f\"CHECKPOINT_2A: Exception Type: {type(e).__name__}\")\n            print(f\"CHECKPOINT_2A: Exception Message: {str(e)}\")\n            \n            # Gemini-specific errors\n            if hasattr(e, 'code'):\n                print(f\"CHECKPOINT_2A: Status Code: {e.code}\")\n            if hasattr(e, 'details'):\n                print(f\"CHECKPOINT_2A: Details: {e.details}\")\n                \n            parsed = {'category': 'error', 'confidence': 0.0}\n        \n        # Combine original row data with predictions\n        results.append({\n            \"Index\": row.Index,\n            \"text\": row.text,\n            \"label\": row.label,\n            \"dataset\": row.dataset,\n            \"split\": row.split,\n            \"predicted\": parsed['category'],\n            \"confidence\": parsed['confidence']\n        })\n\n        \n        # Log progress\n        if subset_row_count % log_every_n_examples == 0:\n            elapsed_time = time.time() - start_time\n            \n            avg_time_per_row = elapsed_time / subset_row_count\n            remaining_rows = total_rows - subset_row_count\n            eta = avg_time_per_row * remaining_rows\n            \n            print(f\"Processed original df idx {row.Index} (subset row {subset_row_count}) | \"\n                  f\"Elapsed: {elapsed_time:.2f}s | ETA: {eta:.2f}s\")\n    \n    return results  # Return list of dictionaries\n    \n\nprint(f\"Starting intent classification using {Config.model_name}\")\nsubset_results = predict_intent(Config.model_name, \n                                df, \n                                bulletpts_intent, \n                                start_index = Config.start_index, \n                                end_index = Config.end_index,\n                                log_every_n_examples = Config.log_every_n_examples)\n\n\n\n# # previously for Ollama\n# # update end_index for filename (if None is used for the end of the df)\n# # Get the last index of the DataFrame\n# last_index = df.index[-1] \n# # Use last index if Config.end_index is None\n# end_index = Config.end_index if Config.end_index is not None else last_index\n# 2025.07.07\n# now for Ollama AND Gemini\n# Gemini - needs to track 'end_index' for API JSON exports (when daily limits are exhausted)\n# Ollama - reuse this code\nend_index = max(r['Index'] for r in subset_results)\n\n\n\n# 2025.05.23 changed from JSON to PKL\n# because we are saving list of dictionaries\n# Save to PKL\n# 2025.06.04 explore changing back to JSON\n# with open(f'results_{Config.model_name}_{Config.dataset_name}_{Config.start_index}_{end_index}.pkl', 'wb') as f:\n#     pickle.dump(subset_results, f)\nwith open(f'results_{Config.model_name}_{Config.dataset_name}_{Config.start_index}_{end_index}.json', 'w') as f:\n    json.dump(subset_results, f, indent=2)\n\nprint(\"Completed intent classification\")\n\n\n#######################\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
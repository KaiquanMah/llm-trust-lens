{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":249340378,"sourceType":"kernelVersion"},{"sourceId":249345702,"sourceType":"kernelVersion"},{"sourceId":249921419,"sourceType":"kernelVersion"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Refactored code for\n* Setting up and running Ollama in Kaggle\n* Downloading THUIAR dataset\n* Zero-Shot Prompt\n* Use LLM to classify intent from an input 'question' dataset\n* To configure your file/folder paths, LLM, dataset, start_index and end_index for each run, please update the config.py file\n\nThis notebook will also be used as the base to test any fixes to the LLM intent classification pipeline.\n* 2025.05.26: Updated results output file from JSON to Pickle, to store list of dictionaries. 1 dictionary contains the results for each record. Lists of dictionaries can be downloaded from multiple notebooks, then concatenated for analysis\n* 2025.05.30: Update prompt and bulletpts_intent.\n  * Check if dataset contains 'oos' (out of scope) category\n  * If dataset has no 'oos' (out of scope) category, turn 1 category into 'oos'. Use updated categories in bulletpts_intent. Also update prompt instructions on when to classify an example as 'oos'\n  * **This force_oos fix is implemented in [notebook 01E](https://www.kaggle.com/code/kaiquanmah/01e-kaggle-ollama-llama3-2-w-force-oos?scriptVersionId=242648764)**\n* 2025.05.30: Add pydantic schema with enums\n  * From an analysis of errors, the model previously had a 45% average accuracy rate across categories. The model predicted a set of categories outside of what we gave it in 'bulletpts_intent'\n  * To fix this, we will try to implement a pydantic schema solution for the model to only predict categories from the allowed list of categories ('bulletpts_intent')\n* 2025.05.30: Set Ollama chat temperature to 0\n  * Previously, we used the default temperature of 0.8, which might have caused the model to predict categories we did not provide to it ([Reading](https://docs.spring.io/spring-ai/reference/api/chat/ollama-chat.html))\n  * **The pydantic schema and temperature fixes are implemented in [notebook 01F](https://www.kaggle.com/code/kaiquanmah/01f-kaggle-ollama-llama3-2-w-pydantic-schema)**\n* 2025.06.03:\n  1. **Remove 'oos' from `bulletpts_intent` input into prompt**, to be consistent with the team's approach when exploring embedding approaches to classify 'oos' examples. **Keep 'oos' in pydantic enums/Literal (for LLM to output 'oos' as an allowed class value)**\n  2. **Remove 0.99 when defining the prompt format - to avoid anchoring LLM on outputting confidence of 0.99**\n  3. **Added ability for user to define which classes are 'oos'**\n  * **These 3 fixes are in [notebook 01G](https://www.kaggle.com/code/kaiquanmah/01g-kaggle-ollama-llama3-2-oos-update)**\n* 2025.06.10:\n  * From an error analysis earlier, **models can get confused between similar intent classes**\n  * Therefore **we will analyse similar intent classes/labels -> get their indexes -> put them into 'oos' in [notebook 01H](https://www.kaggle.com/code/kaiquanmah/01h1-openintent-ollama-llama3-2-3b-banking77)**\n  * **Going from zero-shot prompt previously, to few-shot prompt (with 5 examples) from known intents**. These 5 examples were **non-oos, and misclassified previously**. This 'fix' is in **[notebook 01i](https://www.kaggle.com/code/kaiquanmah/01i1-openintent-ollama-llama3-2-3b-banking77)**\n* 2025.06.16:\n  * For known intents (ie not in the 'oos' class), give 5 examples each in the few-shot prompt **[notebook 01J](https://www.kaggle.com/code/kaiquanmah/01j1-openintent-ollama-llama3-2-3b-banking77)**\n* 2025.06.17:\n  * Now we explore how changing the number of known intent classes affects the recall of oos in **[notebook 01K](https://www.kaggle.com/code/kaiquanmah/01k1-openintent-ollama-llama3-2-3b-banking77)**\n  * For quick experimentation, we implement (1) fewshot prompt with 1 example per known intent class, (2) changing number of known intent classes in various notebook runs, (3) 100 oos sentences for the model to classify (taking from first class for banking77 and stackoverflow dataset, or the oos class for CLINC150 oos dataset)\n    * For (3) - Added 'first_class' variable for each dataset to Config\n    * For (3) - Created new fn to filter and keep 100 records from 'first/oos class' to input to the model to classify\n* 2025.07.07:\n  * Explore free, rate-limited API model (such as Gemini) in **[notebook 01L](https://www.kaggle.com/code/kaiquanmah/01l1-openintent-gemini-banking77-explore)**\n  * Added retry for when we exhaust API limits per minute\n  * Updated end_index tracking that works with Ollama and Gemini when generating JSON results file\n  * **Explore Qwen model from the Nebius platform**","metadata":{}},{"cell_type":"code","source":"# 1. create dirs if they do not exist\nimport os\nos.makedirs('/kaggle/working/src', exist_ok=True)\nos.makedirs('/kaggle/working/prediction', exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T09:01:37.116690Z","iopub.execute_input":"2025-07-11T09:01:37.117060Z","iopub.status.idle":"2025-07-11T09:01:37.122829Z","shell.execute_reply.started":"2025-07-11T09:01:37.117036Z","shell.execute_reply":"2025-07-11T09:01:37.121266Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"%%writefile /kaggle/working/src/setup_ollama.py\nimport os\nimport subprocess\nimport time\nfrom src.config import Config # absolute import\n\n# 1. Install Ollama (if not already installed)\ntry:\n    # Check if Ollama is already installed\n    subprocess.run([\"ollama\", \"--version\"], capture_output=True, check=True)\n    print(\"Ollama is already installed.\")\nexcept FileNotFoundError:\n    print(\"Installing Ollama...\")\n    subprocess.run(\"curl -fsSL https://ollama.com/install.sh  | sh\", shell=True, check=True)\n\n# 2. Start Ollama server in the background\nprint(\"Starting Ollama server...\")\nprocess = subprocess.Popen(\"ollama serve\", shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n\n# Wait for the server to initialize\ntime.sleep(5)\n\n\n# 3. Pull the model\nmodel_name = Config.model_name\nprint(f\"Pulling {model_name} model...\")\nsubprocess.run([\"ollama\", \"pull\", model_name], check=True)\n\n# 4. Install Python client\nsubprocess.run([\"pip\", \"install\", \"ollama\"], check=True)\n\nprint(\"Ollama setup complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T09:01:37.124766Z","iopub.execute_input":"2025-07-11T09:01:37.125092Z","iopub.status.idle":"2025-07-11T09:01:37.147396Z","shell.execute_reply.started":"2025-07-11T09:01:37.125063Z","shell.execute_reply":"2025-07-11T09:01:37.146333Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/src/setup_ollama.py\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"%%writefile requirements.txt\npandas\npydantic\ntyping\nhuggingface-hub\n# google-genai # only used for gemini model\nopenai # used for openrouter's gemini model\ntenacity # for gemini model retries\n# numpy\n# enum","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T09:01:37.148296Z","iopub.execute_input":"2025-07-11T09:01:37.148569Z","iopub.status.idle":"2025-07-11T09:01:37.172426Z","shell.execute_reply.started":"2025-07-11T09:01:37.148544Z","shell.execute_reply":"2025-07-11T09:01:37.171528Z"}},"outputs":[{"name":"stdout","text":"Overwriting requirements.txt\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"%%writefile /kaggle/working/src/__init__.py\n# folder for config","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T09:01:37.174330Z","iopub.execute_input":"2025-07-11T09:01:37.174688Z","iopub.status.idle":"2025-07-11T09:01:37.249459Z","shell.execute_reply.started":"2025-07-11T09:01:37.174665Z","shell.execute_reply":"2025-07-11T09:01:37.248292Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/src/__init__.py\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"%%writefile /kaggle/working/src/config.py\nclass Config:\n    #######################################################\n    # working directory for files\n    #######################################################\n    target_dir = '/kaggle/working/data' # data directory to clone into\n    cloned_data_dir = target_dir + '/data'\n    prediction_dir = target_dir + '/prediction'\n    #######################################################\n    # dataset and model\n    #######################################################\n    dataset_name = 'stackoverflow' # UPDATE options: 'banking', 'stackoverflow', 'oos'\n    idx2label_target_dir = '/kaggle/working/idx2label'\n    idx2label_filename_hf = 'stackoverflow_idx2label.csv' # UPDATE options: banking77_idx2label.csv, stackoverflow_idx2label.csv, clinc150_oos_idx2label.csv\n    fewshot_examples_dir = '/kaggle/working/fewshot'\n    fewshot_subdir = '/fewshot-5examples-per-nonoos/'\n    fewshot_examples_filename = 'stackoverflow_25perc_oos.txt' # UPDATE options: banking_25perc_oos.txt, stackoverflow_25perc_oos.txt, oos_25perc_oos.txt\n    list_oos_idx = [0, 3, 10, 12, 14] # UPDATE gathered from within the team - for reproducible, comparable results with other open intent classification approaches\n    model_name = 'Qwen3-30B-A3B' # 'gemma-2-9b-it-fast'\n    start_index=0 # eg: 0, 10001, 11851\n    end_index=None # eg: 10, 10000, 11850 or None (use end_index=None to process the full dataset)\n    log_every_n_examples=10 # 2\n    force_oos = True  # NEW: Add flag to force dataset to contain 'oos' class for the last class value (sorted alphabetically), if 'oos' class does not exist in the original dataset\n    #######################################################\n    # evaluate threshold when 'oos' recall drops\n    #######################################################\n    filter_oos_qns_only = False # True (when you are testing 'oos' recall threshold), False\n    n_oos_qns = 100\n    first_class_banking = 'activate_my_card' # following idx2label\n    first_class_stackoverflow = 'wordpress' # following idx2label\n    first_class_oos = 'oos'\n    #######################################################","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T09:01:37.250722Z","iopub.execute_input":"2025-07-11T09:01:37.251049Z","iopub.status.idle":"2025-07-11T09:01:37.275044Z","shell.execute_reply.started":"2025-07-11T09:01:37.251027Z","shell.execute_reply":"2025-07-11T09:01:37.273972Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/src/config.py\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"%%writefile download_dataset.py\nfrom src.config import Config\nimport os\nimport subprocess\ntarget_dir = Config.target_dir # data directory to clone into\ncloned_data_dir = Config.cloned_data_dir\n\n# Create target directory if it doesn't exist\nos.makedirs(target_dir, exist_ok=True)\n\n# do not clone dataset repo if cloned data folder exists\nif os.path.exists(cloned_data_dir):\n    print(\"Dataset has already been downloaded. If this is incorrect, please delete the Adaptive-Decision-Boundary 'data' folder.\")\nelse:\n    # Clone the repository\n    subprocess.run([\"git\",\n                    \"clone\",\n                    \"https://github.com/thuiar/Adaptive-Decision-Boundary.git\",\n                    target_dir\n                   ])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T09:01:37.276271Z","iopub.execute_input":"2025-07-11T09:01:37.276674Z","iopub.status.idle":"2025-07-11T09:01:37.304762Z","shell.execute_reply.started":"2025-07-11T09:01:37.276646Z","shell.execute_reply":"2025-07-11T09:01:37.303719Z"}},"outputs":[{"name":"stdout","text":"Overwriting download_dataset.py\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"%%writefile predict_class.py\nfrom src.config import Config\nimport pandas as pd\nimport os\n# import ollama\nimport json\nimport pickle\nimport time\nfrom pydantic import BaseModel\nfrom typing import Literal\n# from enum import Enum\nfrom huggingface_hub import snapshot_download\n    \n###################\n# Gemini API\n###################\n# from google import genai\n# from google.genai.types import ThinkingConfig\n# from google.api_core import retry\nfrom openai import OpenAI\nfrom tenacity import retry, stop_after_attempt, wait_fixed\nfrom kaggle_secrets import UserSecretsClient\n\n\n###################\n\n\n# Config.target_dir\n# Config.cloned_data_dir'\n# Config.dataset_name\n# Config.model_name\n# Config.start_index\n# Config.end_index\n# Config.log_every_n_examples\n\n\n#######################\n# load data\n#######################\ndef load_data(data_dir):\n    \"\"\"Loads train, dev, and test datasets from a specified directory.\"\"\"\n\n    main_df = pd.DataFrame()\n    for split in ['train', 'dev', 'test']:\n        file_path = os.path.join(data_dir, f'{split}.tsv')\n        if os.path.exists(file_path):\n          try:\n            df = pd.read_csv(file_path, sep='\\t')\n            df['dataset'] = os.path.basename(data_dir)\n            df['split'] = split\n            main_df = pd.concat([main_df, df], ignore_index=True)\n          except pd.errors.ParserError as e:\n            print(f\"Error parsing {file_path}: {e}\")\n            # Handle the error appropriately, e.g., skip the file, log the error, etc.\n        else:\n            print(f\"Warning: {split}.tsv not found in {data_dir}\")\n    return main_df\n\n\ndef filter100examples_oos(dataset_name, df):\n    # dont input 'only oos qns to model'\n    if Config.filter_oos_qns_only == False:\n        filtered_df = df\n    # vs\n    # input 'only oos qns to model'\n    else:\n        if dataset_name == 'banking':\n            first_class = Config.first_class_banking\n        elif dataset_name == 'stackoverflow':\n            first_class = Config.first_class_stackoverflow\n        else:\n            first_class = Config.first_class_oos\n    \n        filtered_df = df.copy()\n        filtered_df = filtered_df.loc[filtered_df[\"label\"] == first_class]\n        filtered_df = filtered_df.sample(n=Config.n_oos_qns, random_state=38)\n    return filtered_df\n\n\ndf = pd.DataFrame()\n\ndata_dir = os.path.join(Config.cloned_data_dir, Config.dataset_name)\nif os.path.exists(data_dir):\n  df = load_data(data_dir)\n  print(f\"Loaded dataset into dataframe: {Config.dataset_name}\")\n  print(f\"Dimensions: {df.shape}\")\n  print(f\"Col names: {df.columns}\")\nelse:\n  print(f\"Warning: Directory {data_dir} not found.\")\n#######################\n\n\n\n#######################\n# unique intents\n#######################\nsorted_intent = list(sorted(df.label.unique()))\nprint(\"=\"*80)\nprint(f\"Original dataset intents: {sorted_intent}\")\nprint(f\"Number of original intents: {len(sorted_intent)}\\n\")\n\n\n# 2025.06.03\n# New OOS approach - get 25/50/75% of class indexes for each dataset within the team (for reproducibility and comparable results)\n# Change their class labels to 'oos'\nsnapshot_download(repo_id=\"KaiquanMah/open-intent-query-classification\", repo_type=\"space\", allow_patterns=\"*_idx2label.csv\", local_dir=Config.idx2label_target_dir)\nidx2label_filepath = Config.idx2label_target_dir + '/dataset_idx2label/' + Config.idx2label_filename_hf\nidx2label = pd.read_csv(idx2label_filepath)\nidx2label_oos = idx2label[idx2label.index.isin(Config.list_oos_idx)]\nidx2label_oos.reset_index(drop=True, inplace=True)\n\n# 2025.06.17 keep track of non-oos labels, to use in IntentSchema\nnonoos_labels = idx2label[~idx2label.label.isin(Config.list_oos_idx)]['label'].values\nprint(\"=\"*80)\nprint(\"Original intents to convert to OOS class\")\nprint(idx2label_oos)\nprint(f\"Percentage of original intents to convert to OOS class: {len(idx2label_oos)/len(idx2label)}\\n\")\n\noos_labels = idx2label_oos['label'].values\nlist_sorted_intent_aft_conversion = ['oos' if intent.lower() in oos_labels else intent for intent in sorted_intent]\nlist_sorted_intent_aft_conversion_deduped = sorted(set(list_sorted_intent_aft_conversion))\nprint(\"=\"*80)\nprint(\"Unique intents after converting some to OOS class\")\nprint(list_sorted_intent_aft_conversion_deduped)\nprint(f\"Number of unique intents after converting some to OOS class: {len(list_sorted_intent_aft_conversion_deduped)}\\n\")\n\n\n\n# unique intents - from set to bullet points (to use in prompts)\n# bulletpts_intent = \"\\n\".join(f\"- {category}\" for category in set_intent)\n# 2025.06.03: do not show 'oos' in the prompt (to avoid leakage of 'oos' class)\nbulletpts_intent = \"\\n\".join(f\"- {category}\" for category in list_sorted_intent_aft_conversion_deduped if category and (category!='oos'))\n\n# 2025.06.04: fix adjustment if 'oos' is already in the original dataset\nint_oos_in_orig_dataset = int('oos' in idx2label.label.values)\nadjust_if_oos_not_in_orig_dataset = [0 if int_oos_in_orig_dataset == 1 else 1][0]\n\nprint(\"=\"*80)\nprint(\"sanity check\")\nprint(f\"Number of original intents: {len(sorted_intent)}\")\nprint(f\"Number of original intents + 1 OOS class (if doesnt exist in original dataset): {len(sorted_intent) + adjust_if_oos_not_in_orig_dataset}\")\nprint(f\"Number of original intents to convert to OOS class: {len(idx2label_oos)}\")\nprint(f\"Percentage of original intents to convert to OOS class: {len(idx2label_oos)/len(idx2label)}\")\nprint(f\"Number of unique intents after converting some to OOS class: {len(list_sorted_intent_aft_conversion_deduped)}\")\nprint(f\"Number of original intents + 1 OOS class (if doesnt exist in original dataset) - converted classes: {len(sorted_intent) + adjust_if_oos_not_in_orig_dataset - len(idx2label_oos)}\")\nprint(f\"Numbers match: {(len(sorted_intent) + adjust_if_oos_not_in_orig_dataset - len(idx2label_oos)) == len(list_sorted_intent_aft_conversion_deduped)}\")\nprint(\"Prepared unique intents\")\n#######################\n\n\n\n\n#######################\n# Enforce schema on the model (e.g. allowed list of predicted categories)\n#######################\n\nclass IntentSchema(BaseModel):\n    # dynamically unpack list of categories for different dataset(s)\n    category: Literal[*list_sorted_intent_aft_conversion_deduped]\n    confidence: float\n    \n#######################\n\n\n\n\n#######################\n# filter after preparing intents\n#######################\ndf = filter100examples_oos(Config.dataset_name, df)\nprint(\"Filtered dataset\")\nprint(f\"Dimensions: {df.shape}\")\nprint(f\"Col names: {df.columns}\")\n#######################\n\n\n\n#######################\n# Prompt\n#######################\n# prompt 2 with less information/compute, improve efficiency\n# 2025.06.10 prompt 3 with 5 few shot examples only - notebook O1H1, O1i1\n# 2025.06.16 prompt 4 with 5 examples per each known intent (ie non-oos intent) - notebook 01J1\nsnapshot_download(repo_id=\"KaiquanMah/open-intent-query-classification\", repo_type=\"space\", allow_patterns=\"*.txt\", local_dir=Config.fewshot_examples_dir)\nwith open(Config.fewshot_examples_dir + Config.fewshot_subdir + Config.fewshot_examples_filename, 'r') as file:\n    fewshot_examples = file.read()\n\ndef get_prompt(dataset_name, split, question, categories, fewshot_examples):\n    \n    prompt = f'''\nYou are an expert in understanding and identifying what users are asking you.\n\nYour task is to analyze an input query from a user and assign the most appropriate category from the following list:\n{categories}\n\nOnly classify as \"oos\" (out of scope category) if none of the other categories apply.\n\nBelow are several examples to guide your classification:\n\n---\n{fewshot_examples}\n---\n\n===============================\n\nNew Question: {question}\n\n===============================\n\nProvide your final classification in **valid JSON format** with the following structure:\n{{\n  \"category\": \"your_chosen_category_name\",\n  \"confidence\": confidence_level_rounded_to_the_nearest_2_decimal_places\n}}\n\n\nEnsure the JSON has:\n- Opening and closing curly braces\n- Double quotes around keys and string values\n- Confidence as a number (not a string), with maximum 2 decimal places\n\nDo not include any explanations or extra text.\n            '''\n    return prompt\n\n\n\n#######################\n\n\n#######################\n# Model on 1 Dataset\n#######################\n# Save a list of dictionaries \n# containing a dictionary for each record's\n# - predicted category\n# - confidence level and\n# - original dataframe values\n\n\n# gemini\nuser_secrets = UserSecretsClient()\nNEBIUS_API_KEY = user_secrets.get_secret(\"NEBIUS_API_KEY\")\nclient = OpenAI(base_url=\"https://api.studio.nebius.com/v1/\",\n                api_key = NEBIUS_API_KEY)\n\n@retry(stop=stop_after_attempt(3), wait=wait_fixed(30))\ndef api_llm(client, prompt):\n    try:\n        print(\"CHECKPOINT_3A\")\n        # gemini_config = {\"temperature\": 0,\n        #                  \"response_mime_type\": \"application/json\",\n        #                  \"response_schema\": IntentSchema.model_json_schema(),\n        #                  \"seed\": 38,\n        #                  # # added for \"gemini-2.5-flash-lite-preview-06-17\" model\n        #                  # \"thinking_config\": ThinkingConfig(thinking_budget=-1, \n        #                  #                    include_thoughts=True)\n        #                 }\n        response = client.beta.chat.completions.parse(model = 'Qwen/'+Config.model_name,\n                                                      messages = [{\"role\": \"user\",\n                                                                  \"content\": prompt}],\n                                                      response_format = IntentSchema,\n                                                      seed = 38,\n                                                      temperature = 0\n                                                      )\n        # print(response)\n        # msg = response.parsed\n        response = response.choices[0].message.content\n        print(\"CHECKPOINT_3B\")\n        return response\n    except:\n        print(f\"CHECKPOINT_4A: Exception Type: {type(e).__name__}\")\n        print(f\"CHECKPOINT_4A: Exception Message: {str(e)}\")\n        \n        # Gemini-specific errors\n        if hasattr(e, 'code'):\n            print(f\"CHECKPOINT_4A: Status Code: {e.code}\")\n        if hasattr(e, 'details'):\n            print(f\"CHECKPOINT_4A: Details: {e.details}\")\n        \n        # raise the exception again so retry can work\n        raise\n\n    \n\ndef predict_intent(model_name, df, categories, start_index=0, end_index=None, log_every_n_examples=100):\n    start_time = time.time()\n    results = []  # Store processed results\n    \n    # Slice DataFrame based on start/end indices\n    if end_index is None:\n        subset_df = df.iloc[start_index:]\n    else:\n        subset_df = df.iloc[start_index:end_index+1]\n    \n    total_rows = len(subset_df)\n    subset_row_count = 0\n\n    \n\n    \n    \n    for row in subset_df.itertuples():\n        subset_row_count+=1\n        prompt = get_prompt(row.dataset, row.split, row.text, categories, fewshot_examples)\n        if subset_row_count == 1:\n            print(\"Example of how prompt looks, for the 1st example in this subset of data\")\n            print(prompt)\n\n            print(\"Example of how IntentSchema looks\")\n            print(IntentSchema.model_json_schema())\n        \n        \n        try:\n            print(\"CHECKPOINT_1A\")\n            \n            # response = ollama.chat(model=model_name, \n            #                        messages=[\n            #                                     {'role': 'user', 'content': prompt}\n            #                                 ],\n            #                        format = IntentSchema.model_json_schema(),\n            #                        options = {'temperature': 0},  # Set temperature to 0 for a more deterministic output\n            #                       )\n            # msg = response['message']['content']\n            # parsed = json.loads(msg)\n            \n            response = api_llm(client, prompt)\n            print(\"CHECKPOINT_1B\")\n            parsed = json.loads(response.text)\n            # parsed = response.parsed\n            print(\"CHECKPOINT_1C\")\n                        \n            # Safely extract keys with defaults - resolve parsing error\n            # maybe LLM did not output a particular key-value pair\n            category = parsed.get('category', 'error')\n            confidence = parsed.get('confidence', 0.0)\n            parsed = {'category': category, 'confidence': confidence}\n        except (json.JSONDecodeError, KeyError, Exception) as e:\n            print(f\"CHECKPOINT_2A: Exception Type: {type(e).__name__}\")\n            print(f\"CHECKPOINT_2A: Exception Message: {str(e)}\")\n            \n            # Gemini-specific errors\n            if hasattr(e, 'code'):\n                print(f\"CHECKPOINT_2A: Status Code: {e.code}\")\n            if hasattr(e, 'details'):\n                print(f\"CHECKPOINT_2A: Details: {e.details}\")\n                \n            parsed = {'category': 'error', 'confidence': 0.0}\n        \n        # Combine original row data with predictions\n        results.append({\n            \"Index\": row.Index,\n            \"text\": row.text,\n            \"label\": row.label,\n            \"dataset\": row.dataset,\n            \"split\": row.split,\n            \"predicted\": parsed['category'],\n            \"confidence\": parsed['confidence']\n        })\n\n        \n        # Log progress\n        if subset_row_count % log_every_n_examples == 0:\n            elapsed_time = time.time() - start_time\n            \n            avg_time_per_row = elapsed_time / subset_row_count\n            remaining_rows = total_rows - subset_row_count\n            eta = avg_time_per_row * remaining_rows\n            \n            print(f\"Processed original df idx {row.Index} (subset row {subset_row_count}) | \"\n                  f\"Elapsed: {elapsed_time:.2f}s | ETA: {eta:.2f}s\")\n    \n    return results  # Return list of dictionaries\n    \n\nprint(f\"Starting intent classification using {Config.model_name}\")\nsubset_results = predict_intent(Config.model_name, \n                                df, \n                                bulletpts_intent, \n                                start_index = Config.start_index, \n                                end_index = Config.end_index,\n                                log_every_n_examples = Config.log_every_n_examples)\n\n\n\n# # previously for Ollama\n# # update end_index for filename (if None is used for the end of the df)\n# # Get the last index of the DataFrame\n# last_index = df.index[-1] \n# # Use last index if Config.end_index is None\n# end_index = Config.end_index if Config.end_index is not None else last_index\n# 2025.07.07\n# now for Ollama AND Gemini\n# Gemini - needs to track 'end_index' for API JSON exports (when daily limits are exhausted)\n# Ollama - reuse this code\nend_index = max(r['Index'] for r in subset_results)\n\n\n\n# 2025.05.23 changed from JSON to PKL\n# because we are saving list of dictionaries\n# Save to PKL\n# 2025.06.04 explore changing back to JSON\n# with open(f'results_{Config.model_name}_{Config.dataset_name}_{Config.start_index}_{end_index}.pkl', 'wb') as f:\n#     pickle.dump(subset_results, f)\nwith open(f'results_{Config.model_name}_{Config.dataset_name}_{Config.start_index}_{end_index}.json', 'w') as f:\n    json.dump(subset_results, f, indent=2)\n\nprint(\"Completed intent classification\")\n\n\n#######################\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T09:01:37.306657Z","iopub.execute_input":"2025-07-11T09:01:37.307012Z","iopub.status.idle":"2025-07-11T09:01:37.338270Z","shell.execute_reply.started":"2025-07-11T09:01:37.306981Z","shell.execute_reply":"2025-07-11T09:01:37.337036Z"}},"outputs":[{"name":"stdout","text":"Overwriting predict_class.py\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"%%writefile /kaggle/working/main.py\nimport subprocess\nimport sys\nfrom src.config import Config\n\n\n# 1. Install libraries from requirements.txt\nprint(\"Installing dependencies...\")\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", \"/kaggle/working/requirements.txt\"], check=True)\n\n\n# # 2. Run setup_ollama.py\n# if 'gemini' not in Config.model_name:\n#     print(\"Starting Ollama setup...\")\n#     # subprocess.run([\"python3\", \"/kaggle/working/src/setup_ollama.py\"], check=True)\n#     print(\"Starting Ollama setup...\")\n#     subprocess.run(\n#         [\"python3\", \"-m\", \"src.setup_ollama\"],  # Run as a module\n#         cwd=\"/kaggle/working\",  # Set working directory to parent of 'src'\n#         check=True\n#     )\n    \n\n# 3. Run download_dataset.py\nprint(\"Downloading dataset...\")\nsubprocess.run([\"python3\", \"/kaggle/working/download_dataset.py\"], check=True)\n\n# 4. Run predict_class.py\nprint(\"Running prediction script...\")\nsubprocess.run([\"python3\", \"/kaggle/working/predict_class.py\"], check=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T09:01:37.339726Z","iopub.execute_input":"2025-07-11T09:01:37.340116Z","iopub.status.idle":"2025-07-11T09:01:37.367934Z","shell.execute_reply.started":"2025-07-11T09:01:37.340087Z","shell.execute_reply":"2025-07-11T09:01:37.366789Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/main.py\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"# Model on subset of examples","metadata":{}},{"cell_type":"code","source":"!python3 /kaggle/working/main.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T06:42:43.036942Z","iopub.execute_input":"2025-07-07T06:42:43.037254Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Sanity check folders","metadata":{}},{"cell_type":"code","source":"!cd /kaggle/working/ && ls -la","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!cd /kaggle/working/src && ls -la","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!cd /kaggle/working/data/data && ls -la","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# idx2label_oos examples","metadata":{}},{"cell_type":"code","source":"pip install huggingface-hub","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import snapshot_download\nsnapshot_download(repo_id=\"KaiquanMah/open-intent-query-classification\", repo_type=\"space\", allow_patterns=\"*_idx2label.csv\", local_dir='/kaggle/working/idx2label')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nidx2label = pd.read_csv('/kaggle/working/idx2label/dataset_idx2label/banking77_idx2label.csv')\nidx2label","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"idx2label_oos = idx2label[idx2label.index.isin([31,32,33,36])]\nidx2label_oos","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(idx2label_oos)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"idx2label_oos.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# percentage of OOS classes over ALL classes in the dataset\nlen(idx2label_oos)/len(idx2label)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Stitch Nebius Qwen API Batch Results for Stackoverflow","metadata":{}},{"cell_type":"code","source":"import subprocess\nimport sys\nfrom src.config import Config\n\n\n# 1. Install libraries from requirements.txt\nprint(\"Installing dependencies...\")\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", \"/kaggle/working/requirements.txt\"], check=True)\n\n\n# # 2. Run setup_ollama.py\n# if 'gemini' not in Config.model_name:\n#     print(\"Starting Ollama setup...\")\n#     # subprocess.run([\"python3\", \"/kaggle/working/src/setup_ollama.py\"], check=True)\n#     print(\"Starting Ollama setup...\")\n#     subprocess.run(\n#         [\"python3\", \"-m\", \"src.setup_ollama\"],  # Run as a module\n#         cwd=\"/kaggle/working\",  # Set working directory to parent of 'src'\n#         check=True\n#     )\n    \n\n# 3. Run download_dataset.py\nprint(\"Downloading dataset...\")\nsubprocess.run([\"python3\", \"/kaggle/working/download_dataset.py\"], check=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T09:01:58.444873Z","iopub.execute_input":"2025-07-11T09:01:58.445582Z","iopub.status.idle":"2025-07-11T09:02:02.490132Z","shell.execute_reply.started":"2025-07-11T09:01:58.445552Z","shell.execute_reply":"2025-07-11T09:02:02.489150Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Installing dependencies...\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 1)) (2.2.3)\nRequirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 2)) (2.11.4)\nRequirement already satisfied: typing in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 3)) (3.7.4.3)\nRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 4)) (0.31.1)\nRequirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 6)) (1.70.0)\nRequirement already satisfied: tenacity in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 7)) (9.1.2)\nRequirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas->-r /kaggle/working/requirements.txt (line 1)) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->-r /kaggle/working/requirements.txt (line 1)) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->-r /kaggle/working/requirements.txt (line 1)) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->-r /kaggle/working/requirements.txt (line 1)) (2025.2)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->-r /kaggle/working/requirements.txt (line 2)) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->-r /kaggle/working/requirements.txt (line 2)) (2.33.2)\nRequirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->-r /kaggle/working/requirements.txt (line 2)) (4.13.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->-r /kaggle/working/requirements.txt (line 2)) (0.4.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->-r /kaggle/working/requirements.txt (line 4)) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->-r /kaggle/working/requirements.txt (line 4)) (2025.3.2)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->-r /kaggle/working/requirements.txt (line 4)) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->-r /kaggle/working/requirements.txt (line 4)) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->-r /kaggle/working/requirements.txt (line 4)) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->-r /kaggle/working/requirements.txt (line 4)) (4.67.1)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->-r /kaggle/working/requirements.txt (line 4)) (1.1.0)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai->-r /kaggle/working/requirements.txt (line 6)) (4.9.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai->-r /kaggle/working/requirements.txt (line 6)) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai->-r /kaggle/working/requirements.txt (line 6)) (0.28.1)\nRequirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai->-r /kaggle/working/requirements.txt (line 6)) (0.9.0)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai->-r /kaggle/working/requirements.txt (line 6)) (1.3.1)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai->-r /kaggle/working/requirements.txt (line 6)) (3.10)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai->-r /kaggle/working/requirements.txt (line 6)) (2025.4.26)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai->-r /kaggle/working/requirements.txt (line 6)) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r /kaggle/working/requirements.txt (line 6)) (0.14.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas->-r /kaggle/working/requirements.txt (line 1)) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas->-r /kaggle/working/requirements.txt (line 1)) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas->-r /kaggle/working/requirements.txt (line 1)) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas->-r /kaggle/working/requirements.txt (line 1)) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas->-r /kaggle/working/requirements.txt (line 1)) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas->-r /kaggle/working/requirements.txt (line 1)) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->-r /kaggle/working/requirements.txt (line 1)) (1.17.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->-r /kaggle/working/requirements.txt (line 4)) (3.4.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->-r /kaggle/working/requirements.txt (line 4)) (2.4.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas->-r /kaggle/working/requirements.txt (line 1)) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas->-r /kaggle/working/requirements.txt (line 1)) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.23.2->pandas->-r /kaggle/working/requirements.txt (line 1)) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.23.2->pandas->-r /kaggle/working/requirements.txt (line 1)) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.23.2->pandas->-r /kaggle/working/requirements.txt (line 1)) (2024.2.0)\nDownloading dataset...\nDataset has already been downloaded. If this is incorrect, please delete the Adaptive-Decision-Boundary 'data' folder.\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"CompletedProcess(args=['python3', '/kaggle/working/download_dataset.py'], returncode=0)"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"from src.config import Config\nimport pandas as pd\nimport os\n# import ollama\nimport json\nimport pickle\nimport time\nfrom pydantic import BaseModel\nfrom typing import Literal\n# from enum import Enum\nfrom huggingface_hub import snapshot_download\n    \n###################\n# Gemini API\n###################\n# from google import genai\n# from google.genai.types import ThinkingConfig\n# from google.api_core import retry\nfrom openai import OpenAI\nfrom tenacity import retry, stop_after_attempt, wait_fixed\nfrom kaggle_secrets import UserSecretsClient\n\n\n###################\n\n\n# Config.target_dir\n# Config.cloned_data_dir'\n# Config.dataset_name\n# Config.model_name\n# Config.start_index\n# Config.end_index\n# Config.log_every_n_examples\n\n\n#######################\n# load data\n#######################\ndef load_data(data_dir):\n    \"\"\"Loads train, dev, and test datasets from a specified directory.\"\"\"\n\n    main_df = pd.DataFrame()\n    for split in ['train', 'dev', 'test']:\n        file_path = os.path.join(data_dir, f'{split}.tsv')\n        if os.path.exists(file_path):\n          try:\n            df = pd.read_csv(file_path, sep='\\t')\n            df['dataset'] = os.path.basename(data_dir)\n            df['split'] = split\n            main_df = pd.concat([main_df, df], ignore_index=True)\n          except pd.errors.ParserError as e:\n            print(f\"Error parsing {file_path}: {e}\")\n            # Handle the error appropriately, e.g., skip the file, log the error, etc.\n        else:\n            print(f\"Warning: {split}.tsv not found in {data_dir}\")\n    return main_df\n\n\ndef filter100examples_oos(dataset_name, df):\n    # dont input 'only oos qns to model'\n    if Config.filter_oos_qns_only == False:\n        filtered_df = df\n    # vs\n    # input 'only oos qns to model'\n    else:\n        if dataset_name == 'banking':\n            first_class = Config.first_class_banking\n        elif dataset_name == 'stackoverflow':\n            first_class = Config.first_class_stackoverflow\n        else:\n            first_class = Config.first_class_oos\n    \n        filtered_df = df.copy()\n        filtered_df = filtered_df.loc[filtered_df[\"label\"] == first_class]\n        filtered_df = filtered_df.sample(n=Config.n_oos_qns, random_state=38)\n    return filtered_df\n\n\ndf = pd.DataFrame()\n\ndata_dir = os.path.join(Config.cloned_data_dir, Config.dataset_name)\nif os.path.exists(data_dir):\n  df = load_data(data_dir)\n  print(f\"Loaded dataset into dataframe: {Config.dataset_name}\")\n  print(f\"Dimensions: {df.shape}\")\n  print(f\"Col names: {df.columns}\")\nelse:\n  print(f\"Warning: Directory {data_dir} not found.\")\n#######################\n\n\n\n#######################\n# unique intents\n#######################\nsorted_intent = list(sorted(df.label.unique()))\nprint(\"=\"*80)\nprint(f\"Original dataset intents: {sorted_intent}\")\nprint(f\"Number of original intents: {len(sorted_intent)}\\n\")\n\n\n# 2025.06.03\n# New OOS approach - get 25/50/75% of class indexes for each dataset within the team (for reproducibility and comparable results)\n# Change their class labels to 'oos'\nsnapshot_download(repo_id=\"KaiquanMah/open-intent-query-classification\", repo_type=\"space\", allow_patterns=\"*_idx2label.csv\", local_dir=Config.idx2label_target_dir)\nidx2label_filepath = Config.idx2label_target_dir + '/dataset_idx2label/' + Config.idx2label_filename_hf\nidx2label = pd.read_csv(idx2label_filepath)\nidx2label_oos = idx2label[idx2label.index.isin(Config.list_oos_idx)]\nidx2label_oos.reset_index(drop=True, inplace=True)\n\n# 2025.06.17 keep track of non-oos labels, to use in IntentSchema\nnonoos_labels = idx2label[~idx2label.label.isin(Config.list_oos_idx)]['label'].values\nprint(\"=\"*80)\nprint(\"Original intents to convert to OOS class\")\nprint(idx2label_oos)\nprint(f\"Percentage of original intents to convert to OOS class: {len(idx2label_oos)/len(idx2label)}\\n\")\n\noos_labels = idx2label_oos['label'].values\nlist_sorted_intent_aft_conversion = ['oos' if intent.lower() in oos_labels else intent for intent in sorted_intent]\nlist_sorted_intent_aft_conversion_deduped = sorted(set(list_sorted_intent_aft_conversion))\nprint(\"=\"*80)\nprint(\"Unique intents after converting some to OOS class\")\nprint(list_sorted_intent_aft_conversion_deduped)\nprint(f\"Number of unique intents after converting some to OOS class: {len(list_sorted_intent_aft_conversion_deduped)}\\n\")\n\n\n\n# unique intents - from set to bullet points (to use in prompts)\n# bulletpts_intent = \"\\n\".join(f\"- {category}\" for category in set_intent)\n# 2025.06.03: do not show 'oos' in the prompt (to avoid leakage of 'oos' class)\nbulletpts_intent = \"\\n\".join(f\"- {category}\" for category in list_sorted_intent_aft_conversion_deduped if category and (category!='oos'))\n\n# 2025.06.04: fix adjustment if 'oos' is already in the original dataset\nint_oos_in_orig_dataset = int('oos' in idx2label.label.values)\nadjust_if_oos_not_in_orig_dataset = [0 if int_oos_in_orig_dataset == 1 else 1][0]\n\nprint(\"=\"*80)\nprint(\"sanity check\")\nprint(f\"Number of original intents: {len(sorted_intent)}\")\nprint(f\"Number of original intents + 1 OOS class (if doesnt exist in original dataset): {len(sorted_intent) + adjust_if_oos_not_in_orig_dataset}\")\nprint(f\"Number of original intents to convert to OOS class: {len(idx2label_oos)}\")\nprint(f\"Percentage of original intents to convert to OOS class: {len(idx2label_oos)/len(idx2label)}\")\nprint(f\"Number of unique intents after converting some to OOS class: {len(list_sorted_intent_aft_conversion_deduped)}\")\nprint(f\"Number of original intents + 1 OOS class (if doesnt exist in original dataset) - converted classes: {len(sorted_intent) + adjust_if_oos_not_in_orig_dataset - len(idx2label_oos)}\")\nprint(f\"Numbers match: {(len(sorted_intent) + adjust_if_oos_not_in_orig_dataset - len(idx2label_oos)) == len(list_sorted_intent_aft_conversion_deduped)}\")\nprint(\"Prepared unique intents\")\n#######################\n\n\n\n\n#######################\n# Enforce schema on the model (e.g. allowed list of predicted categories)\n#######################\n\nclass IntentSchema(BaseModel):\n    # dynamically unpack list of categories for different dataset(s)\n    category: Literal[*list_sorted_intent_aft_conversion_deduped]\n    confidence: float\n    \n#######################\n\n\n\n\n#######################\n# filter after preparing intents\n#######################\ndf = filter100examples_oos(Config.dataset_name, df)\nprint(\"Filtered dataset\")\nprint(f\"Dimensions: {df.shape}\")\nprint(f\"Col names: {df.columns}\")\n#######################\n\n\n\n#######################\n# Prompt\n#######################\n# prompt 2 with less information/compute, improve efficiency\n# 2025.06.10 prompt 3 with 5 few shot examples only - notebook O1H1, O1i1\n# 2025.06.16 prompt 4 with 5 examples per each known intent (ie non-oos intent) - notebook 01J1\nsnapshot_download(repo_id=\"KaiquanMah/open-intent-query-classification\", repo_type=\"space\", allow_patterns=\"*.txt\", local_dir=Config.fewshot_examples_dir)\nwith open(Config.fewshot_examples_dir + Config.fewshot_subdir + Config.fewshot_examples_filename, 'r') as file:\n    fewshot_examples = file.read()\n\ndef get_prompt(dataset_name, split, question, categories, fewshot_examples):\n    \n    prompt = f'''\nYou are an expert in understanding and identifying what users are asking you.\n\nYour task is to analyze an input query from a user and assign the most appropriate category from the following list:\n{categories}\n\nOnly classify as \"oos\" (out of scope category) if none of the other categories apply.\n\nBelow are several examples to guide your classification:\n\n---\n{fewshot_examples}\n---\n\n===============================\n\nNew Question: {question}\n\n===============================\n\nProvide your final classification in **valid JSON format** with the following structure:\n{{\n  \"category\": \"your_chosen_category_name\",\n  \"confidence\": confidence_level_rounded_to_the_nearest_2_decimal_places\n}}\n\n\nEnsure the JSON has:\n- Opening and closing curly braces\n- Double quotes around keys and string values\n- Confidence as a number (not a string), with maximum 2 decimal places\n\nDo not include any explanations or extra text.\n            '''\n    return prompt\n\n\n\n#######################\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T09:02:02.491362Z","iopub.execute_input":"2025-07-11T09:02:02.491588Z","iopub.status.idle":"2025-07-11T09:02:02.953459Z","shell.execute_reply.started":"2025-07-11T09:02:02.491570Z","shell.execute_reply":"2025-07-11T09:02:02.952512Z"}},"outputs":[{"name":"stdout","text":"Loaded dataset into dataframe: stackoverflow\nDimensions: (20000, 4)\nCol names: Index(['text', 'label', 'dataset', 'split'], dtype='object')\n================================================================================\nOriginal dataset intents: ['ajax', 'apache', 'bash', 'cocoa', 'drupal', 'excel', 'haskell', 'hibernate', 'linq', 'magento', 'matlab', 'oracle', 'osx', 'qt', 'scala', 'sharepoint', 'spring', 'svn', 'visual-studio', 'wordpress']\nNumber of original intents: 20\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e3465107ae648839fc66704828aea30"}},"metadata":{}},{"name":"stdout","text":"================================================================================\nOriginal intents to convert to OOS class\n   index    label\n0      3      svn\n1     11   spring\n2     15     ajax\n3     16       qt\n4     17   drupal\n5     18     linq\n6     20  magento\nPercentage of original intents to convert to OOS class: 0.35\n\n================================================================================\nUnique intents after converting some to OOS class\n['apache', 'bash', 'cocoa', 'excel', 'haskell', 'hibernate', 'matlab', 'oos', 'oracle', 'osx', 'scala', 'sharepoint', 'visual-studio', 'wordpress']\nNumber of unique intents after converting some to OOS class: 14\n\n================================================================================\nsanity check\nNumber of original intents: 20\nNumber of original intents + 1 OOS class (if doesnt exist in original dataset): 21\nNumber of original intents to convert to OOS class: 7\nPercentage of original intents to convert to OOS class: 0.35\nNumber of unique intents after converting some to OOS class: 14\nNumber of original intents + 1 OOS class (if doesnt exist in original dataset) - converted classes: 14\nNumbers match: True\nPrepared unique intents\nFiltered dataset\nDimensions: (20000, 4)\nCol names: Index(['text', 'label', 'dataset', 'split'], dtype='object')\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 62 files:   0%|          | 0/62 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2993a3f6da18484285bee14a1043cefd"}},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"len(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T09:02:02.954544Z","iopub.execute_input":"2025-07-11T09:02:02.954842Z","iopub.status.idle":"2025-07-11T09:02:02.960868Z","shell.execute_reply.started":"2025-07-11T09:02:02.954822Z","shell.execute_reply":"2025-07-11T09:02:02.959801Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"20000"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T09:02:02.962743Z","iopub.execute_input":"2025-07-11T09:02:02.963126Z","iopub.status.idle":"2025-07-11T09:02:02.994103Z","shell.execute_reply.started":"2025-07-11T09:02:02.963101Z","shell.execute_reply":"2025-07-11T09:02:02.993162Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"                                                    text       label  \\\n0                   Scala Regex Multiple Block Capturing       scala   \n1                  Use Oracle 6 from ASP.NET application      oracle   \n2                         HQL 1 to many count() question   hibernate   \n3                  scala syntax highlighting in bluefish       scala   \n4                                   Weird bindings issue       cocoa   \n...                                                  ...         ...   \n19995            SharePoint remembering changed password  sharepoint   \n19996  Magento - Find Out of Stock Products With Inve...     magento   \n19997           Python OS X 10.5 development environment         osx   \n19998                 Crop & Resize Images in  Wordpress   wordpress   \n19999  Code won't work under mono, any ideas whats wr...      apache   \n\n             dataset  split  \n0      stackoverflow  train  \n1      stackoverflow  train  \n2      stackoverflow  train  \n3      stackoverflow  train  \n4      stackoverflow  train  \n...              ...    ...  \n19995  stackoverflow   test  \n19996  stackoverflow   test  \n19997  stackoverflow   test  \n19998  stackoverflow   test  \n19999  stackoverflow   test  \n\n[20000 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n      <th>dataset</th>\n      <th>split</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Scala Regex Multiple Block Capturing</td>\n      <td>scala</td>\n      <td>stackoverflow</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Use Oracle 6 from ASP.NET application</td>\n      <td>oracle</td>\n      <td>stackoverflow</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>HQL 1 to many count() question</td>\n      <td>hibernate</td>\n      <td>stackoverflow</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>scala syntax highlighting in bluefish</td>\n      <td>scala</td>\n      <td>stackoverflow</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Weird bindings issue</td>\n      <td>cocoa</td>\n      <td>stackoverflow</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>19995</th>\n      <td>SharePoint remembering changed password</td>\n      <td>sharepoint</td>\n      <td>stackoverflow</td>\n      <td>test</td>\n    </tr>\n    <tr>\n      <th>19996</th>\n      <td>Magento - Find Out of Stock Products With Inve...</td>\n      <td>magento</td>\n      <td>stackoverflow</td>\n      <td>test</td>\n    </tr>\n    <tr>\n      <th>19997</th>\n      <td>Python OS X 10.5 development environment</td>\n      <td>osx</td>\n      <td>stackoverflow</td>\n      <td>test</td>\n    </tr>\n    <tr>\n      <th>19998</th>\n      <td>Crop &amp; Resize Images in  Wordpress</td>\n      <td>wordpress</td>\n      <td>stackoverflow</td>\n      <td>test</td>\n    </tr>\n    <tr>\n      <th>19999</th>\n      <td>Code won't work under mono, any ideas whats wr...</td>\n      <td>apache</td>\n      <td>stackoverflow</td>\n      <td>test</td>\n    </tr>\n  </tbody>\n</table>\n<p>20000 rows  4 columns</p>\n</div>"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"######################\n# preprocess batch outputs from Nebius\n# then export to intermediate.json\n######################\nimport json\n\ndef jsonl_to_json_extract_content(jsonl_file_path, json_file_path):\n    \"\"\"\n    Reads a JSONL file, extracts LLM message content, and saves to a JSON file.\n\n    Args:\n        jsonl_file_path (str): The path to the input JSONL file.\n        json_file_path (str): The path to the output JSON file.\n    \"\"\"\n    extracted_data = []\n    with open(jsonl_file_path, 'r') as infile:\n        for line in infile:\n            record_dict = {}\n\n            data = json.loads(line)\n            # Extract content from the nested structure\n            record_dict[\"Index\"] = data[\"custom_id\"]\n\n            content = json.loads(data[\"response\"][\"choices\"][0][\"message\"][\"content\"])\n            record_dict[\"predicted\"] = content[\"category\"]\n            record_dict[\"confidence\"] = content[\"confidence\"]\n            # Remove backslashes and newline characters\n            # content = content.replace('\\\\', '').replace('\\n', '').strip()\n            extracted_data.append(record_dict)\n\n    with open(json_file_path, 'w') as outfile:\n        json.dump(extracted_data, outfile, indent=2)\n\n# Example usage\njsonl_file_path = '/kaggle/input/01l8-openintent-nebiusqwen-stackoverflow-batch/batch_outputs_stackoverflow_0_None.jsonl'\njson_file_path = 'batch_outputs_stackoverflow_intermediate.json'\njsonl_to_json_extract_content(jsonl_file_path, json_file_path)\n\nprint(f\"Extracted content saved to {json_file_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T09:02:02.995465Z","iopub.execute_input":"2025-07-11T09:02:02.995820Z","iopub.status.idle":"2025-07-11T09:02:03.380211Z","shell.execute_reply.started":"2025-07-11T09:02:02.995796Z","shell.execute_reply":"2025-07-11T09:02:03.379250Z"}},"outputs":[{"name":"stdout","text":"Extracted content saved to batch_outputs_stackoverflow_intermediate.json\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"######################\n# import intermediate.json\n# then stitch data with original df \n# to trace and retrieve original text qn and label/class\n######################\nimport pandas as pd\nimport json\n\n# Load JSON data\nwith open('/kaggle/working/batch_outputs_stackoverflow_intermediate.json', 'r') as f:\n    json_data = json.load(f)  # List of dictionaries\n\n# Create lookup dictionary: {Index: {predicted, confidence}}\njson_lookup = {item[\"Index\"]: item for item in json_data}  # Keys are strings\n\n# Initialize results list\nresults = []\n\n# Iterate through DataFrame rows\nfor row in df.itertuples():\n    # Convert DataFrame Index to string for consistent lookup\n    idx = row.Index\n    \n    # Retrieve prediction if exists, else None\n    pred_entry = json_lookup.get(str(idx))\n    \n    # Build result dictionary\n    results.append({\n        \"Index\": row.Index,\n        \"text\": row.text,\n        \"label\": row.label,\n        \"dataset\": row.dataset,\n        \"split\": row.split,\n        \"predicted\": pred_entry['predicted'],\n        \"confidence\": pred_entry['confidence']\n    })\n\n# Now `results` contains your combined data\nsubset_results = results\n\n#################################################\n# export to JSON in the format we expect\n#################################################\nmodel_name = Config.model_name\ndf = df\ncategories = bulletpts_intent\nstart_index = Config.start_index\nend_index = Config.end_index\nlog_every_n_examples = Config.log_every_n_examples\nend_index = max(r['Index'] for r in subset_results)\n\nwith open(f'results_{Config.model_name}_{Config.dataset_name}_{Config.start_index}_{end_index}.json', 'w') as f:\n    json.dump(subset_results, f, indent=2)\n\nprint(\"Completed intent classification\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T09:02:03.381198Z","iopub.execute_input":"2025-07-11T09:02:03.381447Z","iopub.status.idle":"2025-07-11T09:02:03.661806Z","shell.execute_reply.started":"2025-07-11T09:02:03.381427Z","shell.execute_reply":"2025-07-11T09:02:03.660576Z"}},"outputs":[{"name":"stdout","text":"Completed intent classification\n","output_type":"stream"}],"execution_count":28}]}